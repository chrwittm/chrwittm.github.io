{
 "cells": [
  {
   "cell_type": "raw",
   "id": "de76353f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Running GPT-OSS Locally on macOS with Ollama + Open WebUI\"\n",
    "author: \"Christian Wittmann\"\n",
    "date: \"2025-08-08\"\n",
    "categories: [ollama, gpt-oss, llm]\n",
    "image: \"GPT-OSS.jpg\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f0b94",
   "metadata": {},
   "source": [
    "Wouldn’t it be nice to have a ChatGPT-grade assistant running locally on your laptop? That’s now possible with OpenAI’s first open-weight models, [GPT-OSS](https://openai.com/index/introducing-gpt-oss/). The 20B version is small enough to run on a modern consumer laptop.\n",
    "\n",
    "My MacBook Pro has an M2 Max with 32 GB of unified memory, by far not the latest and greatest, but the experience is… nice, running at around 33 output tokens per second. It’s not going to outrun GPT-5 in the cloud, and it's not multi-modal, but when you just need text-based conversations, it’s a powerful model. And the offline factor is a real advantage when you're disconnected or want to keep everything on-device.\n",
    "\n",
    "Back in February 2024, I wrote about [running Llama 2 locally on my Mac](https://chrwittm.github.io/posts/2024-02-15-running-llama2-on-mac). Fast-forward ~18 months and a lot has changed, not just model performance (it’s now far superior on the same hardware), but also the tooling. This time I chose [Ollama](https://ollama.com/) for its active community and ecosystem of add-ons. In this post, I’ll walk you through all the steps to get GPT-OSS up and running on a Mac (as long as you have at least 16 GB of RAM), complete with a nice web UI via [Open WebUI](https://openwebui.com/).\n",
    "\n",
    "I recorded the process as I went, so this guide is intentionally more verbose than OpenAI’s official [Cookbook](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama). (-> to be explicit about every step you’ll need to take)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd92ab6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Install Ollama\n",
    "\n",
    "Ollama is the runtime that will download, run, and manage your local models. You can install it in two ways:\n",
    "\n",
    "**Option A — macOS installer**\n",
    "\n",
    "- Download from [https://ollama.com/download](https://ollama.com/download)\n",
    "- Run the installer — this gives you both the Ollama app and the `ollama` CLI.\n",
    "- To upgrade: simply download the latest installer and run it again.\n",
    "\n",
    "**Option B — Homebrew (my recommendation)**\n",
    "\n",
    "If you’re comfortable with Terminal, Homebrew makes installation and upgrades much quicker:\n",
    "\n",
    "```bash\n",
    "brew install ollama  # installs Ollama\n",
    "ollama --version     # check your installed version\n",
    "brew upgrade ollama  # upgrade Ollama to the latest version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1134be6f",
   "metadata": {},
   "source": [
    "## Starting and stopping Ollama\n",
    "\n",
    "With the Homebrew installation, I run Ollama as a background service so it’s always ready:\n",
    "\n",
    "```bash\n",
    "brew services start ollama   # starts in the background, auto-starts after reboot\n",
    "brew services restart ollama # restarts after upgrading\n",
    "brew services stop ollama    # stops completely\n",
    "```\n",
    "\n",
    "If you’d rather start it manually, run:\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "This will keep it running in that terminal until you press `Ctrl+C`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342897a",
   "metadata": {},
   "source": [
    "## Managing models\n",
    "\n",
    "Ollama has its own [model library](https://ollama.com/library), and GPT-OSS lives here: [https://ollama.com/library/gpt-oss](https://ollama.com/library/gpt-oss).\n",
    "\n",
    "You can manage the models on your Mac with the following commands (of course, you can replace `gpt-oss` with other model names from the library):\n",
    "\n",
    "```bash\n",
    "ollama list           # see which models are installed\n",
    "ollama pull gpt-oss   # download GPT-OSS\n",
    "ollama rm gpt-oss     # delete GPT-OSS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e57165",
   "metadata": {},
   "source": [
    "## Testing GPT-OSS from the command line\n",
    "\n",
    "Testing as early as possible is important. Before adding the UI, let’s confirm everything is working correctly from the CLI:\n",
    "\n",
    "```bash\n",
    "ollama run gpt-oss\n",
    "```\n",
    "\n",
    "This will start a chat-like session with the model. Try out prompts like:\n",
    "\n",
    "- `List the planets of the solar system`\n",
    "- `Reverse the list`\n",
    "\n",
    "Once you’re done, you can exit the chat with the command `/exit`.\n",
    "\n",
    "You can also do a one-off prompt without entering chat mode:\n",
    "\n",
    "```bash\n",
    "ollama run gpt-oss \"What is the meaning of life in one sentence?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f960b7",
   "metadata": {},
   "source": [
    "## Installing Open WebUI\n",
    "\n",
    "To make the whole experience more user-friendly, let’s add a web UI. I use [Open WebUI](https://openwebui.com/), which provides a clean interface similar to OpenAI, Anthropic, or Google, including chat history and multiple chat sessions.\n",
    "\n",
    "If you read through the documentation, they mention a Docker setup first. However, the method below (listed as “recommended” later in their docs) is easier.\n",
    "\n",
    "First, install **uv**, a modern Python runner:\n",
    "\n",
    "```bash\n",
    "brew install uv\n",
    "```\n",
    "\n",
    "Then start Open WebUI with:\n",
    "\n",
    "```bash\n",
    "DATA_DIR=~/.open-webui uvx --python 3.11 open-webui@latest serve\n",
    "```\n",
    "\n",
    "Let’s break it down:\n",
    "\n",
    "- `DATA_DIR=~/.open-webui`: This sets an environment variable for the command that follows. It tells Open WebUI where to store its chat history and configuration. This path will persist your data between runs.\n",
    "- `uvx`: Use UVX (a modern Python environment manager) to run Open WebUI in an isolated environment.\n",
    "- `--python 3.11`: Specifies the Python version to use. The Open WebUI team recommends Python 3.11.\n",
    "- `open-webui@latest serve`: Installs (if necessary) and starts the latest version of Open WebUI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6ead5",
   "metadata": {},
   "source": [
    "\n",
    "## Running Open WebUI\n",
    "\n",
    "Loading and setting up the virtual environment takes around 30 seconds (longer the very first time you run it). Once it’s ready, open:\n",
    "\n",
    "[http://localhost:8080](http://localhost:8080)\n",
    "\n",
    "> Tip: Ollama must be running first (`brew services start ollama`). You can check if it’s active by visiting [http://127.0.0.1:11434](http://127.0.0.1:11434) — if the service responds, you’re good.\n",
    "\n",
    "From here, the UI should feel familiar if you’ve used ChatGPT. Start a new chat, select GPT-OSS, and you’re ready to go.\n",
    "\n",
    "When you’re finished, press `Ctrl+C` in the terminal where you started Open WebUI.\n",
    "\n",
    "> Note: Leaving Ollama running as a background service is fine — it uses almost no CPU or battery when idle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f857699b",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Yes, took a few steps, but the whole setup can be done in 30–60 minutes, even if you pause to test things along the way. In return, you get your own personal ChatGPT-style assistant running entirely on your Mac: Now you are ready for curiosity projects, privacy-sensitive work, or building your next idea without sending data to the cloud."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
