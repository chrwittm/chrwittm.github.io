{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"How to Turn GPT into a Calculator\"\n",
    "author: \"Christian Wittmann\"\n",
    "date: \"2024-07-30\"\n",
    "categories: [LLM, function_calling, tools, calculator, ReAct]\n",
    "image: \"llm-with-calculator.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) are great writers, but they struggle with numbers: Counting, adding, and basic arithmetic. I guess this is why they are called Large Language Models and not Large Math Models ðŸ˜‰. In this blog post, let's explore why LLMs struggle with math and how we can fix this. We will dive into the topic of function calling and successfully turn an LLM into a calculator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    text-align: center;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"llm-with-calculator.png\" alt=\"Dalle: An LLM with a connected calculator\" style=\"width:50%;\">\n",
    "    <figcaption>Dalle: An LLM with a connected calculator</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why LLMs struggle with math?\n",
    "\n",
    "What is $6574 \\times 9132$? The answer is $60.033.768$, and computers could solve this question easily in their infancy, yet modern LLMs struggle with this question. I asked GPT-3.5 and GPT-4o:\n",
    "\n",
    "- [GPT3.5 answered](https://chatgpt.com/share/33cc61f2-10b3-4c7f-b0c5-c9fe6d93daec): _\"6574 multiplied by 9132 equals 60,088,968.\"_ (incorrect) \n",
    "- [GTP-4o answered](https://chatgpt.com/share/0b431609-02f1-4996-880e-2450d0fa67eb): _\"The product of 6574 and 9132 is 60,052,968.\"_ (incorrect)\n",
    "\n",
    "Both results are incorrect. Why is that? The reason is [rooted in tokenization](https://www.youtube.com/watch?v=zduSFxRajkE&t=6909s) [1] and the fact that these numbers are treated like text. Easy tasks like $3 \\times 7$ will most likely always be \"calculated\" correctly. But they are not really calculated, the LLM just learned the result from the training data. This is the same as when you learned the basic multiplication tables. Essentially, you do not calculate the numbers all the time, but you have learned that $3 \\times 7 = 21$. For the LLM $21$ is just the most likely next token after the input sequence $3 \\times 7$. LLMs, therefore, are somewhat human ðŸ˜‰. Coming back to the original example, LLMs do not really calculate the multiplication of four-digit numbers, they rather estimate the result.\n",
    "\n",
    "Let me ask you a question: How would you solve the task of calculating $6574 \\times 9132$? Most of us would use a calculator, and this is exactly the tooling we need to give to the LLM. Andrej Karpathy has put a calculator on his [vision of an LLM OS](https://www.youtube.com/watch?v=zjkBMFhNj_g&t=2535s) [2]. Since this is a fundamental tool, let's build it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    text-align: center;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"llm-os-calculator.png\" alt=\"The calculator as part of LLM OS\" style=\"width:100%;\">\n",
    "    <figcaption>The calculator as part of LLM OS</a>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the plan of what we will cover in this blog post:\n",
    "\n",
    "- First, we will build a simple chat client based on the OpenAI API.\n",
    "- Next, we dive into the concept of function calling without implementing it in the chat client.\n",
    "- Afterwards, we integrate function calling into the chat client.\n",
    "- Next, we pause for a bit and take a deep breath recapping what we have done so far.\n",
    "- This recap will lead us to a research paper on the subject.\n",
    "- Finally, we will assemble the whole calculator and\n",
    "- Wrap up this blog post.\n",
    "\n",
    "If you like to interactively run this notebook, hop over to GitHub: Here is the [Jupyter notebook version of this blog post](https://github.com/chrwittm/lm-hackers/blob/main/50-function-calling/llm-calculator1/index.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Basic Chat Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple chat client so that we can run our experiments. For a start, here is the chat messages class from [Building Chat for Jupyter Notebooks from Scratch](https://chrwittm.github.io/posts/2024-02-23-chat-from-scratch/): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "class ChatMessages:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the Chat.\"\"\"\n",
    "        self._messages = []\n",
    "\n",
    "    def _append_message(self, role, content):\n",
    "        \"\"\"Appends a message with specified role and content to messages list.\"\"\"\n",
    "        self._messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def append_system_message(self, content):\n",
    "        \"\"\"Appends a system message with specified content to messages list.\"\"\"\n",
    "        self._append_message(\"system\", content)\n",
    "\n",
    "    def append_user_message(self, content):\n",
    "        \"\"\"Appends a user message with specified content to messages list.\"\"\"\n",
    "        self._append_message(\"user\", content)\n",
    "\n",
    "    def append_assistant_message(self, content):\n",
    "        \"\"\"Appends an assistant message with specified content to messages list.\"\"\"\n",
    "        self._append_message(\"assistant\", content)\n",
    "\n",
    "    def get_messages(self):\n",
    "        \"\"\"Returns a shallow copy of the messages list.\"\"\"\n",
    "        return self._messages[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the chat client which talks to the OpenAI API. For more details, please refer to my blog post [How to call the OpenAI API from a Jupyter Notebook](https://chrwittm.github.io/posts/2024-01-27-how-to-call-openai-api/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "#model_name = \"gpt-3.5-turbo\"\n",
    "#model_name = \"gpt-4o-mini\"\n",
    "model_name = \"gpt-4o\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "from openai import chat\n",
    "\n",
    "class ChatClient:\n",
    "\n",
    "    def __init__(self, system_message):\n",
    "        \"\"\"Initializes the Chat with the system message.\"\"\"\n",
    "        self._chat_messages = ChatMessages()\n",
    "        self._chat_messages.append_system_message(system_message)\n",
    "\n",
    "    def ask_gpt(self, prompt):\n",
    "        \"\"\"Calls the LLM chat completion API and returns the response message\"\"\"\n",
    "        self._chat_messages.append_user_message(prompt)\n",
    "\n",
    "        c = chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=self._chat_messages.get_messages())\n",
    "\n",
    "        self._chat_messages.append_assistant_message(c.choices[0].message.content)\n",
    "\n",
    "        return c.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick test of our chat client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_client = ChatClient(\"Answer in a very concise and accurate way\")\n",
    "chat_client.ask_gpt(\"Name the planets in the solar system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neptune, Uranus, Saturn, Jupiter, Mars, Earth, Venus, Mercury.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_client.ask_gpt(\"Reverse the list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, let's transition to calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6574 * 9132 = 60075168.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_client = ChatClient(\"You are a calculator.\")\n",
    "chat_client.ask_gpt(\"What is 6574 * 9132?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost correct ðŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60033768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6574 * 9132"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we teach the LLM some math? The secret is \"[function calling](https://platform.openai.com/docs/guides/function-calling)\". In the OpenAI API (and the APIs of other LLMs) we are can specify \"tools\" we can give to the LLM. Currently, only \"[functions](https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools)\" are supported. What sounds like a restriction is actually quite sufficient and convenient. \n",
    "\n",
    "For now, Let's teach ChatGPT how to properly multiply two numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60033768"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiply(a:int, b:int=1):\n",
    "    \"Multiplies a * b\"\n",
    "    return a * b\n",
    "\n",
    "multiply(a=6574, b=9132)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to pass this function to ChatGPT, we need to pass the tool definition in the request as defined in the [OpenAI API](https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools). Since writing out the JSON can be somewhat painful and repetitive, here is a function that extracts the JSON from a Python function. The original is from [Jeremy Howard's Hacker's Guide](https://github.com/fastai/lm-hackers/blob/main/lm-hackers.ipynb) [3]. In the meantime, the API has changed a bit, so there are a few updates in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'multiply',\n",
       "   'description': 'Multiplies a * b',\n",
       "   'parameters': {'properties': {'a': {'title': 'A', 'type': 'integer'},\n",
       "     'b': {'default': 1, 'title': 'B', 'type': 'integer'}},\n",
       "    'required': ['a'],\n",
       "    'title': 'Input for `multiply`',\n",
       "    'type': 'object'}}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "from pydantic import create_model\n",
    "import inspect, json\n",
    "from inspect import Parameter\n",
    "\n",
    "def get_schema(f):\n",
    "    kw = {n:(o.annotation, ... if o.default==Parameter.empty else o.default)\n",
    "          for n,o in inspect.signature(f).parameters.items()}\n",
    "    # update: schema -> model_json_schema\n",
    "    s = create_model(f'Input for `{f.__name__}`', **kw).model_json_schema()\n",
    "    # update: added function level in tools json\n",
    "    function_params = dict(name=f.__name__, description=f.__doc__, parameters=s)\n",
    "    return dict(type=\"function\", function=function_params)\n",
    "\n",
    "[get_schema(multiply)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass the tool to ChatGPT and ask it to do the calculation. The response looks a bit different: Instead of text, ChatGPT returns a tool call object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text response: None\n",
      "Tool call: [ChatCompletionMessageToolCall(id='call_qP9MhfCgWyjNZHWGyWNEjP2e', function=Function(arguments='{\"a\":6574,\"b\":9132}', name='multiply'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "from openai import chat\n",
    "\n",
    "chat_messages = ChatMessages()\n",
    "chat_messages.append_system_message(\"You are a calculator.\")\n",
    "chat_messages.append_user_message(\"What is 6574 * 9132?\")\n",
    "\n",
    "c = chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=chat_messages.get_messages(),\n",
    "    tools=[get_schema(multiply)])\n",
    "\n",
    "print(f\"Text response: {c.choices[0].message.content}\")\n",
    "print(f\"Tool call: {c.choices[0].message.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that ChatGPT does not (and cannot) perform the calculation directly (because it does not have direct access to the function), but it tells us to call function `multiply` with `arguments='{\"a\":6574,\"b\":9132}`.\n",
    "\n",
    "Re-using another function from [Jeremy Howard's Hacker's Guide](https://github.com/fastai/lm-hackers/blob/main/lm-hackers.ipynb) [3], let's call the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "funcs_ok = {'multiply'}\n",
    "\n",
    "def call_func(c):\n",
    "    \"\"\"Calls a function based on LLM tool calls\"\"\"\n",
    "    fc = c.choices[0].message.tool_calls[0].function #Updated\n",
    "    if fc.name not in funcs_ok: return print(f'Not allowed: {fc.name}')\n",
    "    f = globals()[fc.name]\n",
    "    return f(**json.loads(fc.arguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60033768"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_func(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, this result is correct, but we are still missing an essential piece: We need to send back the result to ChatGPT so that we can continue chatting. Therefore, let's integrate function calling into our chat client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Function Calling to Chat Client\n",
    "\n",
    "This section will be fast, adding in quite a bit of code, but do not worry, we will recap later to understand the underlying details.\n",
    "\n",
    "First, we need to be able to pass the tools to the chat client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.utils import * #for importing patch\n",
    "\n",
    "@patch    \n",
    "def __init__(self:ChatClient, system_message, tools=None):\n",
    "    \"\"\"Initializes the Chat with the system message.\"\"\"\n",
    "    self._chat_messages = ChatMessages()\n",
    "    self._chat_messages.append_system_message(system_message)\n",
    "    self._tools = tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After prompting ChatGPT with tools, it might return a tool call as we have seen. This tool call needs to be stored in the chat history. Therefore, we need to update method `append_assistant_message`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch \n",
    "def append_assistant_message(self:ChatMessages, content=None, tool_calls=None):\n",
    "    \"\"\"Appends an assistant message with specified content to messages list.\"\"\"\n",
    "    if content:\n",
    "        self._append_message(\"assistant\", content)\n",
    "    else:\n",
    "        self._messages.append({\"role\": \"assistant\", \"tool_calls\": tool_calls})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to update the `ask_gpt` method so that we can store the new format of the assistant message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def get_model_response(self:ChatClient):\n",
    "    \"\"\"Calls the LLM chat completion API\"\"\"\n",
    "    return chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=self._chat_messages.get_messages(),\n",
    "        tools=self._tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def ask_gpt(self:ChatClient, prompt):\n",
    "    \"\"\"Calls the LLM chat completion API and returns the response message\"\"\"\n",
    "    self._chat_messages.append_user_message(prompt)\n",
    "\n",
    "    c = self.get_model_response()\n",
    "\n",
    "    self._chat_messages.append_assistant_message(\n",
    "        content=c.choices[0].message.content,\n",
    "        tool_calls=c.choices[0].message.tool_calls)\n",
    "\n",
    "    return c.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. When we run the calculation again, we can observe that ChatGPT does not return a normal chat message, but it returns a tool call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_S1NinsciYzZtnHGDUKDuZ9UK', function=Function(arguments='{\"a\":6574,\"b\":9132}', name='multiply'), type='function')])\n"
     ]
    }
   ],
   "source": [
    "chat_client = ChatClient(\"You are a calculator.\", tools=[get_schema(multiply)])\n",
    "model_response = chat_client.ask_gpt(\"What is 6574 * 9132?\")\n",
    "print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add in functionality to call the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def call_tool(self:ChatClient, tool_call):\n",
    "    \"\"\"returns the result of an LLM tool call\"\"\"\n",
    "    fc = tool_call.function #Updated\n",
    "    if fc.name not in funcs_ok: return print(f'Not allowed: {fc.name}')\n",
    "    f = globals()[fc.name]\n",
    "    return f(**json.loads(fc.arguments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the tool call [needs to be stored](https://platform.openai.com/docs/guides/function-calling) in a [`tool`-message](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages), so let's add the `append_tool_message`-method to the chat message class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch \n",
    "def append_tool_message(self:ChatMessages, content, tool_call_id):\n",
    "    \"\"\"Appends a tool message with specified content to messages list.\"\"\"\n",
    "    self._messages.append({\"role\": \"tool\", \"content\": content, \"tool_call_id\": tool_call_id})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, ChatGPT might return more than one tool - note that the tools are not an object, but an array. The `call_tools` method processes tools by appending the tool message to the chat. Each of these tool messages contains the tool result as `content` and the `tool_call_id`. Once all tools are processed, we call ChatGPT again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def call_tools(self:ChatClient, tool_calls):\n",
    "    \"\"\"Processes the tool calls of the LLM response and calls the LLM API again\"\"\"\n",
    "    for tool_call in tool_calls:\n",
    "        chat_client._chat_messages.append_tool_message(\n",
    "            content=str(self.call_tool(tool_call)),\n",
    "            tool_call_id=tool_call.id)\n",
    "        \n",
    "    self.ask_gpt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling ChatGPT directly after the tool calls might look strange. The reason for doing this is code simplicity. This way we can re-write the `ask_gpt`-method to recursively process tools until all tools are processed. Why this is a good idea will become obvious a little later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch \n",
    "def get_last_assistant_message(self:ChatMessages):\n",
    "    \"\"\"Returns the content of the last assistant message\"\"\"\n",
    "    return self._messages[-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "@patch\n",
    "def ask_gpt(self:ChatClient, prompt=None):\n",
    "    \"\"\"Calls the LLM chat completion API and returns the response message\"\"\"\n",
    "    \n",
    "    if prompt:\n",
    "        self._chat_messages.append_user_message(prompt)\n",
    "\n",
    "    c = self.get_model_response()\n",
    "    content = c.choices[0].message.content\n",
    "    tool_calls = c.choices[0].message.tool_calls\n",
    "\n",
    "    self._chat_messages.append_assistant_message(\n",
    "        content=content,\n",
    "        tool_calls=tool_calls)\n",
    "    \n",
    "    if tool_calls:\n",
    "        self.call_tools(tool_calls)\n",
    "\n",
    "    return Markdown(self._chat_messages.get_last_assistant_message())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a lot of code. The reward is that we can now run multiplication in the chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "6574 * 9132 = 60,033,768"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = \"You are a calculator. Respond in Markdown, no LaTeX\"\n",
    "chat_client = ChatClient(system_message=system_prompt, tools=[get_schema(multiply)])\n",
    "chat_client.ask_gpt(\"What is 6574 * 9132?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Function Calling in Chat Client\n",
    "\n",
    "Since the previous section was very fast, let's review what actually happened when multiply $6574 \\times 9132$ by inspecting the chat messages which were sent. Stating the obvious: More messages have been exchanged than what the chat client showed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "@patch \n",
    "def get_debug_view(self: ChatMessages):\n",
    "    \"\"\"Returns the debug view of the chat messages formatted as Markdown.\"\"\"\n",
    "    debug_view = []\n",
    "    for message in self._messages:\n",
    "        role = message.get('role')\n",
    "        content = message.get('content', '')\n",
    "\n",
    "        if role == 'system' or role == 'user':\n",
    "            debug_view.append(f\"**{role}**: {content}\\n\")\n",
    "\n",
    "        elif role == 'assistant':\n",
    "            if 'tool_calls' in message:\n",
    "                debug_view.append(\"**tool calls**\\n\")\n",
    "                for i, tool_call in enumerate(message['tool_calls'], start=1):\n",
    "                    function_name = tool_call.function.name\n",
    "                    arguments = tool_call.function.arguments\n",
    "                    tool_call_id = tool_call.id\n",
    "                    debug_view.append(f\"{i}. tool: {function_name}: {arguments} (tool call id: {tool_call_id})\\n\")\n",
    "            else:\n",
    "                debug_view.append(f\"**assistant**: {content}\\n\")\n",
    "\n",
    "        elif role == 'tool':\n",
    "            tool_call_id = message.get('tool_call_id', '')\n",
    "            debug_view.append(f\"**tool result**: {content} (tool call id: {tool_call_id})\\n\")\n",
    "\n",
    "    return Markdown('\\n'.join(debug_view))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**system**: You are a calculator. Respond in Markdown, no LaTeX\n",
       "\n",
       "**user**: What is 6574 * 9132?\n",
       "\n",
       "**tool calls**\n",
       "\n",
       "1. tool: multiply: {\"a\":6574,\"b\":9132} (tool call id: call_7Jh50u0EyZzPhhxwjgCmV7c4)\n",
       "\n",
       "**tool result**: 60033768 (tool call id: call_7Jh50u0EyZzPhhxwjgCmV7c4)\n",
       "\n",
       "**assistant**: 6574 * 9132 = 60,033,768\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_client._chat_messages.get_debug_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the system prompt _(\"You are a calculator. Respond in Markdown, no LaTeX\")_ and the user prompt _(\"What is 6574 * 9132?\")_, ChatGPT realized that it should not attempt the calculation on its own, but it should call a tool. Hence, it returned a tool call to `multiply` with `arguments='{\"a\":6574,\"b\":9132}`. We called the function and returned the result _(\"60033768\")_ as a `tool`-message. As a result, ChatGPT returned the final assistant message _(\"The result of 6574 * 9132 is 60,033,768.\")_. In the chat client, we \"suppressed\" the tool call messages and only displayed the final `assistant`-message.\n",
    "\n",
    "Do you remember that we implemented a recursive tool call? The reason for this is to handle more complex calculations which require more steps, for example this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325513601028"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6573 * 9132 * 5423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The values are:\n",
       "\n",
       "6573 * 9132 = 60024636\n",
       "\n",
       "9132 * 5423 = 49522836\n",
       "\n",
       "So, (6573 * 9132) * 5423 = 60024636"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_client = ChatClient(system_message=system_prompt, tools=[get_schema(multiply)])\n",
    "chat_client.ask_gpt(\"What is ( 6573 * 9132 ) * 5423?\") #The additional brackets reduce the number is tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**system**: You are a calculator. Respond in Markdown, no LaTeX\n",
       "\n",
       "**user**: What is ( 6573 * 9132 ) * 5423?\n",
       "\n",
       "**tool calls**\n",
       "\n",
       "1. tool: multiply: {\"a\": 6573, \"b\": 9132} (tool call id: call_qZQr43OS0K17sJmweeqpp7LZ)\n",
       "\n",
       "2. tool: multiply: {\"a\": 9132, \"b\": 5423} (tool call id: call_HdvwojOVjKGj8vFFo6ciBuFk)\n",
       "\n",
       "**tool result**: 60024636 (tool call id: call_qZQr43OS0K17sJmweeqpp7LZ)\n",
       "\n",
       "**tool result**: 49522836 (tool call id: call_HdvwojOVjKGj8vFFo6ciBuFk)\n",
       "\n",
       "**assistant**: The values are:\n",
       "\n",
       "6573 * 9132 = 60024636\n",
       "\n",
       "9132 * 5423 = 49522836\n",
       "\n",
       "So, (6573 * 9132) * 5423 = 60024636\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_client._chat_messages.get_debug_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve $6573 \\times 9132 \\times 5423$, ChatGPT needed to return 2 sets of tool calls: First it calculates $6573 \\times 9132$, and, strangely, it also calculates $5423 \\times 1$. In the second tool call it calculates $60024636 \\times 5423$. It is really interesting to observe how ChatGPT iteratively solved the problem before returning the final assistant message.\n",
    "\n",
    "Taking a step back, this iterative way of calling tools is called \"ReAct\". Let's explore the theoretical foundations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ReAct Paper\n",
    "\n",
    "The [ReAct paper](https://react-lm.github.io/) [4] introduced a framework that combines **Re**asoning and **Act**ing (\"ReAct\") to enhance the capabilities of LLMs. The core idea of the ReAct framework is to run reasoning and acting in a cyclical process. This means that the LLM does not simply produce an output based on an input, but rather it reasons about the task, determines the actions needed, performs these actions, and incorporates the results into its reasoning process. This loop continues until the task is completed.\n",
    "\n",
    "Today's LLMs are remarkably intelligent and can reason about the prompts they are tasked to perform. Function calling gives the LLM the capabilities to act and perform specific actions. In our setup, the tools are the arithmetical operations, and the LLM needs to reason about the sequence to run the calculations. These reasoning capabilities and the knowledge about how to perform the task are knowledge the LLM has acquired during its training phase. It knows that multiplication or division needs to be done before addition or subtraction. It also understands how brackets signal the sequence of calculations. For example, when calculating $(6573 + 1) \\times 9132$, the LLM must first reason that it needs to perform addition, then multiplication. Based on this reasoning, it acts by calling the respective tools in the correct sequence.\n",
    "\n",
    "The following chart summarizes this framework based on what we have seen so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "sequenceDiagram\n",
    "    autonumber\n",
    "\n",
    "    actor User\n",
    "    participant Function\n",
    "    participant ChatClient as Chat Client\n",
    "    participant LLM\n",
    "\n",
    "    User->>Function: Define the function\n",
    "    Function->>User: Retrieve JSON function definition (via get_schema)\n",
    "    User->>ChatClient: Create Chat Client including tool(s)\n",
    "    User->>ChatClient: Send prompt (via ask_gpt)\n",
    "    ChatClient->>LLM: Send prompt with tool(s)\n",
    "\n",
    "    loop Reasoning and Acting\n",
    "        LLM->>LLM: Reasoning: Analyze prompt and tools\n",
    "        LLM->>ChatClient: Acting: Generate tool call(s)\n",
    "        ChatClient->>Function: Call the function(s) (via call_tools / call_tool)\n",
    "        Function->>ChatClient: Return result(s)\n",
    "        ChatClient->>LLM: Acting: Pass on result(s)\n",
    "        LLM->>LLM: Reasoning: Incorporate result(s) and continue reasoning\n",
    "    end\n",
    "\n",
    "    LLM->>ChatClient: Return final result\n",
    "    ChatClient->>User: Output final result\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented the ReAct framework not only by providing the tools to the LLM but also by allowing recursive processing of tool calls in our chat client. This way, the chat client can handle multiple tool calls it receives from the LLM, ensuring that the LLM can continue reasoning and acting until the task is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Calculator\n",
    "\n",
    "Now that we know how function calling works, and we understand how the LLM uses tools by reasoning and acting, it is time to create the full calculator.\n",
    "\n",
    "The only thing we need to do is to define more functions the LLM can use as tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: float, b: float = 1.0):\n",
    "    \"Adds a + b\"\n",
    "    return a + b\n",
    "\n",
    "def subtract(a: float, b: float = 1.0):\n",
    "    \"Subtracts a - b\"\n",
    "    return a - b\n",
    "\n",
    "def multiply(a: float, b: float = 1.0):\n",
    "    \"Multiplies a * b\"\n",
    "    return a * b\n",
    "\n",
    "def divide(a: float, b: float = 1.0):\n",
    "    \"Divides a / b\"\n",
    "    if b == 0:\n",
    "        return \"Division by zero is not allowed.\"\n",
    "    return a / b\n",
    "\n",
    "funcs_ok = {'add', 'subtract', 'multiply', 'divide'}\n",
    "\n",
    "def get_calc_tools():\n",
    "    return [get_schema(add), get_schema(subtract), get_schema(multiply), get_schema(divide)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the intersting definition of the `divide`-function. Traditionally, you would expect a definition like this:\n",
    "\n",
    "```python\n",
    "def divide(a: float, b: float = 1.0) -> float:\n",
    "    \"Divides a / b\"\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Division by zero is not allowed.\")\n",
    "    return a / b\n",
    "```\n",
    "\n",
    "In the context of LLM function calling, however, raising a `ValueError` is not very useful, because the LLM needs to receive the result in the `tool`-message. Therefore, returning a string `\"Division by zero is not allowed.\"` is more useful.\n",
    "\n",
    "Let's run a first test: $(6573 + 1) \\times 9132$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a calculator. \\n\"\n",
    "    \"Do not do even the simplest computations on your own, \\n\"\n",
    "    \"but use the tools provided. \\n\"\n",
    "    \"After the tool calls, explain the steps you took when answering. \\n\"\n",
    "    \"Answer with an accuracy of 3 decimals. \\n\"\n",
    "    \"Respond in markdown, no LaTeX.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected result: 60033768\n",
      "LLM response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The calculation steps are as follows:\n",
       "1. First, add 6573 and 1 to get 6574.\n",
       "2. Then, multiply 6574 by 9132 to get 60,033,768.\n",
       "\n",
       "So, (6573 + 1) * 9132 = 60,033,768."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "print(f\"Expected result: {(6573 + 1) * 9132}\")\n",
    "print(f\"LLM response:\")\n",
    "chat_client = ChatClient(system_message=system_prompt, tools=get_calc_tools())\n",
    "chat_client.ask_gpt(\"What is (6573 + 1) * 9132?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more detailed we, we can inspect the chat messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**system**: You are a calculator. \n",
       "Do not do even the simplest computations on your own, \n",
       "but use the tools provided. \n",
       "After the tool calls, explain the steps you took when answering. \n",
       "Answer with an accuracy of 3 decimals. \n",
       "Respond in markdown, no LaTeX.\n",
       "\n",
       "**user**: What is (6573 + 1) * 9132?\n",
       "\n",
       "**tool calls**\n",
       "\n",
       "1. tool: add: {\"a\": 6573, \"b\": 1} (tool call id: call_et4e3TZcl14w920hmanHPuTM)\n",
       "\n",
       "2. tool: multiply: {\"a\": 6574, \"b\": 9132} (tool call id: call_ZnP6lnKaXlANjz9SDjfbVvjs)\n",
       "\n",
       "**tool result**: 6574 (tool call id: call_et4e3TZcl14w920hmanHPuTM)\n",
       "\n",
       "**tool result**: 60033768 (tool call id: call_ZnP6lnKaXlANjz9SDjfbVvjs)\n",
       "\n",
       "**assistant**: The calculation steps are as follows:\n",
       "1. First, add 6573 and 1 to get 6574.\n",
       "2. Then, multiply 6574 by 9132 to get 60,033,768.\n",
       "\n",
       "So, (6573 + 1) * 9132 = 60,033,768.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_client._chat_messages.get_debug_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a final, more complicated example: $(( 5647 + 3241 ) / ( 7 \\times 2 )) - 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected result: 633.8571428571429\n",
      "LLM response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The result of the expression \\(( ( 5647 + 3241 ) / ( 7 * 2 ) ) - 1\\) is \\(633.857\\).\n",
       "\n",
       "Steps taken:\n",
       "1. Added \\(5647 + 3241\\) to get \\(8888\\).\n",
       "2. Multiplied \\(7 * 2\\) to get \\(14\\).\n",
       "3. Divided \\(8888 / 14\\) to get \\(634.857\\).\n",
       "4. Subtracted \\(1\\) from \\(634.857\\) to get \\(633.857\\)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "print(f\"Expected result: {( ( 5647 + 3241 ) / ( 7 * 2 ) ) - 1}\")\n",
    "print(f\"LLM response:\")\n",
    "chat_client = ChatClient(system_message=system_prompt, tools=get_calc_tools())\n",
    "chat_client.ask_gpt(\"What is ( ( 5647 + 3241 ) / ( 7 * 2 ) ) - 1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**system**: You are a calculator. \n",
       "Do not do even the simplest computations on your own, \n",
       "but use the tools provided. \n",
       "After the tool calls, explain the steps you took when answering. \n",
       "Answer with an accuracy of 3 decimals. \n",
       "Respond in markdown, no LaTeX.\n",
       "\n",
       "**user**: What is ( ( 5647 + 3241 ) / ( 7 * 2 ) ) - 1?\n",
       "\n",
       "**tool calls**\n",
       "\n",
       "1. tool: add: {\"a\": 5647, \"b\": 3241} (tool call id: call_eIvBYclF1XReEpx8zxnoQGjU)\n",
       "\n",
       "2. tool: multiply: {\"a\": 7, \"b\": 2} (tool call id: call_MWiGRfO1cpv7smie1rhXeCm2)\n",
       "\n",
       "**tool result**: 8888 (tool call id: call_eIvBYclF1XReEpx8zxnoQGjU)\n",
       "\n",
       "**tool result**: 14 (tool call id: call_MWiGRfO1cpv7smie1rhXeCm2)\n",
       "\n",
       "**tool calls**\n",
       "\n",
       "1. tool: divide: {\"a\":8888,\"b\":14} (tool call id: call_s1tal77M3YGYrsud0Y0pLX1f)\n",
       "\n",
       "**tool result**: 634.8571428571429 (tool call id: call_s1tal77M3YGYrsud0Y0pLX1f)\n",
       "\n",
       "**tool calls**\n",
       "\n",
       "1. tool: subtract: {\"a\":634.857,\"b\":1} (tool call id: call_cGZ15kGFQ20ztFZqiUve3GOS)\n",
       "\n",
       "**tool result**: 633.857 (tool call id: call_cGZ15kGFQ20ztFZqiUve3GOS)\n",
       "\n",
       "**assistant**: The result of the expression \\(( ( 5647 + 3241 ) / ( 7 * 2 ) ) - 1\\) is \\(633.857\\).\n",
       "\n",
       "Steps taken:\n",
       "1. Added \\(5647 + 3241\\) to get \\(8888\\).\n",
       "2. Multiplied \\(7 * 2\\) to get \\(14\\).\n",
       "3. Divided \\(8888 / 14\\) to get \\(634.857\\).\n",
       "4. Subtracted \\(1\\) from \\(634.857\\) to get \\(633.857\\).\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_client._chat_messages.get_debug_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the models capable of?\n",
    "\n",
    "Before closing this blog post, we need to discuss which model can safely be used for the calculator job. After all, we put a lot of trust into the model do get the job done. If the model would not run the tools in the right sequence, the result would be incorrect. So let's create a mini-series of tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 6574 * 9132 ) = 60033768.000\n",
      "( 6573 + 9132 ) * 5423 = 85168215.000\n",
      "( 6573 * 9132 ) * 5423 = 325513601028.000\n",
      "( 5647 + 3241 ) / ( 7 * 2 ) - 1  = 633.857\n",
      "( 5647 + 3241 ) / ( 7 * 2 ) + ( ( 657 + 343 ) * 2 ) = 2634.857\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "models = [\"gpt-3.5-turbo\", \"gpt-4o-mini\", \"gpt-4o\"]\n",
    "calculations = [\n",
    "    \"( 6574 * 9132 )\",\n",
    "    \"( 6573 + 9132 ) * 5423\",\n",
    "    \"( 6573 * 9132 ) * 5423\",\n",
    "    \"( 5647 + 3241 ) / ( 7 * 2 ) - 1 \",\n",
    "    \"( 5647 + 3241 ) / ( 7 * 2 ) + ( ( 657 + 343 ) * 2 )\" ]\n",
    "\n",
    "results = []\n",
    "\n",
    "for calculation in calculations:\n",
    "    try:\n",
    "        result = eval(calculation)\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        results.append(f\"Error in calculation: {e}\")\n",
    "\n",
    "for calc, res in zip(calculations, results):\n",
    "    print(f\"{calc} = {res:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ddb12 th {\n",
       "  background-color: white;\n",
       "  color: black;\n",
       "  border: 1px solid black;\n",
       "  text-align: center;\n",
       "  vertical-align: middle;\n",
       "}\n",
       "#T_ddb12 td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_ddb12 caption {\n",
       "  caption-side: top;\n",
       "  font-size: 1.25em;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_ddb12_row0_col0, #T_ddb12_row0_col1, #T_ddb12_row0_col2, #T_ddb12_row1_col0, #T_ddb12_row1_col1, #T_ddb12_row1_col2, #T_ddb12_row2_col0, #T_ddb12_row2_col2, #T_ddb12_row3_col0, #T_ddb12_row3_col1, #T_ddb12_row3_col2, #T_ddb12_row4_col1, #T_ddb12_row4_col2 {\n",
       "  color: green;\n",
       "  background-color: white;\n",
       "  text-align: center;\n",
       "  vertical-align: middle;\n",
       "}\n",
       "#T_ddb12_row2_col1, #T_ddb12_row4_col0 {\n",
       "  color: red;\n",
       "  background-color: white;\n",
       "  text-align: center;\n",
       "  vertical-align: middle;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ddb12\">\n",
       "  <caption>Model Comparison Results</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ddb12_level0_col0\" class=\"col_heading level0 col0\" >gpt-3.5-turbo</th>\n",
       "      <th id=\"T_ddb12_level0_col1\" class=\"col_heading level0 col1\" >gpt-4o-mini</th>\n",
       "      <th id=\"T_ddb12_level0_col2\" class=\"col_heading level0 col2\" >gpt-4o</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Calculation</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ddb12_level0_row0\" class=\"row_heading level0 row0\" >( 6574 * 9132 )</th>\n",
       "      <td id=\"T_ddb12_row0_col0\" class=\"data row0 col0\" >True</td>\n",
       "      <td id=\"T_ddb12_row0_col1\" class=\"data row0 col1\" >True</td>\n",
       "      <td id=\"T_ddb12_row0_col2\" class=\"data row0 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ddb12_level0_row1\" class=\"row_heading level0 row1\" >( 6573 + 9132 ) * 5423</th>\n",
       "      <td id=\"T_ddb12_row1_col0\" class=\"data row1 col0\" >True</td>\n",
       "      <td id=\"T_ddb12_row1_col1\" class=\"data row1 col1\" >True</td>\n",
       "      <td id=\"T_ddb12_row1_col2\" class=\"data row1 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ddb12_level0_row2\" class=\"row_heading level0 row2\" >( 6573 * 9132 ) * 5423</th>\n",
       "      <td id=\"T_ddb12_row2_col0\" class=\"data row2 col0\" >True</td>\n",
       "      <td id=\"T_ddb12_row2_col1\" class=\"data row2 col1\" >False</td>\n",
       "      <td id=\"T_ddb12_row2_col2\" class=\"data row2 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ddb12_level0_row3\" class=\"row_heading level0 row3\" >( 5647 + 3241 ) / ( 7 * 2 ) - 1 </th>\n",
       "      <td id=\"T_ddb12_row3_col0\" class=\"data row3 col0\" >True</td>\n",
       "      <td id=\"T_ddb12_row3_col1\" class=\"data row3 col1\" >True</td>\n",
       "      <td id=\"T_ddb12_row3_col2\" class=\"data row3 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ddb12_level0_row4\" class=\"row_heading level0 row4\" >( 5647 + 3241 ) / ( 7 * 2 ) + ( ( 657 + 343 ) * 2 )</th>\n",
       "      <td id=\"T_ddb12_row4_col0\" class=\"data row4 col0\" >False</td>\n",
       "      <td id=\"T_ddb12_row4_col1\" class=\"data row4 col1\" >True</td>\n",
       "      <td id=\"T_ddb12_row4_col2\" class=\"data row4 col2\" >True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a calculator. \\n\"\n",
    "    \"Do not do even the simplest computations on your own, \\n\"\n",
    "    \"but use the tools provided. \\n\"\n",
    "    \"Answer with an accuracy of 3 decimals. \\n\"\n",
    "    \"Strictly only respond only with the result of the calculation, just one number, no additional text.\"\n",
    ")\n",
    "\n",
    "# Store results in a dictionary\n",
    "results = {calc: {} for calc in calculations}\n",
    "\n",
    "# Run calculations for each model\n",
    "for model in models:\n",
    "    model_name = model\n",
    "    for calculation in calculations:\n",
    "        chat_client = ChatClient(system_message=system_prompt, tools=get_calc_tools())\n",
    "        result = chat_client.ask_gpt(calculation)\n",
    "        result_number = float(result.data)\n",
    "        actual_result = eval(calculation)\n",
    "        results[calculation][model] = abs(result_number - actual_result) < 0.001\n",
    "\n",
    "# Create a dataframe for the results\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results.columns = models\n",
    "df_results.index.name = 'Calculation'\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Style the DataFrame for better visualization\n",
    "def color_true_false(val):\n",
    "    color = 'green' if val else 'red'\n",
    "    return f'color: {color}; background-color: white'\n",
    "\n",
    "styled_df = df_results.style \\\n",
    "    .map(lambda x: 'color: green; background-color: white' if x else 'color: red; background-color: white') \\\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('background-color', 'white'), ('color', 'black'), ('border', '1px solid black'), ('text-align', 'center'), ('vertical-align', 'middle')]},\n",
    "        {'selector': 'td', 'props': [('border', '1px solid black')]},\n",
    "        {'selector': 'caption', 'props': [('caption-side', 'top'), ('font-size', '1.25em'), ('font-weight', 'bold')]}\n",
    "    ]) \\\n",
    "    .set_caption(\"Model Comparison Results\") \\\n",
    "    .set_properties(**{'text-align': 'center', 'vertical-align': 'middle'})\n",
    "\n",
    "html = styled_df.to_html()\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the test several times, you can notice that the results vary. Mostly the models reason correctly. Some calculations, however, seem to be difficult. The incorrect results 99% of the time only show up in the left columns. GPT-4o feels like a reliable partner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this blog post, we successfully turned an LLM into a reliable calculator. While this exercise might seem somewhat academic (every real calculator is faster by several orders of magnitude), it provides valuable insights into enhancing LLM capabilities through tool integration.\n",
    "\n",
    "We learned how to provide tools to an LLM to perform reliable calculations, essentially grounding the LLM in math. We achieved this by giving the LLM access to Python functions for arithmetic operations. Our approach was very generic, using the `get_schema` function to extract the definitions of the Python functions and processing the tools with the `call_tools`-method. Beyond the scope of the calculator, using this approach, we could expose any function to the LLM, such as sending emails or accessing databases.\n",
    "\n",
    "We also explored the ReAct framework, transforming the LLM into an agent that combines reasoning and acting. This significantly enhanced its capabilities. The LLM reasoned about the calculations we asked it to do, and it acted by running the corresponding Python functions until it calculated the final result.\n",
    "\n",
    "Implementing this calculator is fundamentally different from traditional approaches because we did not code any mathematical logic ourselves. We only defined the basic mathematical functions, exposed them to the LLM, and trusted its intelligence to perform the calculations. This required a leap of faith, and we saw that not all models consistently get the calculations right. Using an advanced model like GPT-4o, however, demonstrates the potential of LLMs to handle complex operations independently.\n",
    "\n",
    "As the intelligence of future models likely increases significantly, it will become an essential skill to judge the level of faith we can safely place in an LLM. Understanding which tasks an LLM can perform without specific guidance and where we need either human support or classical code-based solutions is essential. Making accurate assessments will not only allow us to push the boundaries of what is possible with LLMs but can also lead to significant reductions in the complexity of traditional IT systems. However, pushing these boundaries must be done responsibly, ensuring that we balance innovation with safety and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Karpathy, A. (2024). [Let's build the GPT Tokenizer](https://youtu.be/zduSFxRajkE?si=eOf5uPqkOuKLELPe) by Andrej Karpathy\n",
    "\n",
    "[2] Karpathy, A. (2023). [Intro to Large Language Models](https://youtu.be/zjkBMFhNj_g?si=22yHD6Y0j7hI28sc)\n",
    "\n",
    "[3] Howard, J. (2023). [A Hackers' Guide to Language Models](https://youtu.be/jkrNMKz9pWU?si=88WgZx2u3HaldCgj)\n",
    "\n",
    "[4] Yao, S., Yu, T., Wu, Y., Zhao, Z., Yu, K., & Liu, S. (2022). [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9da906b64701e68312bc07fbc15a3a13814f930718c2c6b0e41a29d035806a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
