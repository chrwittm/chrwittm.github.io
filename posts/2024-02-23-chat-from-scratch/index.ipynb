{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Building Chat for Jupyter Notebooks from Scratch\"\n",
    "author: \"Christian Wittmann\"\n",
    "date: \"2024-02-23\"\n",
    "categories: [chat, jupyter, llama.cpp, llm]\n",
    "image: \"llama-lets-chat.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a light-weight chat for llama2 from scratch which can be reused in your Jupyter notebooks.\n",
    "\n",
    "Working exploratively with large language models (LLMs), I wanted to not only send prompts to LLMs, but to chat with the LLM from within my Jupyter notebook. It turns out, that building a chat from scratch is not complicated. While I use a [local llama2 model](https://chrwittm.github.io/posts/2024-02-15-running-llama2-on-mac/) in this notebook, the concepts I describe are universal and also transfer to other implementations.\n",
    "\n",
    "Before we get started: If you want to interactively run through this blog post, please check out the [corresponding Jupyter notebook](https://github.com/chrwittm/lm-hackers/blob/main/20-local-llama-on-mac/30-notebook-chat.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    text-align: center;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"llama-lets-chat.png\" alt=\"Dalle: A clean developer's desk with a laptop. A friendly llama peeking out of the screen wanting. Insert a 'Let's chat!'-speech bubble to chat with the spectator of the image.\" style=\"width:50%;\">\n",
    "    <figcaption>Dalle: A clean developer's desk with a laptop. A friendly llama peeking out of the screen wanting. Insert a 'Let's chat!'-speech bubble to chat with the spectator of the image.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Chat Messages\n",
    "\n",
    "\"Chat Messages\" are the messages you exchange with the LLM. There are 3 roles:\n",
    "\n",
    "- The `system`-message gives overall instructions to the LLM which should be used during the whole chat. It only appears once.\n",
    "- `User`-messages are the messages (\"prompts\") you send to the LLM.\n",
    "- The LLM replies with `assistant`-messages.\n",
    "\n",
    "The chat messages need to be passed to the LLM via the API in a [JSON format](chat-messages.json). Since the chat is stateless, you need to pass the whole previous conversation to be able to ask follow-up questions. The following animated GIF shows the structure in analogy to ChatGPT. Additionally, it animates how the messages build up over time during the chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Chat Messages](chat-messages-thumb.png)](chat-messages.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on that, let's build a class which contains and manages the chat messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatMessages:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the Chat.\"\"\"\n",
    "        self._messages = []\n",
    "\n",
    "    def _append_message(self, role, content):\n",
    "        \"\"\"Appends a message with specified role and content to messages list.\"\"\"\n",
    "        self._messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def append_system_message(self, content):\n",
    "        \"\"\"Appends a system message with specified content to messages list.\"\"\"\n",
    "        self._append_message(\"system\", content)\n",
    "\n",
    "    def append_user_message(self, content):\n",
    "        \"\"\"Appends a user message with specified content to messages list.\"\"\"\n",
    "        self._append_message(\"user\", content)\n",
    "\n",
    "    def append_assistant_message(self, content):\n",
    "        \"\"\"Appends an assistant message with specified content to messages list.\"\"\"\n",
    "        self._append_message(\"assistant\", content)\n",
    "    \n",
    "    def get_messages(self):\n",
    "        \"\"\"Returns a shallow copy of the messages list.\"\"\"\n",
    "        return self._messages[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 methods `append_system_message`, `append_user_message`, and `append_assistant_message` call the private method `_append_message` so that there is no confusion as to which message types can be added. The `get_messages`-method returns a copy of the chat messages, making sure that it is not possible to access the private `_messages` attribute of `ChatMessages`.\n",
    "\n",
    "Let's quickly re-create the example shown on the image above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages = ChatMessages()\n",
    "chat_messages.append_system_message(\"For \\\"Question n\\\", please respond with \\\"Response n\\\"\")\n",
    "chat_messages.append_user_message(\"Question 1\")\n",
    "chat_messages.append_assistant_message(\"Response 1\")\n",
    "chat_messages.append_user_message(\"Question 2\")\n",
    "chat_messages.append_assistant_message(\"Response 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"role\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"system\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"content\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"For \\\"Question n\\\", please respond with \\\"Response n\\\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"role\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"user\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"content\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Question 1\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"role\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"assistant\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"content\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Response 1\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"role\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"user\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"content\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Question 2\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"role\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"assistant\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"content\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Response 2\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "]\u001b[37m\u001b[39;49;00m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pygments import highlight\n",
    "from pygments.lexers import JsonLexer\n",
    "from pygments.formatters import TerminalFormatter\n",
    "\n",
    "# Convert messages to a formatted JSON string\n",
    "json_str = json.dumps(chat_messages.get_messages(), indent=4)\n",
    "\n",
    "# Highlight the JSON string to add colors\n",
    "print(highlight(json_str, JsonLexer(), TerminalFormatter()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Chat Version 1\n",
    "\n",
    "We have seen in my [previous blog post](https://chrwittm.github.io/posts/2024-02-15-running-llama2-on-mac/) how we can prompt the llama2 model by calling the `create_chat_completion`-method.\n",
    "\n",
    "The class `Llama2ChatVersion2` simplifies prompting the LLM by doing the following:\n",
    "\n",
    "- Upon initialization of the chat object, the chat messages are initialized and the system message is added.\n",
    "- Whe prompting llama2, both the prompt (the user message) and the response (the assistant message) are added to the chat messages.\n",
    "- The plain text returned from llama2 is formatted for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, clear_output\n",
    "\n",
    "class Llama2ChatVersion2:\n",
    "\n",
    "    def __init__(self, llm, system_message):\n",
    "        \"\"\"Initializes the Chat with the system message.\"\"\"\n",
    "        self._llm = llm\n",
    "        self._chat_messages = ChatMessages()\n",
    "        self._chat_messages.append_system_message(system_message)\n",
    "\n",
    "    def _get_llama2_response(self):\n",
    "        \"\"\"Returns Llama2 model response for given messages.\"\"\"\n",
    "        self.model_response = self._llm.create_chat_completion(self._chat_messages.get_messages())\n",
    "        return self.model_response['choices'][0]['message']['content']\n",
    "\n",
    "    def _format_markdown_with_style(self, text, font_size=16):\n",
    "        \"\"\"Wraps text in a <span> with specified font size, defaults to 16.\"\"\"\n",
    "        return f\"<span style='font-size: {font_size}px;'>{text}</span>\"\n",
    "\n",
    "    def prompt_llama2(self, user_prompt):\n",
    "        \"\"\"Processes user prompt, displays Llama2 response formatted in Markdown.\"\"\"\n",
    "        self._chat_messages.append_user_message(user_prompt)\n",
    "        llama2_response = self._get_llama2_response()\n",
    "        self._chat_messages.append_assistant_message(llama2_response)\n",
    "        display(Markdown(self._format_markdown_with_style(llama2_response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a llama2 instance and prompt it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "#llm = Llama(model_path=\"../models/Llama-2-7b-chat/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=2048, verbose=False)\n",
    "llm = Llama(model_path=\"../../../lm-hackers/models/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=2048, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  Sure! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\n",
       "\n",
       "1. Mercury\n",
       "2. Venus\n",
       "3. Earth\n",
       "4. Mars\n",
       "5. Jupiter\n",
       "6. Saturn\n",
       "7. Uranus\n",
       "8. Neptune</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\n",
    "chat.prompt_llama2(\"Name the planets in the solar system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks good, but from a useability perspective it is not great, because you have to wait for the whole response to be completed before you see an output. On my machine this takes a few seconds for this short reply. Especially for longer answers, however, you might start to wonder if the process is running correctly, or is it just the impatient me?\n",
    "\n",
    "In any case, it would be nice to see the model's response streamed, i.e. you see the model writing the answer world-by-word / token-by-token, the same way you are used to seeing ChatGPT print out its answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Streaming Works\n",
    "\n",
    "The following cell contains an illustrative example of how streaming works: The function `mock_llm_stream()` returns a generator object because it does not `return` a result, but it `yield`s results. This means that the code is not executed when the function is called, but it only returns a generator object which lazily returns values when it is iterated over. The `for`-loop iterates over the generator object, and each iteration returns a word after some latency, simulating the token generation by the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example for a text streamed via a generator object. "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def fetch_next_word(words, current_index):\n",
    "    \"\"\"\n",
    "    Mock function to simulate making an API call to fetch the next word from the LLM.\n",
    "    \"\"\"\n",
    "    # Simulate network delay or processing time\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    if current_index < len(words):\n",
    "        return words[current_index]\n",
    "    else:\n",
    "        raise StopIteration(\"End of sentence reached.\")\n",
    "\n",
    "def mock_llm_stream():\n",
    "    sentence = \"This is an example for a text streamed via a generator object.\"\n",
    "    words = sentence.split()\n",
    "    current_index = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Simulate fetching the next word from the LLM\n",
    "            word = fetch_next_word(words, current_index)\n",
    "            yield word\n",
    "            current_index += 1\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "# Capture the generator function in a variable\n",
    "mock_llm_response = mock_llm_stream()\n",
    "\n",
    "# Example of how to use this mock_llm_response\n",
    "for word in mock_llm_response:\n",
    "    print(word, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's stream a response from llama2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\n",
      "\n",
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "4. Mars\n",
      "5. Jupiter\n",
      "6. Saturn\n",
      "7. Uranus\n",
      "8. Neptune"
     ]
    }
   ],
   "source": [
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Answer in a very concise and accurate way\"},\n",
    "    {\"role\": \"user\", \"content\": \"Name the planets in the solar system\"}]\n",
    "\n",
    "model_response = llm.create_chat_completion(messages = messages, stream=True)\n",
    "\n",
    "complete_response = \"\"\n",
    "\n",
    "for part in model_response:\n",
    "    # Check if 'content' key exists in the 'delta' dictionary\n",
    "    if 'content' in part['choices'][0]['delta']:\n",
    "        content = part['choices'][0]['delta']['content']\n",
    "        print(content, end='')\n",
    "        complete_response += content\n",
    "    else:\n",
    "        # Handle the case where 'content' key is not present\n",
    "        # For example, you can print a placeholder or do nothing\n",
    "        # print(\"(no content)\", end='')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Version 2 - Streaming included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include the streaming functionality into our chat messages- and chat-classes. For this we are going to use a nice trick from `fastcore` to add the 2 new methods: We can `@patch` the methods into the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.utils import * #for importing patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch    \n",
    "def _get_llama2_response_stream(self:Llama2ChatVersion2):\n",
    "    \"\"\"Returns generator object for streaming Llama2 model responses for given messages.\"\"\"\n",
    "    return self._llm.create_chat_completion(self._chat_messages.get_messages(), stream=True)\n",
    "\n",
    "@patch\n",
    "def prompt_llama2_stream(self:Llama2ChatVersion2, user_prompt):\n",
    "    \"\"\"Processes user prompt in streaming mode, displays updates in Markdown.\"\"\"\n",
    "    self._chat_messages.append_user_message(user_prompt)\n",
    "    llama2_response_stream = self._get_llama2_response_stream()\n",
    "    \n",
    "    complete_stream = \"\"\n",
    "\n",
    "    for part in llama2_response_stream:\n",
    "        # Check if 'content' key exists in the 'delta' dictionary\n",
    "        if 'content' in part['choices'][0]['delta']:\n",
    "            stream_content = part['choices'][0]['delta']['content']\n",
    "            complete_stream += stream_content\n",
    "\n",
    "            # Clear previous output and display new content\n",
    "            clear_output(wait=True)\n",
    "            display(Markdown(self._format_markdown_with_style(complete_stream)))\n",
    "\n",
    "        else:\n",
    "            # Handle the case where 'content' key is not present\n",
    "            pass\n",
    "    \n",
    "    self._chat_messages.append_assistant_message(complete_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the method `prompt_llama2_stream` to get a more interactive response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  Sure! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\n",
       "\n",
       "1. Mercury\n",
       "2. Venus\n",
       "3. Earth\n",
       "4. Mars\n",
       "5. Jupiter\n",
       "6. Saturn\n",
       "7. Uranus\n",
       "8. Neptune</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\n",
    "chat.prompt_llama2_stream(\"Name the planets in the solar system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for the fun of it, let's continue the chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  Of course! Here are the names of the planets in our solar system in reverse order, from farthest to closest to the Sun:\n",
       "1. Neptune\n",
       "2. Uranus\n",
       "3. Saturn\n",
       "4. Jupiter\n",
       "5. Mars\n",
       "6. Earth\n",
       "7. Venus\n",
       "8. Mercury</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat.prompt_llama2_stream(\"Please reverse the list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  Sure! Here are the names of the planets in our solar system sorted by their mass, from lowest to highest:\n",
       "1. Mercury (0.38 Earth masses)\n",
       "2. Mars (0.11 Earth masses)\n",
       "3. Venus (0.81 Earth masses)\n",
       "4. Earth (1.00 Earth masses)\n",
       "5. Jupiter (29.6 Earth masses)\n",
       "6. Saturn (95.1 Earth masses)\n",
       "7. Uranus (14.5 Earth masses)\n",
       "8. Neptune (17.1 Earth masses)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat.prompt_llama2_stream(\"Please sort the list by the mass of the planet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping back to the beginning, you can see how the chat is represented in the chat massages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'Answer in a very concise and accurate way'},\n",
       " {'role': 'user', 'content': 'Name the planets in the solar system'},\n",
       " {'role': 'assistant',\n",
       "  'content': '  Sure! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\\n\\n1. Mercury\\n2. Venus\\n3. Earth\\n4. Mars\\n5. Jupiter\\n6. Saturn\\n7. Uranus\\n8. Neptune'},\n",
       " {'role': 'user', 'content': 'Please reverse the list'},\n",
       " {'role': 'assistant',\n",
       "  'content': '  Of course! Here are the names of the planets in our solar system in reverse order, from farthest to closest to the Sun:\\n1. Neptune\\n2. Uranus\\n3. Saturn\\n4. Jupiter\\n5. Mars\\n6. Earth\\n7. Venus\\n8. Mercury'},\n",
       " {'role': 'user', 'content': 'Please sort the list by the mass of the planet'},\n",
       " {'role': 'assistant',\n",
       "  'content': '  Sure! Here are the names of the planets in our solar system sorted by their mass, from lowest to highest:\\n1. Mercury (0.38 Earth masses)\\n2. Mars (0.11 Earth masses)\\n3. Venus (0.81 Earth masses)\\n4. Earth (1.00 Earth masses)\\n5. Jupiter (29.6 Earth masses)\\n6. Saturn (95.1 Earth masses)\\n7. Uranus (14.5 Earth masses)\\n8. Neptune (17.1 Earth masses)'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat._chat_messages.get_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this blog post, we implemented an LLM chat from scratch in a very light-weight format. We learned how the chat messages need to be handled to create the chat experience and we even added streaming support.\n",
    "\n",
    "And the best thing, we can re-use this chat functionality in other notebooks without having to re-write it or copy&paste again, keeping our notebooks dry and clean. I have moved the core code of this notebook to a [separate `.py-file`](https://github.com/chrwittm/lm-hackers/blob/main/notebook_chat/notebook_chat.py), and [this notebook](https://github.com/chrwittm/lm-hackers/blob/main/20-local-llama-on-mac/35-notebook-chat-consumer.ipynb) demonstrates how to re-use the notebook chat in another notebook. 😀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
