{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Remembering the Wittmann Tours World Trip with RAG\"\n",
    "author: \"Christian Wittmann\"\n",
    "date: \"2024-03-22\"\n",
    "categories: [rag, llm, nlp]\n",
    "image: \"remembering-world-trip.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back in 2017/2018 my wife and I did a world trip, and we documented it on our blog [Wittmann-Tours.de](https://wittmann-tours.de). These 14 month were among the most exciting times of my life, but nonetheless, I start forgetting details. Therefore, I thought it would be great to have a large language model (LLM) which could help me remember the details.\n",
    "\n",
    "## From Idea to Data\n",
    "\n",
    "How did I come up with this idea? Reworking the Retrieval Augmented Retrieval (RAG) section from the [hackers guide](https://www.youtube.com/watch?v=jkrNMKz9pWU&t=4232s), I decided, I wanted to build my own RAG application, but what would be a good data source? I needed a good data source which fulfilled 2 criteria:\n",
    "\n",
    "- The LLM I used for the exercise should not know the content of the data source.\n",
    "- I should know the content of the data source very well to be able to judge if the LLM, once RAG is implemented, could answer questions related to the data source correctly.\n",
    "\n",
    "Since our blog is pretty low-profile, [Wittmann-Tours.de](https://wittmann-tours.de/) fulfilled all the requirements perfectly, but how can I use it fot this project? As with many machine learning projects, data is the key to success. Therefore, I needed to pause a bit on the RAG implementation and started to work on converting my blog to markdown. This was more difficult than expected, and you can read the [full story in this blog post](https://chrwittm.github.io/posts/2024-03-08-how-to-convert-wordpress-into-markdown/). By now, the Wittmann-Tours blog is available my [Wittmann-Tours GitHub repo](https://github.com/chrwittm/wittmann-tours) under [license CC-BY NC](http://creativecommons.org/licenses/by-nc/4.0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    text-align: center;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"remembering-world-trip.png\" alt=\"Dalle: Remembering the Wittmann Tours World Trip with RAG\" style=\"width:50%;\">\n",
    "    <figcaption>Dalle: Remembering the Wittmann Tours World Trip with RAG</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Retrieval Augmented Generation (RAG)?\n",
    "\n",
    "Before explaining Retrieval Augmented Generation (RAG), let's look at some of the challenges we can typically face with large language models (LLMs):\n",
    "\n",
    "- LLMs can [hallucinate](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)), meaning that they can generate false or misleading information, nonetheless presented in a very plausible sounding way.\n",
    "- LLMs do not have up-to-date information because they have been trained on a fixed corpus of information.\n",
    "- LLMs cannot always give sources. Why? Here's my take: As [Andrej Karpathy pointed out](https://youtu.be/zjkBMFhNj_g?si=mDId720mO2Sf-xF0&t=347), an LLM is a lossy compression of text containing only the gestalt of the text. This translates well to our own way of learning. After you have read a Wikipedia article, you cannot recite the whole article, but you have learned the essence of the article. Similarly, the detailed references are lost during LLM references.\n",
    "\n",
    "> Speaking of sources: IBM Research has published a very accessible [Youtube Video](https://www.youtube.com/watch?v=T-D1OfcDW1M) / [blog post](https://research.ibm.com/blog/retrieval-augmented-generation-RAG) describing key challenges with LLMs and how RAG addresses these issues.\n",
    "\n",
    "RAG addresses these issues by giving additional context to a question/prompt you send to an LLM, therefore addressing the issues above:\n",
    "\n",
    "- When you ask an LLM about a topic, and pass along the related Wikipedia article, the answer is very likely to be factually correct.\n",
    "- You can pass any additional information to the LLM, crossing also into the time after the training cut-off.\n",
    "- Let's assume there is a mechanism to select the appropriate source (more on that later), the LLM can tell you the source of the information it used for the answer.\n",
    "\n",
    "Going back to its origin, RAG was first introduced in a [paper by Facebook AI Research](https://arxiv.org/abs/2005.11401v4) (today's research group of Meta) in 2020 as a mechanism to enhance the capabilities of language models by adding relevant contextual information to the user prompt. As we have seen in the [hacker's guide](https://youtu.be/jkrNMKz9pWU?si=DiTeD1SMswqG1uTw&t=4379), here is a simple way how we can formulate a RAG-prompt:\n",
    "\n",
    "```python\n",
    "prompt_with_context = (\n",
    "    f\"Answer the question with the help of the provided context.\"\n",
    "    f\"## Question\"\n",
    "    f\"{question}\"\n",
    "    f\"## Context\"\n",
    "    f\"{context}\"\n",
    ")\n",
    "```\n",
    "\n",
    "Let's try that out and build mini-framework which enables a Llama2-LLM to answer questions related to the [Wittmann-Tours.de](https://wittmann-tours.de) blog. If you prefer to run all the code yourself, please check out [this notebook on GitHub](https://github.com/chrwittm/lm-hackers/blob/main/30-rag/20-rag-reworking-hackers-guide.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Setup\n",
    "\n",
    "Let me reuse the pattern I developed in the [chat consumer notebook](https://github.com/chrwittm/lm-hackers/blob/main/20-local-llama-on-mac/35-notebook-chat-consumer.ipynb) based on my blog post [Building Chat for Jupyter Notebooks from Scratch](https://chrwittm.github.io/posts/2024-02-23-chat-from-scratch/). This time, let's use the maximum context window size possible for this model (`n_ctx=4096`) to fit as much context as possible into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../../../lm-hackers/models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =   288.00 MiB\n"
     ]
    }
   ],
   "source": [
    "#|output: false\n",
    "import sys\n",
    "sys.path.append('../../../lm-hackers/notebook_chat')\n",
    "from notebook_chat import ChatMessages, Llama2ChatVersion2\n",
    "\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"../../../lm-hackers/models/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=4096, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading The Wittmann-Tours.de dataset\n",
    "\n",
    "The [Wittmann-Tours.de](https://wittmann-tours.de) blog is available for download as a dataset in [the Wittmann-Tours repo](https://github.com/chrwittm/wittmann-tours).\n",
    "\n",
    "```bash\n",
    "wget -P ./wt-blogposts https://github.com/chrwittm/wittmann-tours/raw/main/zip/blogposts-md.zip\n",
    "unzip -o ./wt-blogposts/blogposts-md.zip -d ./wt-blogposts/\n",
    "```\n",
    "\n",
    "As a result we have all the blog posts in a folder called `wt-blogposts`.\n",
    "\n",
    "> Note: This is just an example, I write this blog [in Jupyter notebooks](https://chrwittm.github.io/posts/2022-10-21-how-i-created-this-blog/), the the references will look a bit different below reflecting the file system on my local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Wittmann-Tours LLM with RAG\n",
    "\n",
    "Before we start, let's quickly verify that the LLM does not know about the blog by asking an example question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  The name of the guide who led your tour in the Masoala rainforest on Madagascar is... (drumroll) ...Rahel!</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What was the name of the guide who led us on our tour in the Masoala rain forest on Madagascar?\"\n",
    "chat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\n",
    "chat.prompt_llama2_stream(f\"{question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it tried to guess, but the answer is not correct. Let's provide more context, here is the [blog post about Masoala](https://wittmann-tours.de/drei-tage-im-masoala-regenwald):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The blogpost has 18435 characters\n",
      "---\n",
      "title: 'Drei Tage im Masoala-Regenwald'\n",
      "description: \"\"\n",
      "published: 2019-07-14\n",
      "redirect_from: \n",
      "            - https://wittmann-tours.de/drei-tage-im-masoala-regenwald/\n",
      "categories: \"Brookesia, Chamäleon, Lemur, Madagaskar, Madagaskar, Maki, Masoala, Regenwald, roter Vari, Taggecko, Umweltschutz, Vari, Wald, Wanderung\"\n",
      "hero: ./img/wp-content-uploads-2019-06-CW-20180820-105656-0464-1024x683.jpg\n",
      "---\n",
      "# Drei Tage im Masoala-Regenwald\n",
      "\n",
      "Nach einer knapp 2-stündigen Bootsfahrt von [Nosy Mangabe](http://wittmann-tours.de/nosy-mangabe) aus erreichten wir unser Ziel, die Masoala Forest Lodge. Wir landeten an einem Strand und gingen kaum 200 Meter landeinwärts, wo ein paar hübsche kleine Bungalows auf uns warteten. So viel Luxus hatten wir nach der vorherigen Campingnacht kaum erwartet. Um das gute Wetter - sprich kein Regen - auszunutzen, starteten wir umgehend auf die erste Wanderung durch den Urwald.\n"
     ]
    }
   ],
   "source": [
    "path_to_blogpost = \"../../../lm-hackers/wt-blogposts/drei-tage-im-masoalaregenwald/index.md\"\n",
    "\n",
    "with open(path_to_blogpost, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(f\"The blogpost has {len(content)} characters\")\n",
    "print(content[:905])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can ask the question in context it is important to realize that [the model we use](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF) has a maximum context window of 4096 tokens. Since the blog post is longer, I only pass the first section. Realizing this limitation, I will not solve this here, because the main goal it to understand how we can provide context at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_with_context(question, context):\n",
    "    return ( f\"Answer the question with the help of the provided context.\"\n",
    "             f\"## Question\"\n",
    "             f\"{question}\"\n",
    "             f\"## Context\"\n",
    "             f\"{context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  The guide who led us on our tour in the Masoala rainforest was named Armand.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What was the name of the guide who led us on our tour in the Masoala rain forest on Madagascar?\"\n",
    "chat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\n",
    "chat.prompt_llama2_stream(f\"{get_question_with_context(question, content[:6000])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is correct! However, it feels a bit like a self-fulfilling prophecy since we manually added the right context.\n",
    "\n",
    "How can we automate the search for the right context? The secret sauce contains a semantic search via embeddings.\n",
    "\n",
    "## Using Embeddings to Determine Context\n",
    "\n",
    "I have discussed embeddings in detail in my blog post [Visualizing Embeddings in 2D](https://chrwittm.github.io/posts/2024-03-15-embeddings/). Here is a quick recap:\n",
    "\n",
    "An embedding is a high-dimensional numerical representations (a vector) of text which encodes semantic information. To put this in simple language: An embedding is a bunch of numbers which magically happens to describe the meaning of the text they represent. Embeddings which contain similar information are close to each other, and we can calculate the distance between 2 embeddings. Before we calculate the similarity between embeddings of questions and blog posts, let's visualize them to get a more intuitive understanding what is happening under the hood. \n",
    "\n",
    "### Visual Context Determination\n",
    "\n",
    "Visualizing the blog posts embeddings the same way as in [Visualizing Embeddings in 2D](https://chrwittm.github.io/posts/2024-03-15-embeddings/), we get the [following chart](https://github.com/chrwittm/lm-hackers/blob/main/30-rag/26-visualizing-wittmann-tours-embeddings-en-model.ipynb):\n",
    "\n",
    "![2D Visualization of Question/Blog Post Embeddings with Vectors and Cosine Similarity Circle](blog-post-similarity.png)\n",
    "\n",
    "This chart looks very confusing at first glance, so let's unpack:\n",
    "\n",
    "- The arrows (the vectors) each represent a blog post, i.e. the embedding of the blog post which contains semantic information.\n",
    "- The arrows are color-coded by country. We can see that some clusters, for example, the arrow for Ethiopia in yellow mostly point in other directions than the red ones representing Japan. There are, however, also close ones. Without having checked, I assume that this could be blog posts talking about food.\n",
    "- There are 2 black arrows in the chart labeled \"Nearest\". These are the two vectors representing the [Masoala Blog Post](https://wittmann-tours.de/drei-tage-im-masoala-regenwald/) and the question _\"What was the name of the guide who led us on our 3-day tour in the Masoala rain forest on Madagascar?\"_. (Please also read the Postscriptum for this blog post.)\n",
    "- The colorful circle in the middle represents the cosine similarity: Green represents closeness. Spanning via yellow to red, the similarity decreases.\n",
    "\n",
    "This nice visualization, shows how the computer can do a semantic search and determine the most relevant context for a question we ask an LLM: One of the black arrows represents the question, and the other represents the blog post. Because the (consine) similarity is highest between the 2, the [Masoala blog post](https://wittmann-tours.de/drei-tage-im-masoala-regenwald) is the best context for the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Context Determination\n",
    "\n",
    "The visualization above hopefully provided a way to intuitively understand in 2D what we will now compute in higher dimensionality dimensions. Let's start with defining some functions. If you want to follow along interactively, here is the related [jupyter notebook](https://github.com/chrwittm/lm-hackers/blob/main/30-rag/20-rag-reworking-hackers-guide.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#emb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=\"mps\") #English\n",
    "emb_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"mps\") #Multi-lingual\n",
    "\n",
    "def get_blog_post_files(path_to_blog):\n",
    "\n",
    "    pattern = os.path.join(path_to_blog, \"**/*.md\")\n",
    "    return glob.glob(pattern, recursive=True)\n",
    "\n",
    "def get_blog_post(path):\n",
    "    with open(path, 'r') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    return emb_model.encode(text, convert_to_tensor=True)\n",
    "\n",
    "def get_blog_post_embedding(path):\n",
    "    blog_post_text = get_blog_post(path)\n",
    "    return get_text_embedding(blog_post_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see an example of an high-dimensional embedding, here is the question embedding and the first 5-dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0244, -0.0237, -0.0702, -0.0352, -0.0163], device='mps:0')\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "question_embedding = get_text_embedding(question)\n",
    "print(question_embedding[:5])\n",
    "print(question_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the cosine similarity in 1024-dimensional space for 3 examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_similarity(embedding1, embedding2):\n",
    "    return F.cosine_similarity(embedding1, embedding2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.6135, device='mps:0'),\n",
       " tensor(0.5241, device='mps:0'),\n",
       " tensor(0.3370, device='mps:0')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_posts = [\"../../../lm-hackers/wt-blogposts/drei-tage-im-masoalaregenwald/index.md\",\n",
    "              \"../../../lm-hackers/wt-blogposts/der-bolivianische-dschungel-im-madidi-nationalpark/index.md\",\n",
    "              \"../../../lm-hackers/wt-blogposts/essen-mit-stern-hongkong-kulinarisch/index.md\"]\n",
    "\n",
    "similarities = [get_similarity(question_embedding, get_blog_post_embedding(blog_post)) for blog_post in blog_posts]\n",
    "similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the blog post about the Masoala Rain Forst has the highest cosine similarity.\n",
    "\n",
    "Let's put it all together and do a lookup over all blog posts related to a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blog_post_as_context(question):\n",
    "\n",
    "    files = get_blog_post_files(\"../../../lm-hackers/wt-blogposts\")\n",
    "\n",
    "    best_match = \"\"\n",
    "    best_match_embedding = get_text_embedding(best_match)\n",
    "    question_embedding = get_text_embedding(question)\n",
    "    best_similarity = get_similarity(question_embedding, best_match_embedding)\n",
    "\n",
    "    for file in files:\n",
    "        blog_post_embedding = get_blog_post_embedding(file)\n",
    "        blog_post_similarity = get_similarity(question_embedding, blog_post_embedding)\n",
    "        if blog_post_similarity > best_similarity:\n",
    "            best_similarity = blog_post_similarity\n",
    "            best_match = file\n",
    "    \n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mechanism in the cell above is quite inefficient, but the goal is to show the basic concept how to find the blog post best related to a question, or to put is in more general words, how to determine the best context for a question.\n",
    "\n",
    "Let's combine the context determination with prompting the LLM and displaying the source of the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  The guide who led us on our tour in the Masoala rainforest on Madagascar was named Armand.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context used: ../../../lm-hackers/wt-blogposts/nosy-mangabe/index.md\n"
     ]
    }
   ],
   "source": [
    "question = \"What was the name of the guide who led us on our tour in the Masoala rain forest on Madagascar?\"\n",
    "blog_post_path = get_blog_post_as_context(question)\n",
    "blog_post_context = get_blog_post(blog_post_path)\n",
    "chat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\n",
    "chat.prompt_llama2_stream(f\"{get_question_with_context(question, blog_post_context[:6000])}\")\n",
    "print(f\"Context used: {blog_post_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have successfully built a RAG-application to answer questions related to the [Wittmann-Tours.de](https://wittmann-tours.de) blog - Juhu! Between the lines, you can already see more general concepts:\n",
    "\n",
    "- We have indexed all blog posts into embeddings to encode their semantics. This essentially translates to adding data to a vector database.\n",
    "- We have computed the embedding for a question and visualized/computed finding the closest blog post. This essentially translates to querying a vector database.\n",
    "\n",
    "The implementation successfully helped to mitigate typical LLM-related problems described above in the following way:\n",
    "\n",
    "- By grounding the LLM in data related to the question, the LLM did not hallucinate.\n",
    "- The LLM can answer questions related to topics it was never trained on.\n",
    "- Alongside the answer we get the source of the information and can cross-check if needed.\n",
    "\n",
    "## Postscriptum\n",
    "\n",
    "Just as I wrote the conclusion of this blog post, I realized what I would label a \"copy&paste error\". This error, however, did not really materialize, and I think it is very interesting, so I though it is worth mentioning - But what happened?\n",
    "\n",
    "Initially, I used the embedding model ([BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)), copy&pasting from the [hacker's guide](https://github.com/fastai/lm-hackers/blob/main/lm-hackers.ipynb) into my [embedding notebook](https://chrwittm.github.io/posts/2024-03-15-embeddings/), copy&pasting into this notebook. This model, however, has been trained on English, but the [Wittmann-Tours.de](https://wittmann-tours.de) blog is written in German. Nonetheless, it performed very well in calculating the similarity of the vectors, and it even bridged the gap between the German texts and the English question.\n",
    "\n",
    "I only noticed one slight inconsistency: When visualizing the blog posts in 2D, the question _\"What was the name of the guide who led us on our 3-day tour in the Masoala rain forest on Madagascar?\"_ did not show the Masoala blog post as the closest vector. Initially, I just thought the reason was rooted in the dimensionality reduction from 384 to 2 dimensions. After realizing the language issue, changing the embedding model to the multi-lingual [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3) fixed that. For educational purposes, however, I used the [slightly tweaked visualization from the english embedding model](https://github.com/chrwittm/lm-hackers/blob/main/30-rag/26-visualizing-wittmann-tours-embeddings-en-model.ipynb) because the [visualization from the multi-lingual model](https://github.com/chrwittm/lm-hackers/blob/main/30-rag/25-visualizing-wittmann-tours-embeddings.ipynb) is too perfect: The arrows of the question and of the blog post are displayed as one. Embrace your errors, and usually they yield something useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] The original RAG-paper: [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401v4)\n",
    "\n",
    "[2] [The Hacker's Guide](https://github.com/fastai/lm-hackers/blob/main/lm-hackers.ipynb) by Jeremy Howard\n",
    "\n",
    "[3] [Visualizing Embeddings in 2D](https://chrwittm.github.io/posts/2024-03-15-embeddings/)\n",
    "\n",
    "[4] The [Wittmann-Tours.de](https://wittmann-tours.de) blog, also available [on GitHub](https://github.com/chrwittm/wittmann-tours)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
