{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Empirically estimating tokens per word across languages\"\n",
    "author: \"Christian Wittmann\"\n",
    "date: \"2025-05-23\"\n",
    "categories: [tokenization, nlp]\n",
    "image: \"tokens-per-word2.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens are the new currency of generative AI. We’re paying for generative AI usage in tokens, sometimes directly via APIs, more often invisibly when using generative AI apps. But how many tokens does a given piece of text actually contain? Can you estimate this intuitively?\n",
    "\n",
    "Personally, I can’t. Even though estimating the number of words is not trivial either (more on that later), I was looking for a simple rule of thumb on how to convert the number of words into the number of tokens. The simple answer is that English text contains on average [1.3 tokens per word](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them). But how about other languages? German, for example, tends to have longer words than English. I was surprised that I couldn’t find convincing empirical research on this topic, hence I decided to conduct my own.\n",
    "\n",
    "My approach was straightforward: I tokenized a lot of random Wikipedia articles in various languages and counted their words to determine a real token-per-word ratio. Here is the result, and subsequently, I will explain how I arrived at this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    text-align: center;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"tokens-per-word2.png\" alt=\"Token-per-word ratios across languages\" style=\"width:50%;\">\n",
    "    <figcaption>Token-per-word ratios across languages</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why measuring tokens per word is important\n",
    "\n",
    "With the rapidly declining cost per token, one might argue that token count doesn’t really matter anymore. While there’s merit to this viewpoint, I still believe it’s highly valuable to have an intuitive understanding of token counts for several reasons.\n",
    "\n",
    "First, despite significant increases, context windows remain comparatively finite. Understanding roughly how many tokens a text contains helps you reason about what realistically fits within these limits.\n",
    "\n",
    "Second, the foundational measurement for text processing is tokens per word, making this ratio essential for intuitive estimation.\n",
    "\n",
    "Regarding cost, it’s true that for an individual prompt, the token cost is negligible. However, at enterprise scale, token counts can quickly become significant. Particularly when designing AI applications at scale, having reliable ballpark numbers can substantially impact decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related work\n",
    "\n",
    "Research in this area is limited. The most relevant papers and posts I found are:\n",
    "\n",
    "- [Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models](https://arxiv.org/abs/2305.13707): Even though this paper has a similar idea, it is more focused on cost and fairness, and there's no figure in there for direct token-per-word ratio.\n",
    "- [All Languages Are NOT Created (Tokenized) Equal](https://www.artfish.ai/p/all-languages-are-not-created-tokenized): Similar research question, but the main result is comparative, i.e. how much longer other languages tokenize compared to English. I will use this as reference to verify my results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog post, I’ll focus on OpenAI’s tokenizers `cl100k_base` (for GPT-4) and `o200k_base` (for GPT-4o):\n",
    "\n",
    "| Encoding name    | OpenAI models                            |\n",
    "|------------------|------------------------------------------|\n",
    "| `o200k_base`     | `gpt-4o`, `gpt-4o-mini`                  |\n",
    "| `cl100k_base`    | `gpt-4-turbo`, `gpt-4`, `gpt-3.5-turbo`  |\n",
    "\n",
    "Source: [How to count tokens with tiktoken](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)\n",
    "\n",
    "Tokenizing a given text is actually quite straightforward, simply pip install the `tiktoken` library, and you’re ready to start tokenizing:\n",
    "\n",
    "```sh\n",
    "!pip install tiktoken\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def get_encoder(encoding_name=\"o200k_base\"):\n",
    "    \"\"\"Returns a tiktoken encoder. Defaults to GPT-4o's tokenizer.\"\"\"\n",
    "    return tiktoken.get_encoding(encoding_name)\n",
    "\n",
    "def count_tokens(text: str, encoder=None) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in the input text using the specified encoder.\n",
    "    If no encoder is provided, a new one will be created.\n",
    "    \"\"\"\n",
    "    if encoder is None:\n",
    "        encoder = get_encoder()\n",
    "    return len(encoder.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Text: This is a simple test sentence to see how tokenization works.\n",
      "Tokens:       [2500, 382, 261, 4705, 1746, 21872, 316, 1921, 1495, 6602, 2860, 5882, 13]\n",
      "Token Count:  13\n"
     ]
    }
   ],
   "source": [
    "encoder = get_encoder(encoding_name=\"o200k_base\")\n",
    "input_text = \"This is a simple test sentence to see how tokenization works.\"\n",
    "print(f\"Example Text: {input_text}\")\n",
    "print(f\"Tokens:       {encoder.encode(input_text)}\")\n",
    "print(f\"Token Count:  {count_tokens(input_text, encoder)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Wikipedia articles\n",
    "\n",
    "Next, we’ll need some real text data to tokenize. We’ll use random Wikipedia articles because they’re easily accessible in virtually any language and provide diverse content suitable for generalization. I’m assuming that the token-per-word ratio becomes relatively constant as datasets grow larger, as individual variations even out.\n",
    "\n",
    "To fetch articles programmatically, we can conveniently use the Wikipedia API, available via the Python package `wikipedia`:\n",
    "\n",
    "```sh\n",
    "pip install wikipedia\n",
    "```\n",
    "\n",
    "With this setup, we can easily retrieve and tokenize diverse text samples across different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "import wikipediaapi\n",
    "import requests\n",
    "from urllib.parse import unquote\n",
    "\n",
    "def get_wikipedia_article(language: str = \"en\", title: str = None) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Fetches the plain text of a Wikipedia article.\n",
    "    If `title` is None, a random article is fetched.\n",
    "    \"\"\"\n",
    "    wiki = wikipediaapi.Wikipedia(\n",
    "        language=language,\n",
    "        user_agent=\"TokenCountResearch/1.0 (chrwittm@gmail.com)\"\n",
    "    )\n",
    "    \n",
    "    if title is None:\n",
    "        # Get a random article by following a redirect\n",
    "        url = f\"https://{language}.wikipedia.org/wiki/Special:Random\"\n",
    "        response = requests.get(url, allow_redirects=True)\n",
    "        title = response.url.split(\"/wiki/\")[-1]\n",
    "        title = unquote(title)  # 🔧 decode Unicode\n",
    "\n",
    "    page = wiki.page(title)\n",
    "\n",
    "    if not page.exists():\n",
    "        print(f\"Article '{title}' not found in language '{language}'.\")\n",
    "        return title, \"\"\n",
    "\n",
    "    return title, page.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the Wikipedia article on [Artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called A\n"
     ]
    }
   ],
   "source": [
    "title, text = get_wikipedia_article(language=\"en\", title=\"Artificial intelligence\")\n",
    "print(text[:500])  # Print preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reading a random German Wikipedia article, you can use the following Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random German article title: IC_583\n",
      "Article preview:\n",
      "IC 583 ist eine spiralförmige Radiogalaxie vom Hubble-Typ Sbc im Sternbild Löwe an der Ekliptik. Sie ist schätzungsweise 349 Millionen Lichtjahre von der Milchstraße entfernt und hat einen Durchmesser von etwa 145.000 Lichtjahren. Vom Sonnensystem aus entfernt sich die Galaxie mit einer errechneten Radialgeschwindigkeit von näherungsweise 7.900 Kilometern pro Sekunde.\n",
      "Gemeinsam mit IC 582 bildet sie das Galaxienpaar Holm 155 und mit PGC 1542326 ein gravitativ gebundenes Triplet. Im selben Himmel\n"
     ]
    }
   ],
   "source": [
    "title_random_de, text_random_de = get_wikipedia_article(language=\"de\")\n",
    "print(f\"Random German article title: {title_random_de}\")\n",
    "print(f\"Article preview:\\n{text_random_de[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting words is surprisingly tricky\n",
    "\n",
    "Counting words seems straightforward at first glance, but it quickly becomes complex once you dig deeper. Initially, my approach was quite naive: splitting text simply based on whitespace. This method works reasonably well for languages using spaces as word separators, such as English or German, although even here, it fails to handle contractions properly (e.g., “can’t,” “don’t,” or “it’s”). For languages with fundamentally different writing systems (Chinese, Japanese, or Korean), this whitespace-based approach completely breaks down because these languages either rarely or never use spaces to separate words. Clearly, a more sophisticated approach was required.\n",
    "\n",
    "To address this, I turned to spaCy, a robust and multilingual NLP library that intelligently segments text into words by using language-specific models. SpaCy considers linguistic nuances, punctuation, contractions, and special characters, providing accurate and reliable word counting across diverse languages. The spaCy models significantly improve word-count reliability compared to a naive whitespace-based method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of spaCy\n",
    "\n",
    "To start using spaCy, you’ll first need to install it:\n",
    "\n",
    "```sh\n",
    "pip install spacy\n",
    "```\n",
    "\n",
    "Then, you’ll need to download the language-specific models for the languages you’re working with. For example, for English, German, and Chinese, execute:\n",
    "\n",
    "```sh\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "python -m spacy download zh_core_web_sm\n",
    "```\n",
    "\n",
    "To automate the installation process, you can run the following cell to check which language models you have already installed. Subsequently, we'll install missing language packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy models found:\n",
      "\n",
      "Model Name           Language   Model Ver.   spaCy Ver.   Pipeline\n",
      "----------------------------------------------------------------------\n",
      "fr_core_news_sm      fr         3.7.0        >=3.7.0,<3.8.0 tok2vec,morphologizer,parser,attribute_ruler,lemmatizer,ner\n",
      "es_core_news_sm      es         3.7.0        >=3.7.0,<3.8.0 tok2vec,morphologizer,parser,attribute_ruler,lemmatizer,ner\n",
      "ja_core_news_sm      ja         3.7.0        >=3.7.0,<3.8.0 tok2vec,morphologizer,parser,attribute_ruler,ner\n",
      "pl_core_news_sm      pl         3.7.0        >=3.7.0,<3.8.0 tok2vec,morphologizer,parser,lemmatizer,tagger,attribute_ruler,ner\n",
      "it_core_news_sm      it         3.7.0        >=3.7.0,<3.8.0 tok2vec,morphologizer,tagger,parser,lemmatizer,attribute_ruler,ner\n",
      "ko_core_news_sm      ko         3.7.0        >=3.7.0,<3.8.0 tok2vec,tagger,morphologizer,parser,lemmatizer,attribute_ruler,ner\n",
      "en_core_web_sm       en         3.7.1        >=3.7.2,<3.8.0 tok2vec,tagger,parser,attribute_ruler,lemmatizer,ner\n",
      "ru_core_news_sm      ru         3.7.0        >=3.7.0,<3.8.0 tok2vec,morphologizer,parser,attribute_ruler,lemmatizer,ner\n",
      "de_core_news_sm      de         3.7.0        >=3.7.0,<3.8.0 tok2vec,tagger,morphologizer,parser,lemmatizer,attribute_ruler,ner\n",
      "pt_core_news_sm      pt         3.7.0        >=3.7.0,<3.8.0 tok2vec,morphologizer,parser,lemmatizer,attribute_ruler,ner\n",
      "zh_core_web_sm       zh         3.7.0        >=3.7.0,<3.8.0 tok2vec,tagger,parser,attribute_ruler,ner\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "def check_installed_spacy_models():\n",
    "    \"\"\"\n",
    "    For each installed spaCy model (by name), load it and print key metadata:\n",
    "      - Model Name\n",
    "      - Language code\n",
    "      - Model Version\n",
    "      - Required spaCy version (if available)\n",
    "      - Pipeline components\n",
    "    \"\"\"\n",
    "    import spacy\n",
    "    from spacy.cli.validate import get_installed_models\n",
    "\n",
    "    installed_model_names = get_installed_models()\n",
    "\n",
    "    if not installed_model_names:\n",
    "        print(\"No spaCy models found.\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"spaCy models found:\\n\")\n",
    "\n",
    "    print(f\"{'Model Name':<20} {'Language':<10} {'Model Ver.':<12} {'spaCy Ver.':<12} Pipeline\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for model_name in installed_model_names:\n",
    "        # Attempt to load the model to read its meta\n",
    "        try:\n",
    "            nlp = spacy.load(model_name)\n",
    "            meta = getattr(nlp, \"meta\", {})\n",
    "            # Extract metadata safely\n",
    "            lang = meta.get(\"lang\", \"n/a\")\n",
    "            version = meta.get(\"version\", \"n/a\")\n",
    "            spacy_req = meta.get(\"spacy_version\", \"n/a\")\n",
    "            pipeline = meta.get(\"pipeline\", [])\n",
    "\n",
    "            print(\n",
    "                f\"{model_name:<20} \"\n",
    "                f\"{lang:<10} \"\n",
    "                f\"{version:<12} \"\n",
    "                f\"{spacy_req:<12} \"\n",
    "                f\"{','.join(pipeline)}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"{model_name:<20} FAILED TO LOAD: {e}\")\n",
    "\n",
    "check_installed_spacy_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy models' names follow two main naming conventions depending on the language and source corpus. For English, models are typically named like `en_core_web_sm`, where “web” refers to the OntoNotes web-based corpus used for training. For most other languages, models are named like `de_core_news_sm`, reflecting their training on news-domain texts from Universal Dependencies corpora. While the difference can be confusing at first, it reflects the underlying data sources and training pipelines. To keep things simple and avoid guesswork, we define the full model name explicitly in our language configuration dictionary.\n",
    "\n",
    "I picked the following languages for this analysis, because they are supported by spaCy, and I found them to be interesting. This means that they are either commonly used or they are otherwise interesting because they use non-Latin scripts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES = {\n",
    "    \"de\": {\"name\": \"German\",      \"model\": \"de_core_news_sm\",    \"emoji\": \"🇩🇪\"},\n",
    "    \"en\": {\"name\": \"English\",     \"model\": \"en_core_web_sm\",     \"emoji\": \"🇺🇸\"},\n",
    "    \"es\": {\"name\": \"Spanish\",     \"model\": \"es_core_news_sm\",    \"emoji\": \"🇪🇸\"},\n",
    "    \"fr\": {\"name\": \"French\",      \"model\": \"fr_core_news_sm\",    \"emoji\": \"🇫🇷\"},\n",
    "    \"it\": {\"name\": \"Italian\",     \"model\": \"it_core_news_sm\",    \"emoji\": \"🇮🇹\"},\n",
    "    #\"ja\": {\"name\": \"Japanese\",    \"model\": \"ja_core_news_sm\",    \"emoji\": \"🇯🇵\"},\n",
    "    \"ko\": {\"name\": \"Korean\",      \"model\": \"ko_core_news_sm\",    \"emoji\": \"🇰🇷\"},\n",
    "    \"pl\": {\"name\": \"Polish\",      \"model\": \"pl_core_news_sm\",    \"emoji\": \"🇵🇱\"},\n",
    "    \"pt\": {\"name\": \"Portuguese\",  \"model\": \"pt_core_news_sm\",    \"emoji\": \"🇵🇹\"},\n",
    "    \"ru\": {\"name\": \"Russian\",     \"model\": \"ru_core_news_sm\",    \"emoji\": \"🇷🇺\"},\n",
    "    \"zh\": {\"name\": \"Chinese\",     \"model\": \"zh_core_web_sm\",     \"emoji\": \"🇨🇳\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "def install_spacy_models(language_dict):\n",
    "    \"\"\"\n",
    "    Tries to install spaCy language models for all entries in the language_dict.\n",
    "    Requires model names to be specified per language.\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    for lang_code, data in language_dict.items():\n",
    "        model = data[\"model\"]\n",
    "        print(f\"Installing spaCy model for {data['name']} ({lang_code}) — {model}\")\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", model], check=True)\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"❌ Failed to install spaCy model for {lang_code} ({model})\")\n",
    "\n",
    "install_languages = False\n",
    "\n",
    "if install_languages:\n",
    "    install_spacy_models(LANGUAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: If you have installed new languages, be sure to restart your Python kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Words with spaCy\n",
    "\n",
    "Now, we are almost ready to count words using spaCy's tokenizer. It’s important to clarify here that the term “token” can be somewhat ambiguous and context-dependent. In natural language processing libraries like spaCy, a “token” typically means a word, punctuation mark, or similar meaningful unit of text. However, when dealing with Large Language Models (LLMs), a “token” usually refers to a subword unit produced by the tokenizer. Therefore, always keep this distinction in mind to avoid confusion as you follow along.\n",
    "\n",
    "For what we want to do, we need to separate the spaCy tokens into words tokens and other tokens like punctuation, spaces, etc. as illustrated by the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Simple cache/dictionary to hold loaded spaCy models:\n",
    "_spacy_models = {}\n",
    "\n",
    "def get_spacy_model(language_code: str = \"en\"):\n",
    "    \"\"\"\n",
    "    Loads and caches the spaCy language model for the given language code.\n",
    "    Uses the model name defined in the LANGUAGES dict.\n",
    "    Falls back to a blank model if the specified model is not available.\n",
    "    \"\"\"\n",
    "    if language_code not in _spacy_models:\n",
    "        model_name = LANGUAGES.get(language_code, {}).get(\"model\", None)\n",
    "        try:\n",
    "            if model_name:\n",
    "                _spacy_models[language_code] = spacy.load(model_name)\n",
    "            else:\n",
    "                raise ValueError(f\"No model defined for language code: '{language_code}'\")\n",
    "        except (OSError, ValueError) as e:\n",
    "            print(f\"⚠️ Could not load model '{model_name}' for language '{language_code}': {e}\")\n",
    "            print(\"→ Falling back to blank spaCy model (basic tokenization only).\")\n",
    "            _spacy_models[language_code] = spacy.blank(language_code)\n",
    "    return _spacy_models[language_code]\n",
    "\n",
    "def get_spacy_tokens(text: str, language_code: str = \"en\") -> tuple[list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Tokenizes the input text using spaCy's tokenizer.\n",
    "    Returns two lists: one with spaCy tokens (words) and one with omitted tokens \n",
    "    (punctuation, spaces, symbols, etc.).\n",
    "    \"\"\"\n",
    "    nlp = get_spacy_model(language_code)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    punctuation_set = set(string.punctuation)\n",
    "    \n",
    "    word_tokens = [\n",
    "        t for t in doc \n",
    "        if not t.is_space \n",
    "           and not t.is_punct \n",
    "           and t.pos_ != \"SYM\" \n",
    "           and t.text not in punctuation_set\n",
    "    ]\n",
    "    omitted_tokens = [\n",
    "        t for t in doc \n",
    "        if t.is_space \n",
    "           or t.is_punct \n",
    "           or t.pos_ == \"SYM\" \n",
    "           or t.text in punctuation_set\n",
    "    ]\n",
    "    \n",
    "    return word_tokens, omitted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[You, 're, very, tall, Do, you, play, basketball]\n",
      "[!, ?]\n"
     ]
    }
   ],
   "source": [
    "word_tokens, omitted_tokens = get_spacy_tokens(\"You're very tall! Do you play basketball?\", language_code=\"en\")\n",
    "print(word_tokens)\n",
    "print(omitted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking ahead: Making sure that we only count words will later drive up the token-per-word ratio because the LLM tokenizer will also tokenize punctuations and other markup in the Wikipedia articles like headings, tables, and lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting everything together, here is a function that counts words using spaCy's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "def count_words_spacy(text: str, language_code: str = \"en\") -> int:\n",
    "    \"\"\"\n",
    "    Counts words in the input text using spaCy's tokenizer.\n",
    "    Skips punctuation/whitespace tokens.\n",
    "    \"\"\"\n",
    "    nlp = get_spacy_model(language_code)\n",
    "    doc = nlp(text)\n",
    "    punctuation_set = set(string.punctuation)\n",
    "    \n",
    "    # Filter out space/punctuation tokens:\n",
    "    tokens = [\n",
    "        t for t in doc \n",
    "        if not t.is_space \n",
    "           and not t.is_punct \n",
    "           and t.pos_ != \"SYM\" \n",
    "           and t.text not in punctuation_set\n",
    "    ]\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for the fun of it, let's compare spaCy's word count with the naive whitespace-based method using the [English Wikipedia article on Artificial Intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive white-space word count: 13233\n",
      "SpaCy word count:             13443\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "def count_words_naive(text: str) -> int:\n",
    "    \"\"\"Counts the number of words in the input text using simple whitespace splitting.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "print(f\"Naive white-space word count: {count_words_naive(text)}\")\n",
    "print(f\"SpaCy word count:             {count_words_spacy(text, language_code='en')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the naive white-space word count and the spaCy word count is due to the fact that spaCy's tokenizer recognizes contractions, hyphenation, and special characters more accurately. For example, spaCy tokenizes \"step-by-step\" into three separate tokens: \"step\", \"by\", and \"step\", whereas the naive white-space word count would only count it as one token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other languages, like Chinese, do not even use spaces to separate words. Here is an example of a Chinese sentence: \"我喜欢吃苹果和香蕉。\" which means \"I like to eat apples and bananas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected words:               6\n",
      "Naive white-space word count: 1\n",
      "SpaCy word count:             6\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "text_zh = \"我喜欢吃苹果和香蕉。\"\n",
    "print(f\"Expected words:               6\")\n",
    "print(f\"Naive white-space word count: {count_words_naive(text_zh)}\")\n",
    "print(f\"SpaCy word count:             {count_words_spacy(text_zh, language_code='zh')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the token-per-word ratio\n",
    "\n",
    "Finally, we can calculate the token-per-word ratio. Again, let's use the Wikipedia article on [Artificial Intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: Artificial intelligence\n",
      "Words: 13443\n",
      "Tokens: 16875\n",
      "Tokens per word: 1.255\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "def get_tokens_per_word(text: str, language_code: str = \"en\", encoder=None) -> float:\n",
    "    \"\"\"\n",
    "    Calculates average number of tokens (tiktoken) per word (spaCy-based) for the given text.\n",
    "    \"\"\"\n",
    "    words = count_words_spacy(text, language_code=language_code)\n",
    "    tokens = count_tokens(text, encoder=encoder)\n",
    "    \n",
    "    if words == 0:\n",
    "        return 0.0\n",
    "    return tokens / words\n",
    "\n",
    "print(f\"Article: {title}\")\n",
    "print(f\"Words: {count_words_spacy(text, language_code='en')}\")\n",
    "print(f\"Tokens: {count_tokens(text, encoder)}\")\n",
    "print(f\"Tokens per word: {get_tokens_per_word(text=text, language_code='en', encoder=encoder):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch analysis per language\n",
    "\n",
    "For a first comparison across languages, let's use 10 random articles for a few select languages to get a rough idea of the average token-per-word ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "def analyze_wikipedia_language_sample(language: str, n: int, encoder=None, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches exactly `n` valid random Wikipedia articles in the specified language and computes:\n",
    "    - total number of words\n",
    "    - total number of tokens\n",
    "    - average tokens per word\n",
    "    \"\"\"\n",
    "    import time\n",
    "    if encoder is None:\n",
    "        encoder = get_encoder()\n",
    "\n",
    "    total_words = 0\n",
    "    total_tokens = 0\n",
    "    successful_articles = 0\n",
    "    attempts = 0\n",
    "\n",
    "    while successful_articles < n:\n",
    "        attempts += 1\n",
    "        title, text = get_wikipedia_article(language=language)\n",
    "\n",
    "        if not text.strip():\n",
    "            continue  # skip and retry\n",
    "\n",
    "        # Updated to use spaCy-based word counting:\n",
    "        words = count_words_spacy(text, language_code=language)\n",
    "        tokens = count_tokens(text, encoder=encoder)\n",
    "\n",
    "        if words == 0 or tokens == 0:\n",
    "            continue\n",
    "\n",
    "        total_words += words\n",
    "        total_tokens += tokens\n",
    "        successful_articles += 1\n",
    "\n",
    "        tokens_per_word = tokens / words\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[{successful_articles}/{n}] {title} — Words: {words}, Tokens: {tokens}, Tokens/Word: {tokens_per_word:.3f}\")\n",
    "\n",
    "        time.sleep(0.3)  # polite pause\n",
    "\n",
    "    tokens_per_word_avg = total_tokens / total_words if total_words > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'language': language,\n",
    "        'articles': successful_articles,\n",
    "        'total_words': total_words,\n",
    "        'total_tokens': total_tokens,\n",
    "        'tokens_per_word': tokens_per_word_avg\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Paris_Gibson_Square_Museum_of_Art — Words: 5172, Tokens: 6995, Tokens/Word: 1.352\n",
      "[2/10] Dalbergia_pseudobaronii — Words: 291, Tokens: 480, Tokens/Word: 1.649\n",
      "[3/10] Niels_Wulfsberg — Words: 445, Tokens: 692, Tokens/Word: 1.555\n",
      "[4/10] Brush_Script — Words: 215, Tokens: 328, Tokens/Word: 1.526\n",
      "[5/10] Molly_Harper — Words: 352, Tokens: 445, Tokens/Word: 1.264\n",
      "[6/10] Augsburg_Arena — Words: 355, Tokens: 587, Tokens/Word: 1.654\n",
      "[7/10] Salmon_Arm_Airport — Words: 46, Tokens: 82, Tokens/Word: 1.783\n",
      "[8/10] Sarah_LeFanu — Words: 211, Tokens: 309, Tokens/Word: 1.464\n",
      "[9/10] Sir_James_Horlick,_1st_Baronet — Words: 348, Tokens: 493, Tokens/Word: 1.417\n",
      "[10/10] 21st_Infantry_Division_(Russian_Empire) — Words: 104, Tokens: 228, Tokens/Word: 2.192\n",
      "\n",
      "--- Summary ---\n",
      "language: en\n",
      "articles: 10\n",
      "total_words: 7539\n",
      "total_tokens: 10639\n",
      "tokens_per_word: 1.41119511871601\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "results = analyze_wikipedia_language_sample(language=\"en\", n=10, encoder=encoder)\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Budschak_(Bolhrad) — Words: 454, Tokens: 876, Tokens/Word: 1.930\n",
      "[2/10] Montana-Territorium — Words: 334, Tokens: 615, Tokens/Word: 1.841\n",
      "[3/10] Holme_Rose — Words: 481, Tokens: 886, Tokens/Word: 1.842\n",
      "[4/10] Dolmen_von_Fontenaille — Words: 125, Tokens: 225, Tokens/Word: 1.800\n",
      "[5/10] Monika_Wernicke — Words: 154, Tokens: 282, Tokens/Word: 1.831\n",
      "[6/10] Friedrichshöhe_(Leichlingen) — Words: 197, Tokens: 398, Tokens/Word: 2.020\n",
      "[7/10] Flügelaltar_von_Schloss_Tirol — Words: 1574, Tokens: 2804, Tokens/Word: 1.781\n",
      "[8/10] Milenino_(Kursk) — Words: 198, Tokens: 426, Tokens/Word: 2.152\n",
      "[9/10] Irmintraut_Richarz — Words: 197, Tokens: 402, Tokens/Word: 2.041\n",
      "[10/10] Jürgen_Bolten — Words: 1241, Tokens: 2512, Tokens/Word: 2.024\n",
      "\n",
      "--- Summary ---\n",
      "language: de\n",
      "articles: 10\n",
      "total_words: 4955\n",
      "total_tokens: 9426\n",
      "tokens_per_word: 1.9023208879919273\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "results = analyze_wikipedia_language_sample(language=\"de\", n=10, encoder=encoder)\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] 9mm警用轉輪手槍 — Words: 313, Tokens: 577, Tokens/Word: 1.843\n",
      "[2/10] 十川誠志 — Words: 750, Tokens: 1960, Tokens/Word: 2.613\n",
      "[3/10] 白晓卉 — Words: 225, Tokens: 439, Tokens/Word: 1.951\n",
      "[4/10] 多椎半鱨 — Words: 57, Tokens: 137, Tokens/Word: 2.404\n",
      "[5/10] 360图片 — Words: 121, Tokens: 190, Tokens/Word: 1.570\n",
      "[6/10] 賈特人 — Words: 434, Tokens: 842, Tokens/Word: 1.940\n",
      "[7/10] 维勒迪约-拉布卢埃尔 — Words: 181, Tokens: 440, Tokens/Word: 2.431\n",
      "[8/10] U-161号潜艇_(1918年) — Words: 599, Tokens: 980, Tokens/Word: 1.636\n",
      "[9/10] 比利肯 — Words: 668, Tokens: 1383, Tokens/Word: 2.070\n",
      "[10/10] 桑省 — Words: 183, Tokens: 358, Tokens/Word: 1.956\n",
      "\n",
      "--- Summary ---\n",
      "language: zh\n",
      "articles: 10\n",
      "total_words: 3531\n",
      "total_tokens: 7306\n",
      "tokens_per_word: 2.069102237326536\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "results = analyze_wikipedia_language_sample(language=\"zh\", n=10, encoder=encoder)\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis\n",
    "\n",
    "Let's put everything together and run the full analysis. The numbers start to converge at about 100 articles. To be on the safe side, let's do 200 articles each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "def analyze_multiple_languages(language_dict, n, encoder=None):\n",
    "    \"\"\"\n",
    "    Analyzes multiple languages using their configuration from the LANGUAGES dictionary.\n",
    "    \n",
    "    For each language, fetches `n` random Wikipedia articles and calculates:\n",
    "      - total word count (using spaCy)\n",
    "      - total token count (using tiktoken)\n",
    "      - average tokens per word\n",
    "\n",
    "    Returns a list of dictionaries for easy tabular display.\n",
    "    Each row contains: language code, name, total words, total tokens, tokens per word.\n",
    "    \"\"\"\n",
    "    if encoder is None:\n",
    "        encoder = get_encoder()\n",
    "    \n",
    "    results_table = []\n",
    "    \n",
    "    for lang_code, config in language_dict.items():\n",
    "        lang_name = config[\"name\"]\n",
    "        #print(f\"\\n🔍 Analyzing {lang_name} ({lang_code})...\")\n",
    "        \n",
    "        try:\n",
    "            summary = analyze_wikipedia_language_sample(\n",
    "                language=lang_code,\n",
    "                n=n,\n",
    "                encoder=encoder,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            row = {\n",
    "                \"language\": lang_code,\n",
    "                \"name\": lang_name,\n",
    "                \"total_words\": summary[\"total_words\"],\n",
    "                \"total_tokens\": summary[\"total_tokens\"],\n",
    "                \"tokens_per_word\": summary[\"tokens_per_word\"]\n",
    "            }\n",
    "            results_table.append(row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {lang_name} ({lang_code}): {e}\")\n",
    "\n",
    "    return results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "def print_language_analysis_table_pandas(results_table, language_dict=None):\n",
    "    \"\"\"\n",
    "    Creates and displays a pandas DataFrame from the results_table.\n",
    "    Adds flag emoji (if available), replaces codes with names, sorts by Tokens/Word.\n",
    "    Suppresses the index column in the Jupyter output.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.DataFrame(results_table)\n",
    "\n",
    "    if language_dict:\n",
    "        df[\"name\"] = df[\"language\"].map(lambda code: language_dict.get(code, {}).get(\"name\", code))\n",
    "        df[\"emoji\"] = df[\"language\"].map(lambda code: language_dict.get(code, {}).get(\"emoji\", \"\"))\n",
    "\n",
    "    # Reorder and rename columns\n",
    "    df = df[[\"emoji\", \"language\", \"name\", \"total_words\", \"total_tokens\", \"tokens_per_word\"]]\n",
    "    df.columns = [\"Flag\", \"Code\", \"Language\", \"Words\", \"Tokens\", \"Tokens/Word\"]\n",
    "\n",
    "    # Sort by tokens per word (ascending)\n",
    "    df = df.sort_values(by=\"Tokens/Word\")\n",
    "\n",
    "    # Format and hide index (Jupyter only)\n",
    "    styled_df = df.style.format({\"Tokens/Word\": \"{:.3f}\"}).hide(axis=\"index\")\n",
    "    display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the result for `cl100k_base`, GPT-4's tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ab78c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_ab78c_level0_col0\" class=\"col_heading level0 col0\" >Flag</th>\n",
       "      <th id=\"T_ab78c_level0_col1\" class=\"col_heading level0 col1\" >Code</th>\n",
       "      <th id=\"T_ab78c_level0_col2\" class=\"col_heading level0 col2\" >Language</th>\n",
       "      <th id=\"T_ab78c_level0_col3\" class=\"col_heading level0 col3\" >Words</th>\n",
       "      <th id=\"T_ab78c_level0_col4\" class=\"col_heading level0 col4\" >Tokens</th>\n",
       "      <th id=\"T_ab78c_level0_col5\" class=\"col_heading level0 col5\" >Tokens/Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_ab78c_row0_col0\" class=\"data row0 col0\" >🇺🇸</td>\n",
       "      <td id=\"T_ab78c_row0_col1\" class=\"data row0 col1\" >en</td>\n",
       "      <td id=\"T_ab78c_row0_col2\" class=\"data row0 col2\" >English</td>\n",
       "      <td id=\"T_ab78c_row0_col3\" class=\"data row0 col3\" >87627</td>\n",
       "      <td id=\"T_ab78c_row0_col4\" class=\"data row0 col4\" >129379</td>\n",
       "      <td id=\"T_ab78c_row0_col5\" class=\"data row0 col5\" >1.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ab78c_row1_col0\" class=\"data row1 col0\" >🇪🇸</td>\n",
       "      <td id=\"T_ab78c_row1_col1\" class=\"data row1 col1\" >es</td>\n",
       "      <td id=\"T_ab78c_row1_col2\" class=\"data row1 col2\" >Spanish</td>\n",
       "      <td id=\"T_ab78c_row1_col3\" class=\"data row1 col3\" >111473</td>\n",
       "      <td id=\"T_ab78c_row1_col4\" class=\"data row1 col4\" >202116</td>\n",
       "      <td id=\"T_ab78c_row1_col5\" class=\"data row1 col5\" >1.813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ab78c_row2_col0\" class=\"data row2 col0\" >🇫🇷</td>\n",
       "      <td id=\"T_ab78c_row2_col1\" class=\"data row2 col1\" >fr</td>\n",
       "      <td id=\"T_ab78c_row2_col2\" class=\"data row2 col2\" >French</td>\n",
       "      <td id=\"T_ab78c_row2_col3\" class=\"data row2 col3\" >91423</td>\n",
       "      <td id=\"T_ab78c_row2_col4\" class=\"data row2 col4\" >169248</td>\n",
       "      <td id=\"T_ab78c_row2_col5\" class=\"data row2 col5\" >1.851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ab78c_row3_col0\" class=\"data row3 col0\" >🇵🇹</td>\n",
       "      <td id=\"T_ab78c_row3_col1\" class=\"data row3 col1\" >pt</td>\n",
       "      <td id=\"T_ab78c_row3_col2\" class=\"data row3 col2\" >Portuguese</td>\n",
       "      <td id=\"T_ab78c_row3_col3\" class=\"data row3 col3\" >96179</td>\n",
       "      <td id=\"T_ab78c_row3_col4\" class=\"data row3 col4\" >182802</td>\n",
       "      <td id=\"T_ab78c_row3_col5\" class=\"data row3 col5\" >1.901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ab78c_row4_col0\" class=\"data row4 col0\" >🇮🇹</td>\n",
       "      <td id=\"T_ab78c_row4_col1\" class=\"data row4 col1\" >it</td>\n",
       "      <td id=\"T_ab78c_row4_col2\" class=\"data row4 col2\" >Italian</td>\n",
       "      <td id=\"T_ab78c_row4_col3\" class=\"data row4 col3\" >104952</td>\n",
       "      <td id=\"T_ab78c_row4_col4\" class=\"data row4 col4\" >204641</td>\n",
       "      <td id=\"T_ab78c_row4_col5\" class=\"data row4 col5\" >1.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ab78c_row5_col0\" class=\"data row5 col0\" >🇩🇪</td>\n",
       "      <td id=\"T_ab78c_row5_col1\" class=\"data row5 col1\" >de</td>\n",
       "      <td id=\"T_ab78c_row5_col2\" class=\"data row5 col2\" >German</td>\n",
       "      <td id=\"T_ab78c_row5_col3\" class=\"data row5 col3\" >92316</td>\n",
       "      <td id=\"T_ab78c_row5_col4\" class=\"data row5 col4\" >208453</td>\n",
       "      <td id=\"T_ab78c_row5_col5\" class=\"data row5 col5\" >2.258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ab78c_row6_col0\" class=\"data row6 col0\" >🇵🇱</td>\n",
       "      <td id=\"T_ab78c_row6_col1\" class=\"data row6 col1\" >pl</td>\n",
       "      <td id=\"T_ab78c_row6_col2\" class=\"data row6 col2\" >Polish</td>\n",
       "      <td id=\"T_ab78c_row6_col3\" class=\"data row6 col3\" >49354</td>\n",
       "      <td id=\"T_ab78c_row6_col4\" class=\"data row6 col4\" >138230</td>\n",
       "      <td id=\"T_ab78c_row6_col5\" class=\"data row6 col5\" >2.801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ab78c_row7_col0\" class=\"data row7 col0\" >🇨🇳</td>\n",
       "      <td id=\"T_ab78c_row7_col1\" class=\"data row7 col1\" >zh</td>\n",
       "      <td id=\"T_ab78c_row7_col2\" class=\"data row7 col2\" >Chinese</td>\n",
       "      <td id=\"T_ab78c_row7_col3\" class=\"data row7 col3\" >63073</td>\n",
       "      <td id=\"T_ab78c_row7_col4\" class=\"data row7 col4\" >183660</td>\n",
       "      <td id=\"T_ab78c_row7_col5\" class=\"data row7 col5\" >2.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ab78c_row8_col0\" class=\"data row8 col0\" >🇷🇺</td>\n",
       "      <td id=\"T_ab78c_row8_col1\" class=\"data row8 col1\" >ru</td>\n",
       "      <td id=\"T_ab78c_row8_col2\" class=\"data row8 col2\" >Russian</td>\n",
       "      <td id=\"T_ab78c_row8_col3\" class=\"data row8 col3\" >86594</td>\n",
       "      <td id=\"T_ab78c_row8_col4\" class=\"data row8 col4\" >312824</td>\n",
       "      <td id=\"T_ab78c_row8_col5\" class=\"data row8 col5\" >3.613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ab78c_row9_col0\" class=\"data row9 col0\" >🇰🇷</td>\n",
       "      <td id=\"T_ab78c_row9_col1\" class=\"data row9 col1\" >ko</td>\n",
       "      <td id=\"T_ab78c_row9_col2\" class=\"data row9 col2\" >Korean</td>\n",
       "      <td id=\"T_ab78c_row9_col3\" class=\"data row9 col3\" >55708</td>\n",
       "      <td id=\"T_ab78c_row9_col4\" class=\"data row9 col4\" >243870</td>\n",
       "      <td id=\"T_ab78c_row9_col5\" class=\"data row9 col5\" >4.378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x397bb5660>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "n_articles = 200  # or 30, 50, 100, etc.\n",
    "\n",
    "encoder = get_encoder(encoding_name=\"cl100k_base\")\n",
    "results = analyze_multiple_languages(LANGUAGES, n_articles, encoder=encoder)\n",
    "print_language_analysis_table_pandas(results, language_dict=LANGUAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the result for `o200k_base`, GPT-4o's tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_90e40\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_90e40_level0_col0\" class=\"col_heading level0 col0\" >Flag</th>\n",
       "      <th id=\"T_90e40_level0_col1\" class=\"col_heading level0 col1\" >Code</th>\n",
       "      <th id=\"T_90e40_level0_col2\" class=\"col_heading level0 col2\" >Language</th>\n",
       "      <th id=\"T_90e40_level0_col3\" class=\"col_heading level0 col3\" >Words</th>\n",
       "      <th id=\"T_90e40_level0_col4\" class=\"col_heading level0 col4\" >Tokens</th>\n",
       "      <th id=\"T_90e40_level0_col5\" class=\"col_heading level0 col5\" >Tokens/Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_90e40_row0_col0\" class=\"data row0 col0\" >🇺🇸</td>\n",
       "      <td id=\"T_90e40_row0_col1\" class=\"data row0 col1\" >en</td>\n",
       "      <td id=\"T_90e40_row0_col2\" class=\"data row0 col2\" >English</td>\n",
       "      <td id=\"T_90e40_row0_col3\" class=\"data row0 col3\" >121339</td>\n",
       "      <td id=\"T_90e40_row0_col4\" class=\"data row0 col4\" >168038</td>\n",
       "      <td id=\"T_90e40_row0_col5\" class=\"data row0 col5\" >1.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_90e40_row1_col0\" class=\"data row1 col0\" >🇪🇸</td>\n",
       "      <td id=\"T_90e40_row1_col1\" class=\"data row1 col1\" >es</td>\n",
       "      <td id=\"T_90e40_row1_col2\" class=\"data row1 col2\" >Spanish</td>\n",
       "      <td id=\"T_90e40_row1_col3\" class=\"data row1 col3\" >108311</td>\n",
       "      <td id=\"T_90e40_row1_col4\" class=\"data row1 col4\" >170586</td>\n",
       "      <td id=\"T_90e40_row1_col5\" class=\"data row1 col5\" >1.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_90e40_row2_col0\" class=\"data row2 col0\" >🇵🇹</td>\n",
       "      <td id=\"T_90e40_row2_col1\" class=\"data row2 col1\" >pt</td>\n",
       "      <td id=\"T_90e40_row2_col2\" class=\"data row2 col2\" >Portuguese</td>\n",
       "      <td id=\"T_90e40_row2_col3\" class=\"data row2 col3\" >78972</td>\n",
       "      <td id=\"T_90e40_row2_col4\" class=\"data row2 col4\" >126866</td>\n",
       "      <td id=\"T_90e40_row2_col5\" class=\"data row2 col5\" >1.606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_90e40_row3_col0\" class=\"data row3 col0\" >🇫🇷</td>\n",
       "      <td id=\"T_90e40_row3_col1\" class=\"data row3 col1\" >fr</td>\n",
       "      <td id=\"T_90e40_row3_col2\" class=\"data row3 col2\" >French</td>\n",
       "      <td id=\"T_90e40_row3_col3\" class=\"data row3 col3\" >97867</td>\n",
       "      <td id=\"T_90e40_row3_col4\" class=\"data row3 col4\" >157370</td>\n",
       "      <td id=\"T_90e40_row3_col5\" class=\"data row3 col5\" >1.608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_90e40_row4_col0\" class=\"data row4 col0\" >🇮🇹</td>\n",
       "      <td id=\"T_90e40_row4_col1\" class=\"data row4 col1\" >it</td>\n",
       "      <td id=\"T_90e40_row4_col2\" class=\"data row4 col2\" >Italian</td>\n",
       "      <td id=\"T_90e40_row4_col3\" class=\"data row4 col3\" >92080</td>\n",
       "      <td id=\"T_90e40_row4_col4\" class=\"data row4 col4\" >165649</td>\n",
       "      <td id=\"T_90e40_row4_col5\" class=\"data row4 col5\" >1.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_90e40_row5_col0\" class=\"data row5 col0\" >🇩🇪</td>\n",
       "      <td id=\"T_90e40_row5_col1\" class=\"data row5 col1\" >de</td>\n",
       "      <td id=\"T_90e40_row5_col2\" class=\"data row5 col2\" >German</td>\n",
       "      <td id=\"T_90e40_row5_col3\" class=\"data row5 col3\" >107755</td>\n",
       "      <td id=\"T_90e40_row5_col4\" class=\"data row5 col4\" >201239</td>\n",
       "      <td id=\"T_90e40_row5_col5\" class=\"data row5 col5\" >1.868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_90e40_row6_col0\" class=\"data row6 col0\" >🇨🇳</td>\n",
       "      <td id=\"T_90e40_row6_col1\" class=\"data row6 col1\" >zh</td>\n",
       "      <td id=\"T_90e40_row6_col2\" class=\"data row6 col2\" >Chinese</td>\n",
       "      <td id=\"T_90e40_row6_col3\" class=\"data row6 col3\" >88272</td>\n",
       "      <td id=\"T_90e40_row6_col4\" class=\"data row6 col4\" >176034</td>\n",
       "      <td id=\"T_90e40_row6_col5\" class=\"data row6 col5\" >1.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_90e40_row7_col0\" class=\"data row7 col0\" >🇷🇺</td>\n",
       "      <td id=\"T_90e40_row7_col1\" class=\"data row7 col1\" >ru</td>\n",
       "      <td id=\"T_90e40_row7_col2\" class=\"data row7 col2\" >Russian</td>\n",
       "      <td id=\"T_90e40_row7_col3\" class=\"data row7 col3\" >104359</td>\n",
       "      <td id=\"T_90e40_row7_col4\" class=\"data row7 col4\" >240536</td>\n",
       "      <td id=\"T_90e40_row7_col5\" class=\"data row7 col5\" >2.305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_90e40_row8_col0\" class=\"data row8 col0\" >🇵🇱</td>\n",
       "      <td id=\"T_90e40_row8_col1\" class=\"data row8 col1\" >pl</td>\n",
       "      <td id=\"T_90e40_row8_col2\" class=\"data row8 col2\" >Polish</td>\n",
       "      <td id=\"T_90e40_row8_col3\" class=\"data row8 col3\" >51074</td>\n",
       "      <td id=\"T_90e40_row8_col4\" class=\"data row8 col4\" >132056</td>\n",
       "      <td id=\"T_90e40_row8_col5\" class=\"data row8 col5\" >2.586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_90e40_row9_col0\" class=\"data row9 col0\" >🇰🇷</td>\n",
       "      <td id=\"T_90e40_row9_col1\" class=\"data row9 col1\" >ko</td>\n",
       "      <td id=\"T_90e40_row9_col2\" class=\"data row9 col2\" >Korean</td>\n",
       "      <td id=\"T_90e40_row9_col3\" class=\"data row9 col3\" >46488</td>\n",
       "      <td id=\"T_90e40_row9_col4\" class=\"data row9 col4\" >140966</td>\n",
       "      <td id=\"T_90e40_row9_col5\" class=\"data row9 col5\" >3.032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x38e50ada0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "n_articles = 200  # or 30, 50, 100, etc.\n",
    "\n",
    "encoder = get_encoder(encoding_name=\"o200k_base\")\n",
    "results = analyze_multiple_languages(LANGUAGES, n_articles, encoder=encoder)\n",
    "print_language_analysis_table_pandas(results, language_dict=LANGUAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "By analyzing random Wikipedia articles in many languages, we arrived at much more nuanced token-per-word ratios instead of just using the rough guesstimate of 1.3 tokens per word. We are seeing different results, even for English with a ratio of 1.4. For other languages based on the latin alphabet, we're in a range of 1.6 to 1.9. We're also seeing that the tokenizer matters. The new GPT-4o tokenizer, which has double the vocabulary size compared to the previous version, tokenizes Chinese more efficiently. The tokens per word ratio dropped from around 3.0 to 2.0.\n",
    "\n",
    "In closing, we need to recognize that this analysis is limited to Wikipedia articles, which tend to have more complicated vocabulary compared to simpler texts. Additionally, Wikipedia articles contain a lot of markup like headings or tables. Since we removed non-word tokens when counting words, the tokens per word ratio increases because the LLM-tokenizer includes the complete text. If you would tokenize a novel, the ratio is most likely less due to having less markup.\n",
    "\n",
    "Of course, there is still plenty of room for further practical analysis. For instance, it would be interesting to use different tokenizers or to tokenize various types of texts like novels, technical documentation, or conversation transcripts to analyze token-per-word ratios across different datasets. Nonetheless, I hope that this analysis helps you develop a more intuitive understanding of estimating token counts, and thereby estimating the costs involved in processing text with large language models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
