<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Wittmann">
<meta name="dcterms.date" content="2025-05-23">

<title>Empirically estimating tokens per word across languages – chrwittm.github.io</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-08e1269e8557b430e2197ccc38ae6770.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-GF3YYKQQNH"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-GF3YYKQQNH', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"express",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":"",
"website_privacy_policy_url":"/privacy.html"
  ,
"language":"en"
  });
});
</script> 
  
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">chrwittm.github.io</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../license.html"> 
<span class="menu-text">License</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../impressum.html"> 
<span class="menu-text">Impressum</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../privacy.html"> 
<span class="menu-text">Privacy</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/chrwittm"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/chrwittm"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://de.linkedin.com/in/chrwittm"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.kaggle.com/christianwittmann"> <i class="bi bi-file-earmark-code" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Empirically estimating tokens per word across languages</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">tokenization</div>
                <div class="quarto-category">nlp</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Christian Wittmann </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 23, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>Tokens are the new currency of generative AI. We’re paying for generative AI usage in tokens, sometimes directly via APIs, more often invisibly when using generative AI apps. But how many tokens does a given piece of text actually contain? Can you estimate this intuitively?</p>
<p>Personally, I can’t. Even though estimating the number of words is not trivial either (more on that later), I was looking for a simple rule of thumb on how to convert the number of words into the number of tokens. The simple answer is that English text contains on average <a href="https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them">1.3 tokens per word</a>. But how about other languages? German, for example, tends to have longer words than English. I was surprised that I couldn’t find convincing empirical research on this topic, hence I decided to conduct my own.</p>
<p>My approach was straightforward: I tokenized a lot of random Wikipedia articles in various languages and counted their words to determine a real token-per-word ratio. Here is the result, and subsequently, I will explain how I arrived at this result.</p>
<style>
  figure {
    display: block;
    margin-left: auto;
    margin-right: auto;
    text-align: center;
  }
</style>
<figure class="figure">
<img src="tokens-per-word2.png" alt="Token-per-word ratios across languages" style="width:50%;" class="figure-img">
<figcaption>
Token-per-word ratios across languages
</figcaption>
</figure>
<section id="why-measuring-tokens-per-word-is-important" class="level2">
<h2 class="anchored" data-anchor-id="why-measuring-tokens-per-word-is-important">Why measuring tokens per word is important</h2>
<p>With the rapidly declining cost per token, one might argue that token count doesn’t really matter anymore. While there’s merit to this viewpoint, I still believe it’s highly valuable to have an intuitive understanding of token counts for several reasons.</p>
<p>First, despite significant increases, context windows remain comparatively finite. Understanding roughly how many tokens a text contains helps you reason about what realistically fits within these limits.</p>
<p>Second, the foundational measurement for text processing is tokens per word, making this ratio essential for intuitive estimation.</p>
<p>Regarding cost, it’s true that for an individual prompt, the token cost is negligible. However, at enterprise scale, token counts can quickly become significant. Particularly when designing AI applications at scale, having reliable ballpark numbers can substantially impact decisions.</p>
</section>
<section id="related-work" class="level2">
<h2 class="anchored" data-anchor-id="related-work">Related work</h2>
<p>Research in this area is limited. The most relevant papers and posts I found are:</p>
<ul>
<li><a href="https://arxiv.org/abs/2305.13707">Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models</a>: Even though this paper has a similar idea, it is more focused on cost and fairness, and there’s no figure in there for direct token-per-word ratio.</li>
<li><a href="https://www.artfish.ai/p/all-languages-are-not-created-tokenized">All Languages Are NOT Created (Tokenized) Equal</a>: Similar research question, but the main result is comparative, i.e.&nbsp;how much longer other languages tokenize compared to English. I will use this as reference to verify my results.</li>
</ul>
</section>
<section id="how-to-tokenize" class="level2">
<h2 class="anchored" data-anchor-id="how-to-tokenize">How to tokenize</h2>
<p>In this blog post, I’ll focus on OpenAI’s tokenizers <code>cl100k_base</code> (for GPT-4) and <code>o200k_base</code> (for GPT-4o):</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Encoding name</th>
<th>OpenAI models</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>o200k_base</code></td>
<td><code>gpt-4o</code>, <code>gpt-4o-mini</code></td>
</tr>
<tr class="even">
<td><code>cl100k_base</code></td>
<td><code>gpt-4-turbo</code>, <code>gpt-4</code>, <code>gpt-3.5-turbo</code></td>
</tr>
</tbody>
</table>
<p>Source: <a href="https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb">How to count tokens with tiktoken</a></p>
<p>Tokenizing a given text is actually quite straightforward, simply pip install the <code>tiktoken</code> library, and you’re ready to start tokenizing:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">!pip</span> install tiktoken</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="cell-7" class="cell" data-execution_count="64">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tiktoken</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_encoder(encoding_name<span class="op">=</span><span class="st">"o200k_base"</span>):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns a tiktoken encoder. Defaults to GPT-4o's tokenizer."""</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tiktoken.get_encoding(encoding_name)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_tokens(text: <span class="bu">str</span>, encoder<span class="op">=</span><span class="va">None</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Counts the number of tokens in the input text using the specified encoder.</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">    If no encoder is provided, a new one will be created.</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> encoder <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        encoder <span class="op">=</span> get_encoder()</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">len</span>(encoder.encode(text))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-8" class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> get_encoder(encoding_name<span class="op">=</span><span class="st">"o200k_base"</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">"This is a simple test sentence to see how tokenization works."</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Example Text: </span><span class="sc">{</span>input_text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tokens:       </span><span class="sc">{</span>encoder<span class="sc">.</span>encode(input_text)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Token Count:  </span><span class="sc">{</span>count_tokens(input_text, encoder)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Example Text: This is a simple test sentence to see how tokenization works.
Tokens:       [2500, 382, 261, 4705, 1746, 21872, 316, 1921, 1495, 6602, 2860, 5882, 13]
Token Count:  13</code></pre>
</div>
</div>
</section>
<section id="reading-wikipedia-articles" class="level2">
<h2 class="anchored" data-anchor-id="reading-wikipedia-articles">Reading Wikipedia articles</h2>
<p>Next, we’ll need some real text data to tokenize. We’ll use random Wikipedia articles because they’re easily accessible in virtually any language and provide diverse content suitable for generalization. I’m assuming that the token-per-word ratio becomes relatively constant as datasets grow larger, as individual variations even out.</p>
<p>To fetch articles programmatically, we can conveniently use the Wikipedia API, available via the Python package <code>wikipedia</code>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install wikipedia</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>With this setup, we can easily retrieve and tokenize diverse text samples across different languages.</p>
<div id="cell-10" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> wikipediaapi</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> urllib.parse <span class="im">import</span> unquote</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_wikipedia_article(language: <span class="bu">str</span> <span class="op">=</span> <span class="st">"en"</span>, title: <span class="bu">str</span> <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> <span class="bu">tuple</span>[<span class="bu">str</span>, <span class="bu">str</span>]:</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Fetches the plain text of a Wikipedia article.</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">    If `title` is None, a random article is fetched.</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    wiki <span class="op">=</span> wikipediaapi.Wikipedia(</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        language<span class="op">=</span>language,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        user_agent<span class="op">=</span><span class="st">"TokenCountResearch/1.0 (chrwittm@gmail.com)"</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> title <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get a random article by following a redirect</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        url <span class="op">=</span> <span class="ss">f"https://</span><span class="sc">{</span>language<span class="sc">}</span><span class="ss">.wikipedia.org/wiki/Special:Random"</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> requests.get(url, allow_redirects<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> response.url.split(<span class="st">"/wiki/"</span>)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> unquote(title)  <span class="co"># 🔧 decode Unicode</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    page <span class="op">=</span> wiki.page(title)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> page.exists():</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Article '</span><span class="sc">{</span>title<span class="sc">}</span><span class="ss">' not found in language '</span><span class="sc">{</span>language<span class="sc">}</span><span class="ss">'."</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> title, <span class="st">""</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> title, page.text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s read the Wikipedia article on <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">Artificial intelligence</a> as an example:</p>
<div id="cell-12" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>title, text <span class="op">=</span> get_wikipedia_article(language<span class="op">=</span><span class="st">"en"</span>, title<span class="op">=</span><span class="st">"Artificial intelligence"</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(text[:<span class="dv">500</span>])  <span class="co"># Print preview</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Artificial intelligence (AI) refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called A</code></pre>
</div>
</div>
<p>For reading a random German Wikipedia article, you can use the following Python code:</p>
<div id="cell-14" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>title_random_de, text_random_de <span class="op">=</span> get_wikipedia_article(language<span class="op">=</span><span class="st">"de"</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Random German article title: </span><span class="sc">{</span>title_random_de<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Article preview:</span><span class="ch">\n</span><span class="sc">{</span>text_random_de[:<span class="dv">500</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Random German article title: IC_583
Article preview:
IC 583 ist eine spiralförmige Radiogalaxie vom Hubble-Typ Sbc im Sternbild Löwe an der Ekliptik. Sie ist schätzungsweise 349 Millionen Lichtjahre von der Milchstraße entfernt und hat einen Durchmesser von etwa 145.000 Lichtjahren. Vom Sonnensystem aus entfernt sich die Galaxie mit einer errechneten Radialgeschwindigkeit von näherungsweise 7.900 Kilometern pro Sekunde.
Gemeinsam mit IC 582 bildet sie das Galaxienpaar Holm 155 und mit PGC 1542326 ein gravitativ gebundenes Triplet. Im selben Himmel</code></pre>
</div>
</div>
</section>
<section id="counting-words-is-surprisingly-tricky" class="level2">
<h2 class="anchored" data-anchor-id="counting-words-is-surprisingly-tricky">Counting words is surprisingly tricky</h2>
<p>Counting words seems straightforward at first glance, but it quickly becomes complex once you dig deeper. Initially, my approach was quite naive: splitting text simply based on whitespace. This method works reasonably well for languages using spaces as word separators, such as English or German, although even here, it fails to handle contractions properly (e.g., “can’t,” “don’t,” or “it’s”). For languages with fundamentally different writing systems (Chinese, Japanese, or Korean), this whitespace-based approach completely breaks down because these languages either rarely or never use spaces to separate words. Clearly, a more sophisticated approach was required.</p>
<p>To address this, I turned to spaCy, a robust and multilingual NLP library that intelligently segments text into words by using language-specific models. SpaCy considers linguistic nuances, punctuation, contractions, and special characters, providing accurate and reliable word counting across diverse languages. The spaCy models significantly improve word-count reliability compared to a naive whitespace-based method.</p>
<section id="setup-of-spacy" class="level3">
<h3 class="anchored" data-anchor-id="setup-of-spacy">Setup of spaCy</h3>
<p>To start using spaCy, you’ll first need to install it:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install spacy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Then, you’ll need to download the language-specific models for the languages you’re working with. For example, for English, German, and Chinese, execute:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-m</span> spacy download en_core_web_sm</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-m</span> spacy download de_core_news_sm</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-m</span> spacy download zh_core_web_sm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To automate the installation process, you can run the following cell to check which language models you have already installed. Subsequently, we’ll install missing language packages.</p>
<div id="cell-17" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> check_installed_spacy_models():</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">    For each installed spaCy model (by name), load it and print key metadata:</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">      - Model Name</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">      - Language code</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">      - Model Version</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">      - Required spaCy version (if available)</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">      - Pipeline components</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> spacy</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> spacy.cli.validate <span class="im">import</span> get_installed_models</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    installed_model_names <span class="op">=</span> get_installed_models()</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> installed_model_names:</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"No spaCy models found."</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"spaCy models found:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Model Name'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Language'</span><span class="sc">:&lt;10}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Model Ver.'</span><span class="sc">:&lt;12}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'spaCy Ver.'</span><span class="sc">:&lt;12}</span><span class="ss"> Pipeline"</span>)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> model_name <span class="kw">in</span> installed_model_names:</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Attempt to load the model to read its meta</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>            nlp <span class="op">=</span> spacy.load(model_name)</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>            meta <span class="op">=</span> <span class="bu">getattr</span>(nlp, <span class="st">"meta"</span>, {})</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Extract metadata safely</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>            lang <span class="op">=</span> meta.get(<span class="st">"lang"</span>, <span class="st">"n/a"</span>)</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>            version <span class="op">=</span> meta.get(<span class="st">"version"</span>, <span class="st">"n/a"</span>)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>            spacy_req <span class="op">=</span> meta.get(<span class="st">"spacy_version"</span>, <span class="st">"n/a"</span>)</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>            pipeline <span class="op">=</span> meta.get(<span class="st">"pipeline"</span>, [])</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"</span><span class="sc">{</span>model_name<span class="sc">:&lt;20}</span><span class="ss"> "</span></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"</span><span class="sc">{</span>lang<span class="sc">:&lt;10}</span><span class="ss"> "</span></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"</span><span class="sc">{</span>version<span class="sc">:&lt;12}</span><span class="ss"> "</span></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"</span><span class="sc">{</span>spacy_req<span class="sc">:&lt;12}</span><span class="ss"> "</span></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"</span><span class="sc">{</span><span class="st">','</span><span class="sc">.</span>join(pipeline)<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>model_name<span class="sc">:&lt;20}</span><span class="ss"> FAILED TO LOAD: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>check_installed_spacy_models()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>spaCy models found:

Model Name           Language   Model Ver.   spaCy Ver.   Pipeline
----------------------------------------------------------------------
fr_core_news_sm      fr         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,morphologizer,parser,attribute_ruler,lemmatizer,ner
es_core_news_sm      es         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,morphologizer,parser,attribute_ruler,lemmatizer,ner
ja_core_news_sm      ja         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,morphologizer,parser,attribute_ruler,ner
pl_core_news_sm      pl         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,morphologizer,parser,lemmatizer,tagger,attribute_ruler,ner
it_core_news_sm      it         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,morphologizer,tagger,parser,lemmatizer,attribute_ruler,ner
ko_core_news_sm      ko         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,tagger,morphologizer,parser,lemmatizer,attribute_ruler,ner
en_core_web_sm       en         3.7.1        &gt;=3.7.2,&lt;3.8.0 tok2vec,tagger,parser,attribute_ruler,lemmatizer,ner
ru_core_news_sm      ru         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,morphologizer,parser,attribute_ruler,lemmatizer,ner
de_core_news_sm      de         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,tagger,morphologizer,parser,lemmatizer,attribute_ruler,ner
pt_core_news_sm      pt         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,morphologizer,parser,lemmatizer,attribute_ruler,ner
zh_core_web_sm       zh         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,tagger,parser,attribute_ruler,ner</code></pre>
</div>
</div>
<p>spaCy models’ names follow two main naming conventions depending on the language and source corpus. For English, models are typically named like <code>en_core_web_sm</code>, where “web” refers to the OntoNotes web-based corpus used for training. For most other languages, models are named like <code>de_core_news_sm</code>, reflecting their training on news-domain texts from Universal Dependencies corpora. While the difference can be confusing at first, it reflects the underlying data sources and training pipelines. To keep things simple and avoid guesswork, we define the full model name explicitly in our language configuration dictionary.</p>
<p>I picked the following languages for this analysis, because they are supported by spaCy, and I found them to be interesting. This means that they are either commonly used or they are otherwise interesting because they use non-Latin scripts.</p>
<div id="cell-19" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>LANGUAGES <span class="op">=</span> {</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"de"</span>: {<span class="st">"name"</span>: <span class="st">"German"</span>,      <span class="st">"model"</span>: <span class="st">"de_core_news_sm"</span>,    <span class="st">"emoji"</span>: <span class="st">"🇩🇪"</span>},</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"en"</span>: {<span class="st">"name"</span>: <span class="st">"English"</span>,     <span class="st">"model"</span>: <span class="st">"en_core_web_sm"</span>,     <span class="st">"emoji"</span>: <span class="st">"🇺🇸"</span>},</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"es"</span>: {<span class="st">"name"</span>: <span class="st">"Spanish"</span>,     <span class="st">"model"</span>: <span class="st">"es_core_news_sm"</span>,    <span class="st">"emoji"</span>: <span class="st">"🇪🇸"</span>},</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"fr"</span>: {<span class="st">"name"</span>: <span class="st">"French"</span>,      <span class="st">"model"</span>: <span class="st">"fr_core_news_sm"</span>,    <span class="st">"emoji"</span>: <span class="st">"🇫🇷"</span>},</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"it"</span>: {<span class="st">"name"</span>: <span class="st">"Italian"</span>,     <span class="st">"model"</span>: <span class="st">"it_core_news_sm"</span>,    <span class="st">"emoji"</span>: <span class="st">"🇮🇹"</span>},</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">#"ja": {"name": "Japanese",    "model": "ja_core_news_sm",    "emoji": "🇯🇵"},</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ko"</span>: {<span class="st">"name"</span>: <span class="st">"Korean"</span>,      <span class="st">"model"</span>: <span class="st">"ko_core_news_sm"</span>,    <span class="st">"emoji"</span>: <span class="st">"🇰🇷"</span>},</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"pl"</span>: {<span class="st">"name"</span>: <span class="st">"Polish"</span>,      <span class="st">"model"</span>: <span class="st">"pl_core_news_sm"</span>,    <span class="st">"emoji"</span>: <span class="st">"🇵🇱"</span>},</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"pt"</span>: {<span class="st">"name"</span>: <span class="st">"Portuguese"</span>,  <span class="st">"model"</span>: <span class="st">"pt_core_news_sm"</span>,    <span class="st">"emoji"</span>: <span class="st">"🇵🇹"</span>},</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ru"</span>: {<span class="st">"name"</span>: <span class="st">"Russian"</span>,     <span class="st">"model"</span>: <span class="st">"ru_core_news_sm"</span>,    <span class="st">"emoji"</span>: <span class="st">"🇷🇺"</span>},</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"zh"</span>: {<span class="st">"name"</span>: <span class="st">"Chinese"</span>,     <span class="st">"model"</span>: <span class="st">"zh_core_web_sm"</span>,     <span class="st">"emoji"</span>: <span class="st">"🇨🇳"</span>},</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-20" class="cell" data-execution_count="45">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> install_spacy_models(language_dict):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Tries to install spaCy language models for all entries in the language_dict.</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Requires model names to be specified per language.</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> subprocess</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> sys</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> lang_code, data <span class="kw">in</span> language_dict.items():</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> data[<span class="st">"model"</span>]</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Installing spaCy model for </span><span class="sc">{</span>data[<span class="st">'name'</span>]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>lang_code<span class="sc">}</span><span class="ss">) — </span><span class="sc">{</span>model<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>            subprocess.run([sys.executable, <span class="st">"-m"</span>, <span class="st">"spacy"</span>, <span class="st">"download"</span>, model], check<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> subprocess.CalledProcessError:</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"❌ Failed to install spaCy model for </span><span class="sc">{</span>lang_code<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>model<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>install_languages <span class="op">=</span> <span class="va">False</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> install_languages:</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    install_spacy_models(LANGUAGES)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<blockquote class="blockquote">
<p>Note: If you have installed new languages, be sure to restart your Python kernel.</p>
</blockquote>
</section>
<section id="counting-words-with-spacy" class="level3">
<h3 class="anchored" data-anchor-id="counting-words-with-spacy">Counting Words with spaCy</h3>
<p>Now, we are almost ready to count words using spaCy’s tokenizer. It’s important to clarify here that the term “token” can be somewhat ambiguous and context-dependent. In natural language processing libraries like spaCy, a “token” typically means a word, punctuation mark, or similar meaningful unit of text. However, when dealing with Large Language Models (LLMs), a “token” usually refers to a subword unit produced by the tokenizer. Therefore, always keep this distinction in mind to avoid confusion as you follow along.</p>
<p>For what we want to do, we need to separate the spaCy tokens into words tokens and other tokens like punctuation, spaces, etc. as illustrated by the following example:</p>
<div id="cell-23" class="cell" data-execution_count="46">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple cache/dictionary to hold loaded spaCy models:</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>_spacy_models <span class="op">=</span> {}</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_spacy_model(language_code: <span class="bu">str</span> <span class="op">=</span> <span class="st">"en"</span>):</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Loads and caches the spaCy language model for the given language code.</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Uses the model name defined in the LANGUAGES dict.</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Falls back to a blank model if the specified model is not available.</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> language_code <span class="kw">not</span> <span class="kw">in</span> _spacy_models:</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>        model_name <span class="op">=</span> LANGUAGES.get(language_code, {}).get(<span class="st">"model"</span>, <span class="va">None</span>)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> model_name:</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>                _spacy_models[language_code] <span class="op">=</span> spacy.load(model_name)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"No model defined for language code: '</span><span class="sc">{</span>language_code<span class="sc">}</span><span class="ss">'"</span>)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> (<span class="pp">OSError</span>, <span class="pp">ValueError</span>) <span class="im">as</span> e:</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"⚠️ Could not load model '</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">' for language '</span><span class="sc">{</span>language_code<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"→ Falling back to blank spaCy model (basic tokenization only)."</span>)</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>            _spacy_models[language_code] <span class="op">=</span> spacy.blank(language_code)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> _spacy_models[language_code]</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_spacy_tokens(text: <span class="bu">str</span>, language_code: <span class="bu">str</span> <span class="op">=</span> <span class="st">"en"</span>) <span class="op">-&gt;</span> <span class="bu">tuple</span>[<span class="bu">list</span>[<span class="bu">str</span>], <span class="bu">list</span>[<span class="bu">str</span>]]:</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="co">    Tokenizes the input text using spaCy's tokenizer.</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns two lists: one with spaCy tokens (words) and one with omitted tokens </span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="co">    (punctuation, spaces, symbols, etc.).</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    nlp <span class="op">=</span> get_spacy_model(language_code)</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    doc <span class="op">=</span> nlp(text)</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>    punctuation_set <span class="op">=</span> <span class="bu">set</span>(string.punctuation)</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>    word_tokens <span class="op">=</span> [</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>        t <span class="cf">for</span> t <span class="kw">in</span> doc </span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> t.is_space </span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>           <span class="kw">and</span> <span class="kw">not</span> t.is_punct </span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>           <span class="kw">and</span> t.pos_ <span class="op">!=</span> <span class="st">"SYM"</span> </span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>           <span class="kw">and</span> t.text <span class="kw">not</span> <span class="kw">in</span> punctuation_set</span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>    omitted_tokens <span class="op">=</span> [</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>        t <span class="cf">for</span> t <span class="kw">in</span> doc </span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> t.is_space </span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>           <span class="kw">or</span> t.is_punct </span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>           <span class="kw">or</span> t.pos_ <span class="op">==</span> <span class="st">"SYM"</span> </span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>           <span class="kw">or</span> t.text <span class="kw">in</span> punctuation_set</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> word_tokens, omitted_tokens</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-24" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>word_tokens, omitted_tokens <span class="op">=</span> get_spacy_tokens(<span class="st">"You're very tall! Do you play basketball?"</span>, language_code<span class="op">=</span><span class="st">"en"</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(word_tokens)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(omitted_tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[You, 're, very, tall, Do, you, play, basketball]
[!, ?]</code></pre>
</div>
</div>
<p>Thinking ahead: Making sure that we only count words will later drive up the token-per-word ratio because the LLM tokenizer will also tokenize punctuations and other markup in the Wikipedia articles like headings, tables, and lists.</p>
<p>Putting everything together, here is a function that counts words using spaCy’s tokenizer.</p>
<div id="cell-27" class="cell" data-execution_count="48">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_words_spacy(text: <span class="bu">str</span>, language_code: <span class="bu">str</span> <span class="op">=</span> <span class="st">"en"</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Counts words in the input text using spaCy's tokenizer.</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Skips punctuation/whitespace tokens.</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    nlp <span class="op">=</span> get_spacy_model(language_code)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    doc <span class="op">=</span> nlp(text)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    punctuation_set <span class="op">=</span> <span class="bu">set</span>(string.punctuation)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Filter out space/punctuation tokens:</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> [</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        t <span class="cf">for</span> t <span class="kw">in</span> doc </span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> t.is_space </span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>           <span class="kw">and</span> <span class="kw">not</span> t.is_punct </span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>           <span class="kw">and</span> t.pos_ <span class="op">!=</span> <span class="st">"SYM"</span> </span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>           <span class="kw">and</span> t.text <span class="kw">not</span> <span class="kw">in</span> punctuation_set</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">len</span>(tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Just for the fun of it, let’s compare spaCy’s word count with the naive whitespace-based method using the <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">English Wikipedia article on Artificial Intelligence</a>:</p>
<div id="cell-29" class="cell" data-execution_count="49">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_words_naive(text: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Counts the number of words in the input text using simple whitespace splitting."""</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">len</span>(text.split())</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Naive white-space word count: </span><span class="sc">{</span>count_words_naive(text)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SpaCy word count:             </span><span class="sc">{</span>count_words_spacy(text, language_code<span class="op">=</span><span class="st">'en'</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Naive white-space word count: 13233
SpaCy word count:             13443</code></pre>
</div>
</div>
<p>The difference between the naive white-space word count and the spaCy word count is due to the fact that spaCy’s tokenizer recognizes contractions, hyphenation, and special characters more accurately. For example, spaCy tokenizes “step-by-step” into three separate tokens: “step”, “by”, and “step”, whereas the naive white-space word count would only count it as one token.</p>
<p>Other languages, like Chinese, do not even use spaces to separate words. Here is an example of a Chinese sentence: “我喜欢吃苹果和香蕉。” which means “I like to eat apples and bananas.”</p>
<div id="cell-32" class="cell" data-execution_count="50">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>text_zh <span class="op">=</span> <span class="st">"我喜欢吃苹果和香蕉。"</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Expected words:               6"</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Naive white-space word count: </span><span class="sc">{</span>count_words_naive(text_zh)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SpaCy word count:             </span><span class="sc">{</span>count_words_spacy(text_zh, language_code<span class="op">=</span><span class="st">'zh'</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Expected words:               6
Naive white-space word count: 1
SpaCy word count:             6</code></pre>
</div>
</div>
</section>
</section>
<section id="calculating-the-token-per-word-ratio" class="level2">
<h2 class="anchored" data-anchor-id="calculating-the-token-per-word-ratio">Calculating the token-per-word ratio</h2>
<p>Finally, we can calculate the token-per-word ratio. Again, let’s use the Wikipedia article on <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">Artificial Intelligence</a> as an example:</p>
<div id="cell-34" class="cell" data-execution_count="51">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_tokens_per_word(text: <span class="bu">str</span>, language_code: <span class="bu">str</span> <span class="op">=</span> <span class="st">"en"</span>, encoder<span class="op">=</span><span class="va">None</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculates average number of tokens (tiktoken) per word (spaCy-based) for the given text.</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> count_words_spacy(text, language_code<span class="op">=</span>language_code)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> count_tokens(text, encoder<span class="op">=</span>encoder)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> words <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.0</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokens <span class="op">/</span> words</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Article: </span><span class="sc">{</span>title<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Words: </span><span class="sc">{</span>count_words_spacy(text, language_code<span class="op">=</span><span class="st">'en'</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tokens: </span><span class="sc">{</span>count_tokens(text, encoder)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tokens per word: </span><span class="sc">{</span>get_tokens_per_word(text<span class="op">=</span>text, language_code<span class="op">=</span><span class="st">'en'</span>, encoder<span class="op">=</span>encoder)<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Article: Artificial intelligence
Words: 13443
Tokens: 16875
Tokens per word: 1.255</code></pre>
</div>
</div>
</section>
<section id="batch-analysis-per-language" class="level2">
<h2 class="anchored" data-anchor-id="batch-analysis-per-language">Batch analysis per language</h2>
<p>For a first comparison across languages, let’s use 10 random articles for a few select languages to get a rough idea of the average token-per-word ratio.</p>
<div id="cell-36" class="cell" data-execution_count="52">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> analyze_wikipedia_language_sample(language: <span class="bu">str</span>, n: <span class="bu">int</span>, encoder<span class="op">=</span><span class="va">None</span>, verbose: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>) <span class="op">-&gt;</span> <span class="bu">dict</span>:</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Fetches exactly `n` valid random Wikipedia articles in the specified language and computes:</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co">    - total number of words</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="co">    - total number of tokens</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co">    - average tokens per word</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> time</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> encoder <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>        encoder <span class="op">=</span> get_encoder()</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    total_words <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    total_tokens <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    successful_articles <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    attempts <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> successful_articles <span class="op">&lt;</span> n:</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>        attempts <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>        title, text <span class="op">=</span> get_wikipedia_article(language<span class="op">=</span>language)</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> text.strip():</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span>  <span class="co"># skip and retry</span></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Updated to use spaCy-based word counting:</span></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>        words <span class="op">=</span> count_words_spacy(text, language_code<span class="op">=</span>language)</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> count_tokens(text, encoder<span class="op">=</span>encoder)</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> words <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> tokens <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>        total_words <span class="op">+=</span> words</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>        total_tokens <span class="op">+=</span> tokens</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>        successful_articles <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>        tokens_per_word <span class="op">=</span> tokens <span class="op">/</span> words</span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"[</span><span class="sc">{</span>successful_articles<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">] </span><span class="sc">{</span>title<span class="sc">}</span><span class="ss"> — Words: </span><span class="sc">{</span>words<span class="sc">}</span><span class="ss">, Tokens: </span><span class="sc">{</span>tokens<span class="sc">}</span><span class="ss">, Tokens/Word: </span><span class="sc">{</span>tokens_per_word<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>        time.sleep(<span class="fl">0.3</span>)  <span class="co"># polite pause</span></span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>    tokens_per_word_avg <span class="op">=</span> total_tokens <span class="op">/</span> total_words <span class="cf">if</span> total_words <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>        <span class="st">'language'</span>: language,</span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a>        <span class="st">'articles'</span>: successful_articles,</span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>        <span class="st">'total_words'</span>: total_words,</span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a>        <span class="st">'total_tokens'</span>: total_tokens,</span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a>        <span class="st">'tokens_per_word'</span>: tokens_per_word_avg</span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a>    }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-37" class="cell" data-execution_count="57">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> analyze_wikipedia_language_sample(language<span class="op">=</span><span class="st">"en"</span>, n<span class="op">=</span><span class="dv">10</span>, encoder<span class="op">=</span>encoder)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- Summary ---"</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> key, value <span class="kw">in</span> results.items():</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1/10] Paris_Gibson_Square_Museum_of_Art — Words: 5172, Tokens: 6995, Tokens/Word: 1.352
[2/10] Dalbergia_pseudobaronii — Words: 291, Tokens: 480, Tokens/Word: 1.649
[3/10] Niels_Wulfsberg — Words: 445, Tokens: 692, Tokens/Word: 1.555
[4/10] Brush_Script — Words: 215, Tokens: 328, Tokens/Word: 1.526
[5/10] Molly_Harper — Words: 352, Tokens: 445, Tokens/Word: 1.264
[6/10] Augsburg_Arena — Words: 355, Tokens: 587, Tokens/Word: 1.654
[7/10] Salmon_Arm_Airport — Words: 46, Tokens: 82, Tokens/Word: 1.783
[8/10] Sarah_LeFanu — Words: 211, Tokens: 309, Tokens/Word: 1.464
[9/10] Sir_James_Horlick,_1st_Baronet — Words: 348, Tokens: 493, Tokens/Word: 1.417
[10/10] 21st_Infantry_Division_(Russian_Empire) — Words: 104, Tokens: 228, Tokens/Word: 2.192

--- Summary ---
language: en
articles: 10
total_words: 7539
total_tokens: 10639
tokens_per_word: 1.41119511871601</code></pre>
</div>
</div>
<div id="cell-38" class="cell" data-execution_count="66">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> analyze_wikipedia_language_sample(language<span class="op">=</span><span class="st">"de"</span>, n<span class="op">=</span><span class="dv">10</span>, encoder<span class="op">=</span>encoder)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- Summary ---"</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> key, value <span class="kw">in</span> results.items():</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1/10] Budschak_(Bolhrad) — Words: 454, Tokens: 876, Tokens/Word: 1.930
[2/10] Montana-Territorium — Words: 334, Tokens: 615, Tokens/Word: 1.841
[3/10] Holme_Rose — Words: 481, Tokens: 886, Tokens/Word: 1.842
[4/10] Dolmen_von_Fontenaille — Words: 125, Tokens: 225, Tokens/Word: 1.800
[5/10] Monika_Wernicke — Words: 154, Tokens: 282, Tokens/Word: 1.831
[6/10] Friedrichshöhe_(Leichlingen) — Words: 197, Tokens: 398, Tokens/Word: 2.020
[7/10] Flügelaltar_von_Schloss_Tirol — Words: 1574, Tokens: 2804, Tokens/Word: 1.781
[8/10] Milenino_(Kursk) — Words: 198, Tokens: 426, Tokens/Word: 2.152
[9/10] Irmintraut_Richarz — Words: 197, Tokens: 402, Tokens/Word: 2.041
[10/10] Jürgen_Bolten — Words: 1241, Tokens: 2512, Tokens/Word: 2.024

--- Summary ---
language: de
articles: 10
total_words: 4955
total_tokens: 9426
tokens_per_word: 1.9023208879919273</code></pre>
</div>
</div>
<div id="cell-39" class="cell" data-execution_count="38">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> analyze_wikipedia_language_sample(language<span class="op">=</span><span class="st">"zh"</span>, n<span class="op">=</span><span class="dv">10</span>, encoder<span class="op">=</span>encoder)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- Summary ---"</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> key, value <span class="kw">in</span> results.items():</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1/10] 9mm警用轉輪手槍 — Words: 313, Tokens: 577, Tokens/Word: 1.843
[2/10] 十川誠志 — Words: 750, Tokens: 1960, Tokens/Word: 2.613
[3/10] 白晓卉 — Words: 225, Tokens: 439, Tokens/Word: 1.951
[4/10] 多椎半鱨 — Words: 57, Tokens: 137, Tokens/Word: 2.404
[5/10] 360图片 — Words: 121, Tokens: 190, Tokens/Word: 1.570
[6/10] 賈特人 — Words: 434, Tokens: 842, Tokens/Word: 1.940
[7/10] 维勒迪约-拉布卢埃尔 — Words: 181, Tokens: 440, Tokens/Word: 2.431
[8/10] U-161号潜艇_(1918年) — Words: 599, Tokens: 980, Tokens/Word: 1.636
[9/10] 比利肯 — Words: 668, Tokens: 1383, Tokens/Word: 2.070
[10/10] 桑省 — Words: 183, Tokens: 358, Tokens/Word: 1.956

--- Summary ---
language: zh
articles: 10
total_words: 3531
total_tokens: 7306
tokens_per_word: 2.069102237326536</code></pre>
</div>
</div>
</section>
<section id="final-analysis" class="level2">
<h2 class="anchored" data-anchor-id="final-analysis">Final Analysis</h2>
<p>Let’s put everything together and run the full analysis. The numbers start to converge at about 100 articles. To be on the safe side, let’s do 200 articles each.</p>
<div id="cell-41" class="cell" data-execution_count="67">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> analyze_multiple_languages(language_dict, n, encoder<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Analyzes multiple languages using their configuration from the LANGUAGES dictionary.</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co">    For each language, fetches `n` random Wikipedia articles and calculates:</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co">      - total word count (using spaCy)</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co">      - total token count (using tiktoken)</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co">      - average tokens per word</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns a list of dictionaries for easy tabular display.</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Each row contains: language code, name, total words, total tokens, tokens per word.</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> encoder <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>        encoder <span class="op">=</span> get_encoder()</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>    results_table <span class="op">=</span> []</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> lang_code, config <span class="kw">in</span> language_dict.items():</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>        lang_name <span class="op">=</span> config[<span class="st">"name"</span>]</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(f"\n🔍 Analyzing {lang_name} ({lang_code})...")</span></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>            summary <span class="op">=</span> analyze_wikipedia_language_sample(</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>                language<span class="op">=</span>lang_code,</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>                n<span class="op">=</span>n,</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>                encoder<span class="op">=</span>encoder,</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>                verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>            row <span class="op">=</span> {</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>                <span class="st">"language"</span>: lang_code,</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>                <span class="st">"name"</span>: lang_name,</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>                <span class="st">"total_words"</span>: summary[<span class="st">"total_words"</span>],</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>                <span class="st">"total_tokens"</span>: summary[<span class="st">"total_tokens"</span>],</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>                <span class="st">"tokens_per_word"</span>: summary[<span class="st">"tokens_per_word"</span>]</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>            results_table.append(row)</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"❌ Error processing </span><span class="sc">{</span>lang_name<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>lang_code<span class="sc">}</span><span class="ss">): </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> results_table</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-42" class="cell" data-execution_count="68">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_language_analysis_table_pandas(results_table, language_dict<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Creates and displays a pandas DataFrame from the results_table.</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Adds flag emoji (if available), replaces codes with names, sorts by Tokens/Word.</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Suppresses the index column in the Jupyter output.</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame(results_table)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> language_dict:</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>        df[<span class="st">"name"</span>] <span class="op">=</span> df[<span class="st">"language"</span>].<span class="bu">map</span>(<span class="kw">lambda</span> code: language_dict.get(code, {}).get(<span class="st">"name"</span>, code))</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>        df[<span class="st">"emoji"</span>] <span class="op">=</span> df[<span class="st">"language"</span>].<span class="bu">map</span>(<span class="kw">lambda</span> code: language_dict.get(code, {}).get(<span class="st">"emoji"</span>, <span class="st">""</span>))</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reorder and rename columns</span></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df[[<span class="st">"emoji"</span>, <span class="st">"language"</span>, <span class="st">"name"</span>, <span class="st">"total_words"</span>, <span class="st">"total_tokens"</span>, <span class="st">"tokens_per_word"</span>]]</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>    df.columns <span class="op">=</span> [<span class="st">"Flag"</span>, <span class="st">"Code"</span>, <span class="st">"Language"</span>, <span class="st">"Words"</span>, <span class="st">"Tokens"</span>, <span class="st">"Tokens/Word"</span>]</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sort by tokens per word (ascending)</span></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.sort_values(by<span class="op">=</span><span class="st">"Tokens/Word"</span>)</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Format and hide index (Jupyter only)</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>    styled_df <span class="op">=</span> df.style.<span class="bu">format</span>({<span class="st">"Tokens/Word"</span>: <span class="st">"</span><span class="sc">{:.3f}</span><span class="st">"</span>}).hide(axis<span class="op">=</span><span class="st">"index"</span>)</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>    display(styled_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Here is the result for <code>cl100k_base</code>, GPT-4’s tokenizer:</p>
<div id="cell-44" class="cell" data-execution_count="69">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>n_articles <span class="op">=</span> <span class="dv">200</span>  <span class="co"># or 30, 50, 100, etc.</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> get_encoder(encoding_name<span class="op">=</span><span class="st">"cl100k_base"</span>)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> analyze_multiple_languages(LANGUAGES, n_articles, encoder<span class="op">=</span>encoder)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>print_language_analysis_table_pandas(results, language_dict<span class="op">=</span>LANGUAGES)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<style type="text/css">
</style>

<table id="T_ab78c" class="caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th id="T_ab78c_level0_col0" class="col_heading level0 col0" data-quarto-table-cell-role="th">Flag</th>
<th id="T_ab78c_level0_col1" class="col_heading level0 col1" data-quarto-table-cell-role="th">Code</th>
<th id="T_ab78c_level0_col2" class="col_heading level0 col2" data-quarto-table-cell-role="th">Language</th>
<th id="T_ab78c_level0_col3" class="col_heading level0 col3" data-quarto-table-cell-role="th">Words</th>
<th id="T_ab78c_level0_col4" class="col_heading level0 col4" data-quarto-table-cell-role="th">Tokens</th>
<th id="T_ab78c_level0_col5" class="col_heading level0 col5" data-quarto-table-cell-role="th">Tokens/Word</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td id="T_ab78c_row0_col0" class="data row0 col0">🇺🇸</td>
<td id="T_ab78c_row0_col1" class="data row0 col1">en</td>
<td id="T_ab78c_row0_col2" class="data row0 col2">English</td>
<td id="T_ab78c_row0_col3" class="data row0 col3">87627</td>
<td id="T_ab78c_row0_col4" class="data row0 col4">129379</td>
<td id="T_ab78c_row0_col5" class="data row0 col5">1.476</td>
</tr>
<tr class="even">
<td id="T_ab78c_row1_col0" class="data row1 col0">🇪🇸</td>
<td id="T_ab78c_row1_col1" class="data row1 col1">es</td>
<td id="T_ab78c_row1_col2" class="data row1 col2">Spanish</td>
<td id="T_ab78c_row1_col3" class="data row1 col3">111473</td>
<td id="T_ab78c_row1_col4" class="data row1 col4">202116</td>
<td id="T_ab78c_row1_col5" class="data row1 col5">1.813</td>
</tr>
<tr class="odd">
<td id="T_ab78c_row2_col0" class="data row2 col0">🇫🇷</td>
<td id="T_ab78c_row2_col1" class="data row2 col1">fr</td>
<td id="T_ab78c_row2_col2" class="data row2 col2">French</td>
<td id="T_ab78c_row2_col3" class="data row2 col3">91423</td>
<td id="T_ab78c_row2_col4" class="data row2 col4">169248</td>
<td id="T_ab78c_row2_col5" class="data row2 col5">1.851</td>
</tr>
<tr class="even">
<td id="T_ab78c_row3_col0" class="data row3 col0">🇵🇹</td>
<td id="T_ab78c_row3_col1" class="data row3 col1">pt</td>
<td id="T_ab78c_row3_col2" class="data row3 col2">Portuguese</td>
<td id="T_ab78c_row3_col3" class="data row3 col3">96179</td>
<td id="T_ab78c_row3_col4" class="data row3 col4">182802</td>
<td id="T_ab78c_row3_col5" class="data row3 col5">1.901</td>
</tr>
<tr class="odd">
<td id="T_ab78c_row4_col0" class="data row4 col0">🇮🇹</td>
<td id="T_ab78c_row4_col1" class="data row4 col1">it</td>
<td id="T_ab78c_row4_col2" class="data row4 col2">Italian</td>
<td id="T_ab78c_row4_col3" class="data row4 col3">104952</td>
<td id="T_ab78c_row4_col4" class="data row4 col4">204641</td>
<td id="T_ab78c_row4_col5" class="data row4 col5">1.950</td>
</tr>
<tr class="even">
<td id="T_ab78c_row5_col0" class="data row5 col0">🇩🇪</td>
<td id="T_ab78c_row5_col1" class="data row5 col1">de</td>
<td id="T_ab78c_row5_col2" class="data row5 col2">German</td>
<td id="T_ab78c_row5_col3" class="data row5 col3">92316</td>
<td id="T_ab78c_row5_col4" class="data row5 col4">208453</td>
<td id="T_ab78c_row5_col5" class="data row5 col5">2.258</td>
</tr>
<tr class="odd">
<td id="T_ab78c_row6_col0" class="data row6 col0">🇵🇱</td>
<td id="T_ab78c_row6_col1" class="data row6 col1">pl</td>
<td id="T_ab78c_row6_col2" class="data row6 col2">Polish</td>
<td id="T_ab78c_row6_col3" class="data row6 col3">49354</td>
<td id="T_ab78c_row6_col4" class="data row6 col4">138230</td>
<td id="T_ab78c_row6_col5" class="data row6 col5">2.801</td>
</tr>
<tr class="even">
<td id="T_ab78c_row7_col0" class="data row7 col0">🇨🇳</td>
<td id="T_ab78c_row7_col1" class="data row7 col1">zh</td>
<td id="T_ab78c_row7_col2" class="data row7 col2">Chinese</td>
<td id="T_ab78c_row7_col3" class="data row7 col3">63073</td>
<td id="T_ab78c_row7_col4" class="data row7 col4">183660</td>
<td id="T_ab78c_row7_col5" class="data row7 col5">2.912</td>
</tr>
<tr class="odd">
<td id="T_ab78c_row8_col0" class="data row8 col0">🇷🇺</td>
<td id="T_ab78c_row8_col1" class="data row8 col1">ru</td>
<td id="T_ab78c_row8_col2" class="data row8 col2">Russian</td>
<td id="T_ab78c_row8_col3" class="data row8 col3">86594</td>
<td id="T_ab78c_row8_col4" class="data row8 col4">312824</td>
<td id="T_ab78c_row8_col5" class="data row8 col5">3.613</td>
</tr>
<tr class="even">
<td id="T_ab78c_row9_col0" class="data row9 col0">🇰🇷</td>
<td id="T_ab78c_row9_col1" class="data row9 col1">ko</td>
<td id="T_ab78c_row9_col2" class="data row9 col2">Korean</td>
<td id="T_ab78c_row9_col3" class="data row9 col3">55708</td>
<td id="T_ab78c_row9_col4" class="data row9 col4">243870</td>
<td id="T_ab78c_row9_col5" class="data row9 col5">4.378</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Here is the result for <code>o200k_base</code>, GPT-4o’s tokenizer:</p>
<div id="cell-46" class="cell" data-execution_count="70">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>n_articles <span class="op">=</span> <span class="dv">200</span>  <span class="co"># or 30, 50, 100, etc.</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> get_encoder(encoding_name<span class="op">=</span><span class="st">"o200k_base"</span>)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> analyze_multiple_languages(LANGUAGES, n_articles, encoder<span class="op">=</span>encoder)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>print_language_analysis_table_pandas(results, language_dict<span class="op">=</span>LANGUAGES)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<style type="text/css">
</style>

<table id="T_90e40" class="caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th id="T_90e40_level0_col0" class="col_heading level0 col0" data-quarto-table-cell-role="th">Flag</th>
<th id="T_90e40_level0_col1" class="col_heading level0 col1" data-quarto-table-cell-role="th">Code</th>
<th id="T_90e40_level0_col2" class="col_heading level0 col2" data-quarto-table-cell-role="th">Language</th>
<th id="T_90e40_level0_col3" class="col_heading level0 col3" data-quarto-table-cell-role="th">Words</th>
<th id="T_90e40_level0_col4" class="col_heading level0 col4" data-quarto-table-cell-role="th">Tokens</th>
<th id="T_90e40_level0_col5" class="col_heading level0 col5" data-quarto-table-cell-role="th">Tokens/Word</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td id="T_90e40_row0_col0" class="data row0 col0">🇺🇸</td>
<td id="T_90e40_row0_col1" class="data row0 col1">en</td>
<td id="T_90e40_row0_col2" class="data row0 col2">English</td>
<td id="T_90e40_row0_col3" class="data row0 col3">121339</td>
<td id="T_90e40_row0_col4" class="data row0 col4">168038</td>
<td id="T_90e40_row0_col5" class="data row0 col5">1.385</td>
</tr>
<tr class="even">
<td id="T_90e40_row1_col0" class="data row1 col0">🇪🇸</td>
<td id="T_90e40_row1_col1" class="data row1 col1">es</td>
<td id="T_90e40_row1_col2" class="data row1 col2">Spanish</td>
<td id="T_90e40_row1_col3" class="data row1 col3">108311</td>
<td id="T_90e40_row1_col4" class="data row1 col4">170586</td>
<td id="T_90e40_row1_col5" class="data row1 col5">1.575</td>
</tr>
<tr class="odd">
<td id="T_90e40_row2_col0" class="data row2 col0">🇵🇹</td>
<td id="T_90e40_row2_col1" class="data row2 col1">pt</td>
<td id="T_90e40_row2_col2" class="data row2 col2">Portuguese</td>
<td id="T_90e40_row2_col3" class="data row2 col3">78972</td>
<td id="T_90e40_row2_col4" class="data row2 col4">126866</td>
<td id="T_90e40_row2_col5" class="data row2 col5">1.606</td>
</tr>
<tr class="even">
<td id="T_90e40_row3_col0" class="data row3 col0">🇫🇷</td>
<td id="T_90e40_row3_col1" class="data row3 col1">fr</td>
<td id="T_90e40_row3_col2" class="data row3 col2">French</td>
<td id="T_90e40_row3_col3" class="data row3 col3">97867</td>
<td id="T_90e40_row3_col4" class="data row3 col4">157370</td>
<td id="T_90e40_row3_col5" class="data row3 col5">1.608</td>
</tr>
<tr class="odd">
<td id="T_90e40_row4_col0" class="data row4 col0">🇮🇹</td>
<td id="T_90e40_row4_col1" class="data row4 col1">it</td>
<td id="T_90e40_row4_col2" class="data row4 col2">Italian</td>
<td id="T_90e40_row4_col3" class="data row4 col3">92080</td>
<td id="T_90e40_row4_col4" class="data row4 col4">165649</td>
<td id="T_90e40_row4_col5" class="data row4 col5">1.799</td>
</tr>
<tr class="even">
<td id="T_90e40_row5_col0" class="data row5 col0">🇩🇪</td>
<td id="T_90e40_row5_col1" class="data row5 col1">de</td>
<td id="T_90e40_row5_col2" class="data row5 col2">German</td>
<td id="T_90e40_row5_col3" class="data row5 col3">107755</td>
<td id="T_90e40_row5_col4" class="data row5 col4">201239</td>
<td id="T_90e40_row5_col5" class="data row5 col5">1.868</td>
</tr>
<tr class="odd">
<td id="T_90e40_row6_col0" class="data row6 col0">🇨🇳</td>
<td id="T_90e40_row6_col1" class="data row6 col1">zh</td>
<td id="T_90e40_row6_col2" class="data row6 col2">Chinese</td>
<td id="T_90e40_row6_col3" class="data row6 col3">88272</td>
<td id="T_90e40_row6_col4" class="data row6 col4">176034</td>
<td id="T_90e40_row6_col5" class="data row6 col5">1.994</td>
</tr>
<tr class="even">
<td id="T_90e40_row7_col0" class="data row7 col0">🇷🇺</td>
<td id="T_90e40_row7_col1" class="data row7 col1">ru</td>
<td id="T_90e40_row7_col2" class="data row7 col2">Russian</td>
<td id="T_90e40_row7_col3" class="data row7 col3">104359</td>
<td id="T_90e40_row7_col4" class="data row7 col4">240536</td>
<td id="T_90e40_row7_col5" class="data row7 col5">2.305</td>
</tr>
<tr class="odd">
<td id="T_90e40_row8_col0" class="data row8 col0">🇵🇱</td>
<td id="T_90e40_row8_col1" class="data row8 col1">pl</td>
<td id="T_90e40_row8_col2" class="data row8 col2">Polish</td>
<td id="T_90e40_row8_col3" class="data row8 col3">51074</td>
<td id="T_90e40_row8_col4" class="data row8 col4">132056</td>
<td id="T_90e40_row8_col5" class="data row8 col5">2.586</td>
</tr>
<tr class="even">
<td id="T_90e40_row9_col0" class="data row9 col0">🇰🇷</td>
<td id="T_90e40_row9_col1" class="data row9 col1">ko</td>
<td id="T_90e40_row9_col2" class="data row9 col2">Korean</td>
<td id="T_90e40_row9_col3" class="data row9 col3">46488</td>
<td id="T_90e40_row9_col4" class="data row9 col4">140966</td>
<td id="T_90e40_row9_col5" class="data row9 col5">3.032</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>By analyzing random Wikipedia articles in many languages, we arrived at much more nuanced token-per-word ratios instead of just using the rough guesstimate of 1.3 tokens per word. We are seeing different results, even for English with a ratio of 1.4. For other languages based on the latin alphabet, we’re in a range of 1.6 to 1.9. We’re also seeing that the tokenizer matters. The new GPT-4o tokenizer, which has double the vocabulary size compared to the previous version, tokenizes Chinese more efficiently. The tokens per word ratio dropped from around 3.0 to 2.0.</p>
<p>In closing, we need to recognize that this analysis is limited to Wikipedia articles, which tend to have more complicated vocabulary compared to simpler texts. Additionally, Wikipedia articles contain a lot of markup like headings or tables. Since we removed non-word tokens when counting words, the tokens per word ratio increases because the LLM-tokenizer includes the complete text. If you would tokenize a novel, the ratio is most likely less due to having less markup.</p>
<p>Of course, there is still plenty of room for further practical analysis. For instance, it would be interesting to use different tokenizers or to tokenize various types of texts like novels, technical documentation, or conversation transcripts to analyze token-per-word ratios across different datasets. Nonetheless, I hope that this analysis helps you develop a more intuitive understanding of estimating token counts, and thereby estimating the costs involved in processing text with large language models.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/chrwittm\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="chrwittm/chrwittm.github.io.comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
© <span id="y"></span> Christian Wittmann
<script>
  document.getElementById('y').textContent = new Date().getFullYear();
</script>
</div>   
    <div class="nav-footer-center">

<div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
<p><a href="../../impressum.html">Impressum</a> · <a href="../../privacy.html">Privacy</a></p>
</div>
  </div>
</footer>




</body></html>