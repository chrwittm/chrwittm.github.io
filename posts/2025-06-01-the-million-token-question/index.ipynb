{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"The Million-Token Question: What the Bible Teaches Us About LLM Pricing\"\n",
    "author: \"Christian Wittmann\"\n",
    "date: \"2025-06-01\"\n",
    "categories: [tokenization, nlp]\n",
    "image: \"million-token-question.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my [previous blog post](https://chrwittm.github.io/posts/2025-05-23-estimating-tokens-per-word/), I explored how different languages compare in terms of tokens per word. That analysis was based on Wikipedia articles and raised a few follow-up questions. In this blog post, let's continue to explore tokenization to gain a more intuitive understanding and derive some real-world implications: \n",
    "\n",
    "1.\tDoes a higher tokens-per-word ratio actually cost more? Or do different languages naturally express the same content in fewer words, making the token ratio somewhat irrelevant when comparing meaning?\n",
    "2.\tHow much content is actually one million tokens? I named this the \"million-token question‚Äù because 1 million tokens are a typical unit to set a price point, yet it's surprisingly difficult to express 1 million tokens in real-world analogies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    text-align: center;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"million-token-question.png\" alt=\"The Million-Token Question\" style=\"width:50%;\">\n",
    "    <figcaption>The Million-Token Question</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The plan for this post\n",
    "\n",
    "With a bit of luck, I discovered that the content of the German Bible is almost the exact equivalent of one million tokens (using GPT-4o tokenizer `o200k_base`). Using this insight as our starting point, let‚Äôs tokenize several Bible translations. Here are the steps we‚Äôll follow:\n",
    "\n",
    "- Download the Bibles and clean-up the content so that it only contains the plain text.\n",
    "- Count words and tokens\n",
    "- Calculate some interesting KPIs\n",
    "\n",
    "As usual, this blog post is also available as a Jupyter Notebook, and you can run all the code yourself.\n",
    "\n",
    "## Reuse some code\n",
    "\n",
    "Let's start by defining some helper functions, which we will use later in the process. Please feel free to skip over this section if you are mainly interested in the results.\n",
    "\n",
    "The code assumes that you have previously installed spaCy alongside the necessary language packages. In case you haven‚Äôt, please check out my previous blog post on how to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def get_encoder(encoding_name=\"o200k_base\"):\n",
    "    \"\"\"Returns a tiktoken encoder. Defaults to GPT-4/GPT-4o's tokenizer.\"\"\"\n",
    "    return tiktoken.get_encoding(encoding_name)\n",
    "\n",
    "def count_tokens(text: str, encoder=None) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in the input text using the specified encoder.\n",
    "    If no encoder is provided, a new one will be created.\n",
    "    \"\"\"\n",
    "    if encoder is None:\n",
    "        encoder = get_encoder()\n",
    "    return len(encoder.encode(text))\n",
    "\n",
    "encoder = get_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "LANGUAGES = {\n",
    "    \"de\": {\"name\": \"German\",      \"model\": \"de_core_news_sm\",    \"emoji\": \"üá©üá™\"},\n",
    "    \"en\": {\"name\": \"English\",     \"model\": \"en_core_web_sm\",     \"emoji\": \"üá∫üá∏\"},\n",
    "    \"es\": {\"name\": \"Spanish\",     \"model\": \"es_core_news_sm\",    \"emoji\": \"üá™üá∏\"},\n",
    "    \"fr\": {\"name\": \"French\",      \"model\": \"fr_core_news_sm\",    \"emoji\": \"üá´üá∑\"},\n",
    "    \"it\": {\"name\": \"Italian\",     \"model\": \"it_core_news_sm\",    \"emoji\": \"üáÆüáπ\"},\n",
    "    \"ja\": {\"name\": \"Japanese\",    \"model\": \"ja_core_news_sm\",    \"emoji\": \"üáØüáµ\"},\n",
    "    \"ko\": {\"name\": \"Korean\",      \"model\": \"ko_core_news_sm\",    \"emoji\": \"üá∞üá∑\"},\n",
    "    \"pl\": {\"name\": \"Polish\",      \"model\": \"pl_core_news_sm\",    \"emoji\": \"üáµüá±\"},\n",
    "    \"pt\": {\"name\": \"Portuguese\",  \"model\": \"pt_core_news_sm\",    \"emoji\": \"üáµüáπ\"},\n",
    "    \"ru\": {\"name\": \"Russian\",     \"model\": \"ru_core_news_sm\",    \"emoji\": \"üá∑üá∫\"},\n",
    "    \"zh\": {\"name\": \"Chinese\",     \"model\": \"zh_core_web_sm\",     \"emoji\": \"üá®üá≥\"},\n",
    "}\n",
    "\n",
    "# Simple cache/dictionary to hold loaded spaCy models:\n",
    "_spacy_models = {}\n",
    "\n",
    "def get_spacy_model(language_code: str = \"en\"):\n",
    "    \"\"\"\n",
    "    Loads and caches the spaCy language model for the given language code.\n",
    "    Uses the model name defined in the LANGUAGES dict.\n",
    "    Falls back to a blank model if the specified model is not available.\n",
    "    \"\"\"\n",
    "    if language_code not in _spacy_models:\n",
    "        model_name = LANGUAGES.get(language_code, {}).get(\"model\", None)\n",
    "        try:\n",
    "            if model_name:\n",
    "                _spacy_models[language_code] = spacy.load(model_name)\n",
    "            else:\n",
    "                raise ValueError(f\"No model defined for language code: '{language_code}'\")\n",
    "        except (OSError, ValueError) as e:\n",
    "            print(f\"‚ö†Ô∏è Could not load model '{model_name}' for language '{language_code}': {e}\")\n",
    "            print(\"‚Üí Falling back to blank spaCy model (basic tokenization only).\")\n",
    "            _spacy_models[language_code] = spacy.blank(language_code)\n",
    "    return _spacy_models[language_code]\n",
    "\n",
    "def get_spacy_tokens(text: str, language_code: str = \"en\") -> tuple[list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Tokenizes the input text using spaCy's tokenizer.\n",
    "    Returns two lists: one with spaCy tokens (words) and one with omitted tokens \n",
    "    (punctuation, spaces, symbols, etc.).\n",
    "    \"\"\"\n",
    "    nlp = get_spacy_model(language_code)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    punctuation_set = set(string.punctuation)\n",
    "    \n",
    "    word_tokens = [\n",
    "        t for t in doc \n",
    "        if not t.is_space \n",
    "           and not t.is_punct \n",
    "           and t.pos_ != \"SYM\" \n",
    "           and t.text not in punctuation_set\n",
    "    ]\n",
    "    omitted_tokens = [\n",
    "        t for t in doc \n",
    "        if t.is_space \n",
    "           or t.is_punct \n",
    "           or t.pos_ == \"SYM\" \n",
    "           or t.text in punctuation_set\n",
    "    ]\n",
    "    \n",
    "    return word_tokens, omitted_tokens\n",
    "\n",
    "def count_words_spacy(text: str, language_code: str = \"en\") -> int:\n",
    "    \"\"\"\n",
    "    Counts words in the input text using spaCy's tokenizer.\n",
    "    Skips punctuation/whitespace tokens.\n",
    "    \"\"\"\n",
    "    nlp = get_spacy_model(language_code)\n",
    "    doc = nlp(text)\n",
    "    punctuation_set = set(string.punctuation)\n",
    "    \n",
    "    # Filter out space/punctuation tokens:\n",
    "    tokens = [\n",
    "        t for t in doc \n",
    "        if not t.is_space \n",
    "           and not t.is_punct \n",
    "           and t.pos_ != \"SYM\" \n",
    "           and t.text not in punctuation_set\n",
    "    ]\n",
    "    return len(tokens)\n",
    "\n",
    "def get_tokens_per_word(text: str, language_code: str = \"en\", encoder=None) -> float:\n",
    "    \"\"\"\n",
    "    Calculates average number of tokens (tiktoken) per word (spaCy-based) for the given text.\n",
    "    \"\"\"\n",
    "    words = count_words_spacy(text, language_code=language_code)\n",
    "    tokens = count_tokens(text, encoder=encoder)\n",
    "    \n",
    "    if words == 0:\n",
    "        return 0.0\n",
    "    return tokens / words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "def count_words_spacy_long(text, language_code=\"en\", chunk_size=1000000):\n",
    "    \"\"\"\n",
    "    Counts words in large text by splitting it into chunks and using count_words_spacy.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The full text to be analyzed.\n",
    "    language_code (str): Language code to pass to count_words_spacy.\n",
    "    chunk_size (int): Size of each text chunk in characters (default: 1,000,000).\n",
    "\n",
    "    Returns:\n",
    "    int: Total word count.\n",
    "    \"\"\"\n",
    "    total_word_count = 0\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        word_count = count_words_spacy(text=chunk, language_code=language_code)\n",
    "        total_word_count += word_count\n",
    "    return total_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads the full content of a plain text file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the text file.\n",
    "\n",
    "    Returns:\n",
    "    str: The content of the file as a single string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        return text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return \"\"\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"Error decoding file. Try using a different encoding, like 'latin-1'.\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def analyze_text(text: str, language_code: str = \"en\") -> dict:\n",
    "    \"\"\"\n",
    "    Analyzes the input text using spaCy and returns a dictionary with word count and token count.\n",
    "    \"\"\"\n",
    "    word_count = count_words_spacy_long(text, language_code=language_code)\n",
    "    token_count = count_tokens(text, encoder=encoder)\n",
    "    token_per_word = token_count / word_count\n",
    "    return {\"word_count\": word_count, \"token_count\": token_count, \"token_per_word\": token_per_word}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the bible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the full text of the bible in many languages was a bit tricky but [Bible Super Search](https://www.biblesupersearch.com/bible-downloads) provides full downloads in many languages. Bible texts usually contain verse numbers, which could affect the word count. Therefore, I opted for the CSV version, which contains several columns (Verse ID, Book Name, Book Number, Chapter, Verse, Text). This way I could extract only the raw text into separate text files.\n",
    "\n",
    "The site offered more than one version for some languages. With ChatGPT‚Äôs help, I selected the most mainstream translations:\n",
    "\n",
    "- Chinese: Chinese Union (Simplified)\n",
    "- English: American Standard Version\n",
    "- French: Louis Segond 1910\n",
    "- German: Luther Bible (1912)\n",
    "- Italian: Diodati\n",
    "- Korean: Korean\n",
    "- Polish: Uwsp√≥≈Çcze≈õniona Biblia Gda≈Ñska\n",
    "- Portuguese: Tradu√ß√£o de Jo√£o Ferreira de Almeida (Vers√£o Revista e Atualizada)\n",
    "- Russian: Synodal\n",
    "- Spanish: Reina Valera 1909"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zh_chinese_union_simp.csv',\n",
       " 'pt_almeida_ra.csv',\n",
       " 'ru_synodal.csv',\n",
       " 'pl_pol_ubg.csv',\n",
       " 'es_1909.csv',\n",
       " 'it_diodati.csv',\n",
       " 'fr_segond_1910.csv',\n",
       " 'de_luther_1912.csv',\n",
       " 'en_asv.csv',\n",
       " 'ko_korean.csv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "import os\n",
    "\n",
    "def get_filenames_by_extension(extension: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of filenames with the specified extension.\n",
    "    \"\"\"\n",
    "    return [f for f in os.listdir('.') if f.endswith(extension)]\n",
    "\n",
    "csv_filenames = get_filenames_by_extension('.csv')\n",
    "csv_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the download, I converted the files to plain text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file: zh_chinese_union_simp.txt\n",
      "Saved file: pt_almeida_ra.txt\n",
      "Saved file: ru_synodal.txt\n",
      "Saved file: pl_pol_ubg.txt\n",
      "Saved file: es_1909.txt\n",
      "Saved file: it_diodati.txt\n",
      "Saved file: fr_segond_1910.txt\n",
      "Saved file: de_luther_1912.txt\n",
      "Saved file: en_asv.txt\n",
      "Saved file: ko_korean.txt\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def csv_to_plain_text(input_csv: str, output_txt: str, text_column: str = \"Text\",\n",
    "                      encoding: str = \"utf-8\") -> None:\n",
    "    \"\"\"\n",
    "    Convert a Bible CSV into a plain text file with one verse per line,\n",
    "    skipping preamble lines before the actual header.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_csv)\n",
    "    output_path = Path(output_txt)\n",
    "\n",
    "    with input_path.open(mode=\"r\", encoding=encoding, newline='') as infile:\n",
    "        # Read all lines and search for the header row\n",
    "        lines = infile.readlines()\n",
    "        header_line_idx = None\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            # Try parsing this line as a CSV header\n",
    "            headers = [col.strip() for col in line.split(',')]\n",
    "            if text_column in headers:\n",
    "                header_line_idx = i\n",
    "                break\n",
    "\n",
    "        if header_line_idx is None:\n",
    "            raise ValueError(f\"Could not find a header line containing '{text_column}' in file {input_csv}\")\n",
    "\n",
    "        # Rewind file starting from the header line\n",
    "        valid_csv = lines[header_line_idx:]\n",
    "\n",
    "        reader = csv.DictReader(valid_csv)\n",
    "        if text_column not in reader.fieldnames:\n",
    "            raise KeyError(f\"Column '{text_column}' not found in CSV header: {reader.fieldnames}\")\n",
    "\n",
    "        with output_path.open(mode=\"w\", encoding=encoding, newline='\\n') as outfile:\n",
    "            for row in reader:\n",
    "                text = row[text_column].strip()\n",
    "                outfile.write(text + \"\\n\")   # one verse per line\n",
    "\n",
    "    print(f\"Saved file: {output_path.name}\")\n",
    "\n",
    "def get_text_file_name(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a filename, returns the same filename with a .txt extension.\n",
    "    Example: \"data.csv\" -> \"data.txt\"\n",
    "    \"\"\"\n",
    "    return str(Path(filename).with_suffix('.txt'))\n",
    "\n",
    "for csv_filename in csv_filenames:\n",
    "    csv_to_plain_text(csv_filename, get_text_file_name(csv_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Bible Texts\n",
    "\n",
    "We‚Äôve completed all the preparation steps and can now start analyzing the texts. Let's count both the tokens and the words to determine the tokens per word. Additionally, let‚Äôs normalize the tokens per word and the total number of tokens to English to not only see the tokens per word, but also the relative number of tokens per bible version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing it_diodati.txt...\n",
      "Done: {'word_count': 761788, 'token_count': 1275774, 'token_per_word': 1.6747100243112256}\n",
      "Processing fr_segond_1910.txt...\n",
      "Done: {'word_count': 777811, 'token_count': 1122594, 'token_per_word': 1.4432734944607366}\n",
      "Processing es_1909.txt...\n",
      "Done: {'word_count': 700895, 'token_count': 1027817, 'token_per_word': 1.4664350580329435}\n",
      "Processing de_luther_1912.txt...\n",
      "Done: {'word_count': 692385, 'token_count': 1049296, 'token_per_word': 1.5154805491164598}\n",
      "Processing en_asv.txt...\n",
      "Done: {'word_count': 789712, 'token_count': 997707, 'token_per_word': 1.2633808274408898}\n",
      "Processing ko_korean.txt...\n",
      "Done: {'word_count': 464422, 'token_count': 1240510, 'token_per_word': 2.6710836265293203}\n",
      "Processing pt_almeida_ra.txt...\n",
      "Done: {'word_count': 698762, 'token_count': 1042425, 'token_per_word': 1.4918169562740962}\n",
      "Processing zh_chinese_union_simp.txt...\n",
      "Done: {'word_count': 930597, 'token_count': 1520085, 'token_per_word': 1.6334514295661817}\n",
      "Processing ru_synodal.txt...\n",
      "Done: {'word_count': 563072, 'token_count': 1102920, 'token_per_word': 1.958754830643328}\n",
      "Processing pl_pol_ubg.txt...\n",
      "Done: {'word_count': 583927, 'token_count': 1252059, 'token_per_word': 2.144204669419294}\n"
     ]
    }
   ],
   "source": [
    "def analyze_all_text_files(extension: str = '.txt') -> list[dict]:\n",
    "    \"\"\"\n",
    "    Analyzes all text files with the given extension in the current directory.\n",
    "\n",
    "    Returns a list of dictionaries, each containing:\n",
    "    - language (derived from filename)\n",
    "    - filename\n",
    "    - word count\n",
    "    - token count\n",
    "    - tokens per word\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    txt_filenames = get_filenames_by_extension(extension)\n",
    "\n",
    "    for txt_filename in txt_filenames:\n",
    "        print(f\"Processing {txt_filename}...\")  # progress indicator\n",
    "\n",
    "        text = read_text_file(txt_filename)\n",
    "        language = txt_filename[:2].lower()\n",
    "        metrics = analyze_text(text, language_code=language)\n",
    "\n",
    "        print(f\"Done: {metrics}\")  # show results briefly\n",
    "\n",
    "        result = {\n",
    "            \"language\": language,\n",
    "            \"filename\": txt_filename,\n",
    "            \"word_count\": metrics[\"word_count\"],\n",
    "            \"token_count\": metrics[\"token_count\"],\n",
    "            \"tokens_per_word\": metrics[\"token_per_word\"]\n",
    "        }\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "results = analyze_all_text_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1eb45\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_1eb45_level0_col0\" class=\"col_heading level0 col0\" >Flag</th>\n",
       "      <th id=\"T_1eb45_level0_col1\" class=\"col_heading level0 col1\" >Code</th>\n",
       "      <th id=\"T_1eb45_level0_col2\" class=\"col_heading level0 col2\" >Language</th>\n",
       "      <th id=\"T_1eb45_level0_col3\" class=\"col_heading level0 col3\" >Words</th>\n",
       "      <th id=\"T_1eb45_level0_col4\" class=\"col_heading level0 col4\" >Tokens</th>\n",
       "      <th id=\"T_1eb45_level0_col5\" class=\"col_heading level0 col5\" >Tokens/Word</th>\n",
       "      <th id=\"T_1eb45_level0_col6\" class=\"col_heading level0 col6\" >Rel. Tokens/Word (vs EN)</th>\n",
       "      <th id=\"T_1eb45_level0_col7\" class=\"col_heading level0 col7\" >% of English Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_1eb45_row0_col0\" class=\"data row0 col0\" >üá∫üá∏</td>\n",
       "      <td id=\"T_1eb45_row0_col1\" class=\"data row0 col1\" >en</td>\n",
       "      <td id=\"T_1eb45_row0_col2\" class=\"data row0 col2\" >English</td>\n",
       "      <td id=\"T_1eb45_row0_col3\" class=\"data row0 col3\" >789712</td>\n",
       "      <td id=\"T_1eb45_row0_col4\" class=\"data row0 col4\" >997707</td>\n",
       "      <td id=\"T_1eb45_row0_col5\" class=\"data row0 col5\" >1.263</td>\n",
       "      <td id=\"T_1eb45_row0_col6\" class=\"data row0 col6\" >1.00</td>\n",
       "      <td id=\"T_1eb45_row0_col7\" class=\"data row0 col7\" >100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1eb45_row1_col0\" class=\"data row1 col0\" >üá´üá∑</td>\n",
       "      <td id=\"T_1eb45_row1_col1\" class=\"data row1 col1\" >fr</td>\n",
       "      <td id=\"T_1eb45_row1_col2\" class=\"data row1 col2\" >French</td>\n",
       "      <td id=\"T_1eb45_row1_col3\" class=\"data row1 col3\" >777811</td>\n",
       "      <td id=\"T_1eb45_row1_col4\" class=\"data row1 col4\" >1122594</td>\n",
       "      <td id=\"T_1eb45_row1_col5\" class=\"data row1 col5\" >1.443</td>\n",
       "      <td id=\"T_1eb45_row1_col6\" class=\"data row1 col6\" >1.14</td>\n",
       "      <td id=\"T_1eb45_row1_col7\" class=\"data row1 col7\" >112.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1eb45_row2_col0\" class=\"data row2 col0\" >üá™üá∏</td>\n",
       "      <td id=\"T_1eb45_row2_col1\" class=\"data row2 col1\" >es</td>\n",
       "      <td id=\"T_1eb45_row2_col2\" class=\"data row2 col2\" >Spanish</td>\n",
       "      <td id=\"T_1eb45_row2_col3\" class=\"data row2 col3\" >700895</td>\n",
       "      <td id=\"T_1eb45_row2_col4\" class=\"data row2 col4\" >1027817</td>\n",
       "      <td id=\"T_1eb45_row2_col5\" class=\"data row2 col5\" >1.466</td>\n",
       "      <td id=\"T_1eb45_row2_col6\" class=\"data row2 col6\" >1.16</td>\n",
       "      <td id=\"T_1eb45_row2_col7\" class=\"data row2 col7\" >103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1eb45_row3_col0\" class=\"data row3 col0\" >üáµüáπ</td>\n",
       "      <td id=\"T_1eb45_row3_col1\" class=\"data row3 col1\" >pt</td>\n",
       "      <td id=\"T_1eb45_row3_col2\" class=\"data row3 col2\" >Portuguese</td>\n",
       "      <td id=\"T_1eb45_row3_col3\" class=\"data row3 col3\" >698762</td>\n",
       "      <td id=\"T_1eb45_row3_col4\" class=\"data row3 col4\" >1042425</td>\n",
       "      <td id=\"T_1eb45_row3_col5\" class=\"data row3 col5\" >1.492</td>\n",
       "      <td id=\"T_1eb45_row3_col6\" class=\"data row3 col6\" >1.18</td>\n",
       "      <td id=\"T_1eb45_row3_col7\" class=\"data row3 col7\" >104.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1eb45_row4_col0\" class=\"data row4 col0\" >üá©üá™</td>\n",
       "      <td id=\"T_1eb45_row4_col1\" class=\"data row4 col1\" >de</td>\n",
       "      <td id=\"T_1eb45_row4_col2\" class=\"data row4 col2\" >German</td>\n",
       "      <td id=\"T_1eb45_row4_col3\" class=\"data row4 col3\" >692385</td>\n",
       "      <td id=\"T_1eb45_row4_col4\" class=\"data row4 col4\" >1049296</td>\n",
       "      <td id=\"T_1eb45_row4_col5\" class=\"data row4 col5\" >1.515</td>\n",
       "      <td id=\"T_1eb45_row4_col6\" class=\"data row4 col6\" >1.20</td>\n",
       "      <td id=\"T_1eb45_row4_col7\" class=\"data row4 col7\" >105.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1eb45_row5_col0\" class=\"data row5 col0\" >üá®üá≥</td>\n",
       "      <td id=\"T_1eb45_row5_col1\" class=\"data row5 col1\" >zh</td>\n",
       "      <td id=\"T_1eb45_row5_col2\" class=\"data row5 col2\" >Chinese</td>\n",
       "      <td id=\"T_1eb45_row5_col3\" class=\"data row5 col3\" >930597</td>\n",
       "      <td id=\"T_1eb45_row5_col4\" class=\"data row5 col4\" >1520085</td>\n",
       "      <td id=\"T_1eb45_row5_col5\" class=\"data row5 col5\" >1.633</td>\n",
       "      <td id=\"T_1eb45_row5_col6\" class=\"data row5 col6\" >1.29</td>\n",
       "      <td id=\"T_1eb45_row5_col7\" class=\"data row5 col7\" >152.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1eb45_row6_col0\" class=\"data row6 col0\" >üáÆüáπ</td>\n",
       "      <td id=\"T_1eb45_row6_col1\" class=\"data row6 col1\" >it</td>\n",
       "      <td id=\"T_1eb45_row6_col2\" class=\"data row6 col2\" >Italian</td>\n",
       "      <td id=\"T_1eb45_row6_col3\" class=\"data row6 col3\" >761788</td>\n",
       "      <td id=\"T_1eb45_row6_col4\" class=\"data row6 col4\" >1275774</td>\n",
       "      <td id=\"T_1eb45_row6_col5\" class=\"data row6 col5\" >1.675</td>\n",
       "      <td id=\"T_1eb45_row6_col6\" class=\"data row6 col6\" >1.33</td>\n",
       "      <td id=\"T_1eb45_row6_col7\" class=\"data row6 col7\" >127.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1eb45_row7_col0\" class=\"data row7 col0\" >üá∑üá∫</td>\n",
       "      <td id=\"T_1eb45_row7_col1\" class=\"data row7 col1\" >ru</td>\n",
       "      <td id=\"T_1eb45_row7_col2\" class=\"data row7 col2\" >Russian</td>\n",
       "      <td id=\"T_1eb45_row7_col3\" class=\"data row7 col3\" >563072</td>\n",
       "      <td id=\"T_1eb45_row7_col4\" class=\"data row7 col4\" >1102920</td>\n",
       "      <td id=\"T_1eb45_row7_col5\" class=\"data row7 col5\" >1.959</td>\n",
       "      <td id=\"T_1eb45_row7_col6\" class=\"data row7 col6\" >1.55</td>\n",
       "      <td id=\"T_1eb45_row7_col7\" class=\"data row7 col7\" >110.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1eb45_row8_col0\" class=\"data row8 col0\" >üáµüá±</td>\n",
       "      <td id=\"T_1eb45_row8_col1\" class=\"data row8 col1\" >pl</td>\n",
       "      <td id=\"T_1eb45_row8_col2\" class=\"data row8 col2\" >Polish</td>\n",
       "      <td id=\"T_1eb45_row8_col3\" class=\"data row8 col3\" >583927</td>\n",
       "      <td id=\"T_1eb45_row8_col4\" class=\"data row8 col4\" >1252059</td>\n",
       "      <td id=\"T_1eb45_row8_col5\" class=\"data row8 col5\" >2.144</td>\n",
       "      <td id=\"T_1eb45_row8_col6\" class=\"data row8 col6\" >1.70</td>\n",
       "      <td id=\"T_1eb45_row8_col7\" class=\"data row8 col7\" >125.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1eb45_row9_col0\" class=\"data row9 col0\" >üá∞üá∑</td>\n",
       "      <td id=\"T_1eb45_row9_col1\" class=\"data row9 col1\" >ko</td>\n",
       "      <td id=\"T_1eb45_row9_col2\" class=\"data row9 col2\" >Korean</td>\n",
       "      <td id=\"T_1eb45_row9_col3\" class=\"data row9 col3\" >464422</td>\n",
       "      <td id=\"T_1eb45_row9_col4\" class=\"data row9 col4\" >1240510</td>\n",
       "      <td id=\"T_1eb45_row9_col5\" class=\"data row9 col5\" >2.671</td>\n",
       "      <td id=\"T_1eb45_row9_col6\" class=\"data row9 col6\" >2.11</td>\n",
       "      <td id=\"T_1eb45_row9_col7\" class=\"data row9 col7\" >124.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x35a85ebc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_tokenization_dataframe(results: list[dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts tokenization results into a pandas DataFrame with:\n",
    "    - Flag\n",
    "    - ISO code\n",
    "    - Language name\n",
    "    - Word count\n",
    "    - Token count\n",
    "    - Tokens per word\n",
    "    - Tokens/Word relative to English\n",
    "    - Total tokens as % of English tokens\n",
    "\n",
    "    Sorted ascending by Tokens/Word.\n",
    "    \"\"\"\n",
    "    # Use metadata from the shared LANGUAGES dictionary\n",
    "    def get_lang_info(code):\n",
    "        entry = LANGUAGES.get(code, {})\n",
    "        return entry.get(\"emoji\", \"üè≥Ô∏è\"), entry.get(\"name\", \"Unknown\")\n",
    "\n",
    "    # Get English baseline values\n",
    "    english_entry = next((entry for entry in results if entry[\"language\"] == \"en\"), None)\n",
    "    if not english_entry:\n",
    "        raise ValueError(\"English ('en') entry not found in results.\")\n",
    "\n",
    "    english_tokens = english_entry[\"token_count\"]\n",
    "    english_tpw = english_entry[\"tokens_per_word\"]\n",
    "\n",
    "    rows = []\n",
    "    for entry in results:\n",
    "        lang_code = entry[\"language\"]\n",
    "        flag, language = get_lang_info(lang_code)\n",
    "\n",
    "        tokens = entry[\"token_count\"]\n",
    "        tpw = entry[\"tokens_per_word\"]\n",
    "\n",
    "        rel_tpw = tpw / english_tpw\n",
    "        percent_of_english = (tokens / english_tokens) * 100\n",
    "\n",
    "        rows.append({\n",
    "            \"Flag\": flag,\n",
    "            \"Code\": lang_code,\n",
    "            \"Language\": language,\n",
    "            \"Words\": entry[\"word_count\"],\n",
    "            \"Tokens\": tokens,\n",
    "            \"Tokens/Word\": round(tpw, 3),\n",
    "            \"Rel. Tokens/Word (vs EN)\": round(rel_tpw, 2),\n",
    "            \"% of English Tokens\": round(percent_of_english, 1),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.sort_values(by=\"Tokens/Word\", ascending=True).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def display_tokenization_table(df: pd.DataFrame) -> None:\n",
    "    styled = df.style.format({\n",
    "        \"Tokens/Word\": \"{:.3f}\",\n",
    "        \"Rel. Tokens/Word (vs EN)\": \"{:.2f}\",\n",
    "        \"% of English Tokens\": \"{:.1f}\"\n",
    "    }).hide(axis=\"index\")\n",
    "    display(styled)\n",
    "\n",
    "df = get_tokenization_dataframe(results)\n",
    "display_tokenization_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: What the Bible Teaches Us About Tokenization\n",
    "\n",
    "The results turned out to be even more interesting than I expected. We can observe that across the board, the token-per-word ratio for the bible is less than in my previous experiment with Wikipedia articles. I expected this result because the bible contains a lot less markup compared to wikipedia articles. While interesting, other findings stand out more significantly from my point of view.\n",
    "\n",
    "First, we can now confidently say that the Bible answers the million-token question. For English, Spanish, Portuguese, and German, the total token count falls within just 5% of one million tokens. French and Russian also land close, within a 10% margin. Extending this range to about 25%, we can also include Korean, Polish, and Italian. Chinese is an outlier, but you might still think of it as a rough estimate. So next time you read the pricing of LLM tokens in dollars per million tokens, for example, $2.00 per 1M input tokens and $8.00 per 1M output tokens, you can imagine it costs $2.00 to read the bible and $8.00 to write the bible.\n",
    "\n",
    "Here's what actually surprised me: Although the tokens-per-word ratios vary substantially across languages ‚Äî with Polish and Korean being particularly token-hungry, the total token counts across most languages are a lot closer. Once we normalize token counts relative to English, the variation shrinks, and a pattern emerges: most languages convey the same biblical content using roughly the same number of tokens. This insight challenges the assumption that a higher token-per-word ratio necessarily means higher cost or verbosity. In fact, while languages differ in how many words they need to express an idea, those differences appear to balance out when viewed through the lens of token usage ‚Äî except, again, in the case of Chinese.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
