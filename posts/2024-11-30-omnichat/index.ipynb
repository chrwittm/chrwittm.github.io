{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"OmniChat - How to chat with any LLM\"\n",
    "author: \"Christian Wittmann\"\n",
    "date: \"2024-11-30\"\n",
    "categories: [chat, llm, python, injection, openai, llama, anthropic, x.ai, grok, groq]\n",
    "image: \"omnichat.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "source": [
    "So far, I’ve worked with large language models (LLMs) from OpenAI and Meta (the LLaMA series) in separate implementations. I noticed that the code across these implementations was very similar, with only minor variations for provider-specific details. This realization inspired me to harmonize the codebase into a unified, modular chat client with the current working title \"OmniChatClient\". The name \"Omni\" reflects two key aspects of its design: It seamlessly integrates with several LLM providers, and it supports multimodal inputs (text and vision so far).\n",
    "\n",
    "In this blog post, I’ll walk you through the steps I took to create the OmniChatClient. The current version works with OpenAI, the LLaMA series (via Groq), Anthropic’s Claude, and X.AI's Grok, using both the SDKs from OpenAI and Anthropic. By the end, you’ll see how OmniChatClient’s code structure enables flexibility, maintainability, and scalability. I’ll demonstrate how I implemented support for both text and vision inputs. I will save additional features like function calling or adding additional vendor support for later posts.\n",
    "\n",
    "This blog post focuses on the refactoring process that transformed my earlier implementations into OmniChatClient. If you’re interested in the inner mechanics of the chat client and the chat messages, please refer to my previous blog posts like [Building Chat for Jupyter Notebooks from Scratch](https://chrwittm.github.io/posts/2024-02-23-chat-from-scratch/) or [Building the Apple Calculator in a Jupyter Notebook](https://chrwittm.github.io/posts/2024-08-02-llm-calculator2-vision/).\n",
    "\n",
    "A final note before we start: As usual, if you prefer the interactive notebook version of this blog post, please use the [this version on GitHub](https://github.com/chrwittm/lm-hackers/blob/main/60-omni-chat/index.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Before diving into the code, let’s explore the challenge and the approach I took to solve it. At first glance, the APIs of various LLM providers seem very similar: You specify a model name, send `user`-messages, and receive `assistant`-messages, all wrapped in a JSON schema. However, subtle differences emerge on closer inspection. For example:\n",
    "\n",
    "- OpenAI expects the system prompt to be included in the chat messages, whereas Anthropic requires it to be passed as a separate parameter.\n",
    "- The maximum token limit (`max_tokens`) is mandatory in Anthropic’s API, but optional for OpenAI and other providers I’ve worked with so far.\n",
    "\n",
    "These differences become even more pronounced when going beyond simple text-based chat. For instance:\n",
    "\n",
    "- OpenAI and Anthropic use different formats for passing images into the chat.\n",
    "- Llama 3.2 models only support one image at a time, while OpenAI and Anthropic can handle multiple images.\n",
    "\n",
    "Despite these variations, the overarching process flow remains consistent when viewed at a higher level.\n",
    "\n",
    "To address these challenges, I decided to decouple provider-specific details from the generic chat client logic. This is achieved through \"dependency injection\" (for a nice video intro I can recommend [Dependency Injection, The Best Pattern](https://www.youtube.com/watch?v=J1f5b4vcxCQ) from [CodeAesthetic](https://www.youtube.com/@CodeAesthetic)), where an LLM-specific provider class is injected into the chat client. For message management, I chose an inheritance structure, with a base `ChatMessages` class handling common functionality and provider-specific subclasses extending it to support provider-specific message formatting requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Injecting Provider-Specific Class into Chat Client\n",
    "\n",
    "The `ChatClient` is a provider-agnostic class responsible for managing the conversation flow. It delegates provider-specific functionality, such as authentication and API interactions, to a class implementing the `LLMProviderInterface`. This interface defines the contract for provider-specific behavior, ensuring consistent integration regardless of the provider.\n",
    "\n",
    "To maximize reusability, shared logic is abstracted into the `BaseLLMProvider`, which implements `LLMProviderInterface`. Provider-specific classes, such as `OpenAIProvider` and `AnthropicProvider`, extend `BaseLLMProvider` to handle unique API requirements.\n",
    "\n",
    "Upon instantiating the `ChatClient`, you inject the desired provider class, enabling the client to remain decoupled from provider-specific details while leveraging the functionality defined by the provider.\n",
    "\n",
    "The diagram below illustrates the architecture I implemented. For readability, it contains only 2 providers: OpenAI and Anthropic. (You can find a more detailed version later in this post.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "flowchart BT\n",
    "    \n",
    "    LLMProviderInterface -->|Injects| ChatClient \n",
    "\n",
    "    subgraph Injection[\"Dependency Injection\"]\n",
    "        direction BT\n",
    "    \n",
    "        BaseLLMProvider -->|Implements| LLMProviderInterface\n",
    "\n",
    "        subgraph Inheritance[\"Inheritance Hierarchy\"]\n",
    "            direction BT\n",
    "            OpenAIProvider -->|Extends| BaseLLMProvider\n",
    "            AnthropicProvider -->|Extends| BaseLLMProvider\n",
    "            style Inheritance fill:#D0E6F5\n",
    "        end\n",
    "\n",
    "    end   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatMessages with Provider-Specific Classes\n",
    "\n",
    "The `LLMProviderInterface` includes a method called `chat_messages_factory`, which is responsible for creating an instance of the appropriate `ChatMessages` class. These classes follow an inheritance hierarchy to handle both common functionality and provider-specific message formatting.\n",
    "\n",
    "The base `ChatMessages` class provides shared logic for managing conversation history and appending messages in a generic format. Provider-specific subclasses, such as `OpenAIChatMessages` and `AnthropicChatMessages`, extend this base class to handle unique requirements, such as formatting multimodal inputs (e.g., images).\n",
    "\n",
    "This design allows the `ChatClient` to remain agnostic of provider-specific message handling, while the injected provider determines which `ChatMessages` subclass to use.\n",
    "\n",
    "The diagram below illustrates the inheritance hierarchy of `ChatMessages` (again, it is limited to OpenAI and Anthropic for readability):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "flowchart BT\n",
    "    OpenChatMessages -->|Extends| ChatMessages\n",
    "    AnthropicChatMessages -->|Extends| ChatMessages\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full architecture\n",
    "\n",
    "Now that we’ve discussed the individual components, let’s put everything together to see the big picture. The diagram below illustrates how the key classes interact to create a flexible, multi-provider chat system.\n",
    "\n",
    "If this looks overwhelming at first, don’t worry! We’ll construct this step-by-step throughout the blog post. You can always revisit this diagram later once you’ve gone through the detailed explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```{mermaid}\n",
    "classDiagram\n",
    "    %% ChatClient and Provider Interface\n",
    "    class ChatClient {\n",
    "        Responsible for managing the conversation with LLM\n",
    "        - _provider: LLMProviderInterface\n",
    "        - _chat_messages: ChatMessages\n",
    "        +__init__(provider: LLMProviderInterface)\n",
    "        +prompt_model(prompt: str, base64_images: Optional[List[str]])\n",
    "    }\n",
    "\n",
    "    class LLMProviderInterface {\n",
    "        Defines provider-specific implementation points\n",
    "        +__init__(model_name: str, **kwargs)\n",
    "        +chat_messages_factory() ChatMessages\n",
    "        +_get_model_response(chat_messages: ChatMessages)\n",
    "    }\n",
    "\n",
    "    class ChatMessages {\n",
    "        Manages the conversation history and message structure\n",
    "        - _messages: List[Dict]\n",
    "        +append_user_message(content: str, base64_images: Optional[List[str]])\n",
    "        +append_assistant_message(content: str)\n",
    "    }\n",
    "\n",
    "    %% Relationships\n",
    "    ChatClient ..> LLMProviderInterface : Dependency Injection\n",
    "    LLMProviderInterface o-- ChatMessages : Factory Creates\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the text-based OmniChatClient\n",
    "\n",
    "Let’s start simple by implementing a text-only version of the OmniChatClient. This will lay the foundation for adding more advanced functionality like vision support in later iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatMessages\n",
    "\n",
    "First, we need a class to manage the conversation history and structure messages. Below is a simplified version of the ChatMessages class, similar to what I introduced in [Building Chat for Jupyter Notebooks from Scratch](https://chrwittm.github.io/posts/2024-02-23-chat-from-scratch/). Provider-specific classes will extend this base class only when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "class ChatMessages:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the Chat.\"\"\"\n",
    "        self._messages = []\n",
    "\n",
    "    def _append_message(self, role, content):\n",
    "        \"\"\"Appends a message with specified role and content to messages list.\"\"\"\n",
    "        self._messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def append_system_message(self, content):\n",
    "        \"\"\"Appends a system message with specified content to messages list.\"\"\"\n",
    "        if content:\n",
    "            # No empty system message\n",
    "            self._append_message(\"system\", content)\n",
    "\n",
    "    def append_user_message(self, content=None, base64_images=None):\n",
    "        \"\"\"Appends a user message with specified content\"\"\"\n",
    "        self._append_message(\"user\", content)\n",
    "\n",
    "    def append_assistant_message(self, content):\n",
    "        \"\"\"Appends an assistant message with specified content.\"\"\"\n",
    "        self._append_message(\"assistant\", content)\n",
    "\n",
    "    def get_messages(self):\n",
    "        \"\"\"Returns a shallow copy of the messages list.\"\"\"\n",
    "        return self._messages[:]\n",
    "    \n",
    "    def get_debug_view(self):\n",
    "        \"\"\"Returns the debug view of the chat messages formatted as Markdown.\"\"\"\n",
    "        debug_view = []\n",
    "        for message in self._messages:\n",
    "            role = message.get('role')\n",
    "            content = message.get('content', '')\n",
    "            debug_view.append(f\"**{role}**: {content}\\n\")\n",
    "\n",
    "        return Markdown('\\n'.join(debug_view))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMProviderInterface\n",
    "\n",
    "Next, we define the LLMProviderInterface, which serves as a contract for all provider-specific classes. These classes will handle authentication, API interactions, and any provider-specific logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class LLMProviderInterface:\n",
    "    \n",
    "    def initialize_client(self) -> None:\n",
    "        \"\"\"\n",
    "        Abstract method for initializing the model prover client including authentication.\n",
    "        This method is called from the provider's constructor.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def initialize_system_prompt(self, system_prompt):\n",
    "        \"\"\"\n",
    "        Abstract method for initializing the system prompt.\n",
    "\n",
    "        :param system_prompt: The system prompt for the ChatClient.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_model_response(self, chat_messages: ChatMessages):\n",
    "        \"\"\"\n",
    "        Abstract method for fetching the model response.\n",
    "\n",
    "        :param chat_messages: ChatMessages object containing the conversation history.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def chat_messages_factory(self) -> ChatMessages:\n",
    "        \"\"\"Abstract factory method to create the appropriate ChatMessages class.\"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseLLMProvider\n",
    "\n",
    "The `BaseLLMProvider` implements shared functionality like client initialization and serves as the foundation for provider-specific classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLLMProvider(LLMProviderInterface):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        system_prompt: Optional[str] = None,\n",
    "        max_tokens: Optional[int] = None,\n",
    "        temperature: Optional[float] = 0.0,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self._max_tokens = max_tokens\n",
    "        self._temperature = temperature\n",
    "        self.initialize_system_prompt(system_prompt)\n",
    "        self.initialize_client()\n",
    "\n",
    "    def initialize_system_prompt(self, system_prompt = None):\n",
    "        self._system_prompt = system_prompt\n",
    " \n",
    "    def initialize_client(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_model_response(self, chat_messages, tools=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def chat_messages_factory(self):\n",
    "        return ChatMessages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatClient\n",
    "\n",
    "Finally, the `ChatClient` class orchestrates the conversation flow. Notice how provider-specific logic is delegated to the injected provider, ensuring the client remains provider-agnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "class ChatClient:\n",
    "\n",
    "    def __init__(self, provider: LLMProviderInterface):\n",
    "        \"\"\"Initializes the Chat with the system message.\"\"\"\n",
    "        self._provider = provider\n",
    "        self._chat_messages = provider.chat_messages_factory()\n",
    "\n",
    "    def _get_model_response(self):\n",
    "        \"\"\"Delegates response fetching to the LLM-provider class.\"\"\"\n",
    "        return self._provider.get_model_response(\n",
    "            chat_messages=self._chat_messages,\n",
    "        )\n",
    "    \n",
    "    def prompt_model(self, prompt=None, base64_images=None):\n",
    "        \"\"\"This method handles the user prompt, delegates interaction with the model through the provider, \n",
    "        and returns the model response.\"\"\"\n",
    "        self._chat_messages.append_user_message(prompt)\n",
    "        content = self._get_model_response()\n",
    "        self._chat_messages.append_assistant_message(content)\n",
    "        return Markdown(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this foundation, we can now implement provider-specific classes (e.g., for OpenAI and Anthropic). These classes will inherit from BaseLLMProvider and implement methods like `initialize_client` and `get_model_response` to handle provider-specific details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provider Implementations\n",
    "\n",
    "### OpenAI Implementation\n",
    "\n",
    "The [OpenAI](https://platform.openai.com/docs/api-reference/introduction) provider is implemented using the [OpenAI SDK](https://github.com/openai/openai-python). This provider class handles client initialization, constructs the necessary parameters for API calls, and manages the chat message structure.\n",
    "\n",
    "Noteworthy points:\n",
    "\n",
    "- The system prompt is included directly in the chat messages. This behavior is specific to OpenAI and differs from, for example, Anthropic’s implementation.\n",
    "- Optional parameters like `max_tokens` and `temperature` are conditionally added to the API request, ensuring flexibility without hardcoding defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "class OpenAIProvider(BaseLLMProvider):\n",
    "\n",
    "    def initialize_client(self):\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set in the environment variables.\")\n",
    "        \n",
    "        self.client = OpenAI()\n",
    "\n",
    "    def get_model_response(self, chat_messages):\n",
    "        try:\n",
    "            \n",
    "            # Mandatory parameters\n",
    "            params = {\n",
    "                \"model\": self.model_name,\n",
    "                \"messages\": chat_messages.get_messages()\n",
    "            }\n",
    "\n",
    "            # Optional parameters\n",
    "            if self._max_tokens is not None:\n",
    "                params[\"max_tokens\"] = self._max_tokens\n",
    "            if self._temperature is not None:\n",
    "                params[\"temperature\"] = self._temperature\n",
    "                \n",
    "            response = self.client.chat.completions.create(**params)\n",
    "\n",
    "            try:\n",
    "                content = response.choices[0].message.content\n",
    "            except (AttributeError, IndexError) as e:\n",
    "                raise RuntimeError(f\"Malformed response structure: {str(e)}. Response: {response}\")\n",
    "            return content\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to fetch model response. Params: {params}. Error: {str(e)}\")\n",
    "    \n",
    "    def chat_messages_factory(self):\n",
    "        \"\"\"Returns the standard ChatMessages implementation for OpenAI.\"\"\"\n",
    "        chat_messages = ChatMessages()\n",
    "        chat_messages.append_system_message(self._system_prompt)\n",
    "        return chat_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s test the OpenAI provider with a simple prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello, I'm an AI language model created by OpenAI, here to assist you with information and answer your questions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt-4o\"\n",
    "system_prompt = \"Answer in a very concise and accurate way\"\n",
    "provider = OpenAIProvider(model_name=model_name, system_prompt=system_prompt)\n",
    "\n",
    "chat_client = ChatClient(provider=provider)\n",
    "chat_client.prompt_model(\"Hello, who are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic implementation\n",
    "\n",
    "The [Anthropic](https://docs.anthropic.com/en/api/getting-started) provider is implemented using the [Anthropic SDK](https://github.com/anthropics/anthropic-sdk-python). This class manages client initialization, validates required parameters, and adjusts the message structure for Anthropic's API.\n",
    "\n",
    "Noteworthy points:\n",
    "\n",
    "- `max_tokens` is a required field for calling Anthropic’s Claude model. This is validated during provider initialization to avoid runtime errors.\n",
    "- Unlike the OpenAI implementation, the system prompt is passed directly to the `client` as a separate parameter, instead of embedding it in the chat messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "class AnthropicProvider(BaseLLMProvider):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        max_tokens = kwargs.get(\"max_tokens\")\n",
    "        if max_tokens is None:\n",
    "            raise ValueError(\"AnthropicProvider requires 'max_tokens' to be specified.\")\n",
    "        super().__init__(*args, **kwargs)  # Pass all arguments to the base constructor\n",
    "\n",
    "    def initialize_system_prompt(self, system_prompt = None):\n",
    "        super().initialize_system_prompt(system_prompt)\n",
    "        if self._system_prompt == None:\n",
    "            self._system_prompt = \"\"\n",
    "\n",
    "    def initialize_client(self):\n",
    "        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"ANTHROPIC_API_KEY is not set in the environment variables.\")\n",
    "\n",
    "        self.client = Anthropic()\n",
    "\n",
    "    def get_model_response(self, chat_messages):\n",
    "        try:\n",
    "            \n",
    "            # Mandatory parameters\n",
    "            params = {\n",
    "                \"model\": self.model_name,\n",
    "                \"system\": self._system_prompt,\n",
    "                \"messages\": chat_messages.get_messages(),\n",
    "                \"max_tokens\": self._max_tokens\n",
    "            }\n",
    "\n",
    "            # Optional parameters\n",
    "            if self._temperature is not None:\n",
    "                params[\"temperature\"] = self._temperature\n",
    "\n",
    "            response = self.client.messages.create(**params)\n",
    "\n",
    "            try:\n",
    "                content = \"\"\n",
    "                for content_line in response.content:\n",
    "                    if content_line.type == \"text\":\n",
    "                        content = content_line.text  \n",
    "                return content\n",
    "            except (AttributeError, IndexError) as e:\n",
    "                raise RuntimeError(f\"Malformed response structure: {str(e)}. Response: {response}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to fetch model response. Params: {params}. Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let’s run a simple test. Notice how the code remains compact and modular: Only the model name and provider instantiation change. The OmniChatClient abstracts away the different APIs and message structures through the injected provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm Claude, an AI assistant created by Anthropic. I aim to be direct and truthful in our conversations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"claude-3-5-sonnet-20241022\"\n",
    "provider = AnthropicProvider(model_name=model_name, system_prompt=system_prompt, max_tokens=128)\n",
    "\n",
    "chat_client = ChatClient(provider=provider)\n",
    "chat_client.prompt_model(\"Hello, who are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X.ai Implementation\n",
    "\n",
    "This is where the effort we’ve invested in building a modular architecture starts to pay off. With the implementation for [X.ai](https://docs.x.ai/docs), we can reuse much of the code we’ve already written for OpenAI and Anthropic, saving time and effort.\n",
    "\n",
    "To interact with Grok, X.ai allows you to use [either the OpenAI or Anthropic SDKs](https://docs.x.ai/api/integrations). Since we’ve already implemented both, all we need to do is update the endpoint and authorization details.\n",
    "\n",
    "#### Using the OpenAI SDK for X.ai\n",
    "\n",
    "Here’s all the code needed to enable Grok support via the OpenAI SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "class XAIProviderOpenAI(OpenAIProvider):\n",
    "    \n",
    "    def initialize_client(self):\n",
    "        api_key = os.getenv(\"XAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"XAI_API_KEY is not set in the environment variables.\")\n",
    "\n",
    "        base_url = os.getenv(\"XAI_BASE_URL\", \"https://api.x.ai/v1\")\n",
    "\n",
    "        self.client = OpenAI(\n",
    "            api_key=api_key,    \n",
    "            base_url=base_url,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a quick test. The previous prompt returned a pretty shy answer, so I updated it to a more grok-like version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I am Grok, an AI developed by xAI, here to provide helpful and truthful answers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"grok-beta\"\n",
    "provider = XAIProviderOpenAI(model_name=model_name, system_prompt=system_prompt)\n",
    "\n",
    "chat_client = ChatClient(provider=provider)\n",
    "chat_client.prompt_model(\"State your name and identity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Anthropic SDK for X.ai\n",
    "\n",
    "Similarly, we can use the Anthropic SDK to interact with Grok. Here’s the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "class XAIProviderAnthropic(AnthropicProvider):\n",
    "\n",
    "    def initialize_client(self):\n",
    "        api_key = os.getenv(\"XAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"XAI_API_KEY is not set in the environment variables.\")\n",
    "\n",
    "        base_url = \"https://api.x.ai\" \n",
    "\n",
    "        self.client = Anthropic(\n",
    "            api_key=api_key,    \n",
    "            base_url=base_url,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I am Grok, an AI developed by xAI, here to provide helpful and truthful answers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"grok-beta\"\n",
    "provider = XAIProviderAnthropic(model_name=model_name, system_prompt=system_prompt, max_tokens=128)\n",
    "\n",
    "chat_client = ChatClient(provider=provider)\n",
    "chat_client.prompt_model(\"State your name and identity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama Implementation\n",
    "\n",
    "To access Llama 3.2, I chose [Groq](https://groq.com/) as the provider. Groq offers a free tier and provides access to Llama 3.2 with vision capabilities, a practical workaround given that downloading Llama 3 in the EU is restricted under its licensing terms.\n",
    "\n",
    "Here’s the implementation for the Llama 3 provider class base on Groq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "class GroqProviderLlama3(BaseLLMProvider):\n",
    "\n",
    "    def initialize_client(self):\n",
    "        api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"GROQ_API_KEY is not set in the environment variables.\")\n",
    "        \n",
    "        self.client = Groq(\n",
    "            api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "        )\n",
    "\n",
    "    def get_model_response(self, chat_messages):\n",
    "        try:\n",
    "            \n",
    "            # Mandatory parameters\n",
    "            params = {\n",
    "                \"model\": self.model_name,\n",
    "                \"messages\": chat_messages.get_messages()\n",
    "            }\n",
    "\n",
    "            # Optional parameters\n",
    "            if self._max_tokens is not None:\n",
    "                params[\"max_tokens\"] = self._max_tokens\n",
    "            if self._temperature is not None:\n",
    "                params[\"temperature\"] = self._temperature\n",
    "                \n",
    "            response = self.client.chat.completions.create(**params)\n",
    "\n",
    "            try:\n",
    "                content = response.choices[0].message.content\n",
    "            except (AttributeError, IndexError) as e:\n",
    "                raise RuntimeError(f\"Malformed response structure: {str(e)}. Response: {response}\")\n",
    "            return content\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to fetch model response. Params: {params}. Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"llama-3.2-90b-vision-preview\"\n",
    "provider = GroqProviderLlama3(model_name=model_name, system_prompt=system_prompt)\n",
    "\n",
    "chat_client = ChatClient(provider=provider)\n",
    "chat_client.prompt_model(\"Hello, who are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we have done so far\n",
    "\n",
    "We have built a modular and extensible architecture for the OmniChatClient. At its core is the `BaseLLMProvider`, which implements the shared functionality defined by the `LLMProviderInterface`. Each provider-specific class extends the `BaseLLMProvider` to handle unique API requirements and interactions.\n",
    "\n",
    "The inheritance hierarchy below illustrates how different providers OpenAI, Anthropic, Groq, and X.AI—are integrated:\n",
    "- `OpenAIProvider`, `AnthropicProvider`, and `GroqProviderLlama3` extend `BaseLLMProvider`.\n",
    "- X.AI reuses the existing OpenAI and Anthropic implementations by inheriting from their respective providers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "flowchart BT\n",
    "    \n",
    "    BaseLLMProvider -->|Implements| LLMProviderInterface\n",
    "\n",
    "    subgraph Inheritance[\"Inheritance Hierarchy\"]\n",
    "        direction BT\n",
    "        OpenAIProvider -->|Extends| BaseLLMProvider\n",
    "        AnthropicProvider -->|Extends| BaseLLMProvider\n",
    "        GroqProviderLlama3 -->|Extends| BaseLLMProvider\n",
    "        XAIProviderOpenAI -->|Inherits from| OpenAIProvider\n",
    "        XAIAnthropicProvider -->|Inherits from| AnthropicProvider\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Vision Capabilities\n",
    "\n",
    "To add vision capabilities, we first update the `append_user_message` method in the `ChatMessages` class to accept images. Since each provider has a different format for handling images, the actual implementation is done in the provider-specific child classes.\n",
    "\n",
    "To update the method, I use [`@patch`from `fastcore`](https://fastcore.fast.ai/basics.html#patch), which is a nice tool which allows to only add functionality as it is needed in a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.utils import * #for importing patch\n",
    "\n",
    "@patch\n",
    "def append_user_message(self:ChatMessages, content=None, base64_images=None):\n",
    "    \"\"\"\n",
    "    Appends a user message with specified content\n",
    "    \"\"\"\n",
    "    if base64_images:\n",
    "        raise NotImplementedError(\"Vision is not supported by this model.\")\n",
    "    \n",
    "    if not content:\n",
    "        raise ValueError(\"Content cannot be empty or None.\")\n",
    "\n",
    "    self._append_message(\"user\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, we also need to update the `prompt_model` method of the `ChatClient` to allow for vision-based prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def prompt_model(self:ChatClient, prompt=None, base64_images=None):\n",
    "    \"\"\"\n",
    "    Sends a message to the model, including an optional prompt and multiple base64 images.\n",
    "    \n",
    "    :param prompt: The text prompt for the model.\n",
    "    :param base64_images: A list of base64-encoded image strings.\n",
    "    :return: The model's response in Markdown format.\n",
    "    \"\"\"\n",
    "    if base64_images:\n",
    "        # Append user message with multiple images\n",
    "        self._chat_messages.append_user_message(content=prompt, base64_images=base64_images)\n",
    "    elif prompt:\n",
    "        # Append user message with only text\n",
    "        self._chat_messages.append_user_message(content=prompt)\n",
    "\n",
    "    content = self._get_model_response()\n",
    "    self._chat_messages.append_assistant_message(content)\n",
    "    return Markdown(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement the different formats in the vendor-specific chat message classes. Pay attention to the subtle differences in how each provider handles vision inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIChatMessages(ChatMessages):\n",
    "\n",
    "    def append_user_message(self, content=None, base64_images=None):\n",
    "        \"\"\"\n",
    "        Appends a user message with specified content and multiple images to messages list.\n",
    "        As per https://platform.openai.com/docs/guides/vision\n",
    "        \"\"\"\n",
    "        if base64_images:\n",
    "            content_parts = [{\"type\": \"text\", \"text\": content}] if content else []\n",
    "            for base64_image in base64_images:\n",
    "                content_parts.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}\n",
    "                })\n",
    "            self._messages.append({\"role\": \"user\", \"content\": content_parts})\n",
    "        elif content:\n",
    "            self._append_message(\"user\", content)\n",
    "        else:\n",
    "            raise ValueError(\"Content cannot be empty or None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnthropicChatMessages(ChatMessages):\n",
    "\n",
    "    def append_user_message(self, content=None, base64_images=None):\n",
    "        \"\"\"\n",
    "        Appends a user message with specified content and multiple images to messages list.\n",
    "        As per https://docs.anthropic.com/en/api/messages\n",
    "        \"\"\"\n",
    "        if base64_images:\n",
    "            content_parts = [{\"type\": \"text\", \"text\": content}] if content else []\n",
    "            for base64_image in base64_images:\n",
    "                content_parts.append({\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": \"image/png\",\n",
    "                        \"data\": base64_image\n",
    "                    }\n",
    "                })\n",
    "            self._messages.append({\"role\": \"user\", \"content\": content_parts})\n",
    "        elif content:\n",
    "            self._append_message(\"user\", content)\n",
    "        else:\n",
    "            raise ValueError(\"Content cannot be empty or None.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to update the `chat_messages_factory` method in all provider classes, so that they use the corresponding chat messages implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def chat_messages_factory(self:OpenAIProvider):\n",
    "    \"\"\"Returns the standard ChatMessages implementation for OpenAI.\"\"\"\n",
    "    chat_messages = OpenAIChatMessages()\n",
    "    chat_messages.append_system_message(self._system_prompt)\n",
    "    return chat_messages\n",
    "\n",
    "@patch\n",
    "def chat_messages_factory(self:AnthropicProvider):\n",
    "    \"\"\"Returns the Anthropic-specific ChatMessages implementation.\"\"\"\n",
    "    return AnthropicChatMessages()\n",
    "\n",
    "@patch\n",
    "def chat_messages_factory(self:GroqProviderLlama3):\n",
    "    \"\"\"OpenAI Format also works for Llama.\"\"\"\n",
    "    return OpenAIChatMessages()\n",
    "\n",
    "#@patch\n",
    "#def chat_messages_factory(self:XAIProviderAnthropic):\n",
    "#    \"\"\"Returns the Base ChatMessages implementation.\"\"\"\n",
    "#    return ChatMessages()\n",
    "\n",
    "#@patch\n",
    "#def chat_messages_factory(self:XAIProviderOpenAI):\n",
    "#    \"\"\"Returns the Base ChatMessages implementation.\"\"\"\n",
    "#    return ChatMessages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Vision\n",
    "\n",
    "Let’s test the vision capabilities of our implementation using the famous Trolley Problem.\n",
    "\n",
    "#### Preparing the Image\n",
    "\n",
    "First, we encode the image in base64 and have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def encode_image(image_path):\n",
    "  \"\"\"Encodes an image file in base64\"\"\"\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def render_base64_image(base64_string):\n",
    "    \"\"\"Render a Base64-encoded image in a Jupyter Notebook.\"\"\"\n",
    "    display(Image(data=base64.b64decode(base64_string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_path = \"Trolley_Problem.png\"\n",
    "base64_image = encode_image(image_path)\n",
    "render_base64_image(base64_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Models\n",
    "\n",
    "Now, let’s ask each model to describe the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The image depicts the classic \"trolley problem,\" a philosophical thought experiment. It shows a trolley on a track heading towards five people. There is a lever that can be pulled to switch the trolley onto another track, where it would hit one person instead. The scenario is used to discuss ethical decision-making and moral dilemmas."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt-4o\"\n",
    "provider = OpenAIProvider(model_name=model_name)\n",
    "chat_client = ChatClient(provider=provider)\n",
    "chat_client.prompt_model(\"What is in the image?\", base64_images=[base64_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This is the classic \"Trolley Problem\" ethical dilemma illustrated in a simple diagram. It shows a trolley/tram on tracks with two possible paths: one track leads to five people, while the other track has one person. There's also a figure shown near a lever that could switch the trolley's direction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"claude-3-5-sonnet-20241022\"\n",
    "provider = AnthropicProvider(model_name=model_name, system_prompt=system_prompt, max_tokens=128)\n",
    "chat_client = ChatClient(provider=provider)\n",
    "chat_client.prompt_model(\"What is in the image?\", base64_images=[base64_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The image depicts a simplified diagram of a train or tram on tracks with a switch mechanism. There are two tracks: one leading straight ahead and another branching off to the right. \n",
       "\n",
       "- On the left side, there is a train or tram on the left track.\n",
       "- In the middle, there is a person standing near the switch mechanism.\n",
       "- On the right side, there are two groups of people:\n",
       "  - One group is on the straight track ahead.\n",
       "  - Another group is on the track that branches off to the right.\n",
       "\n",
       "This diagram is often used to illustrate the trolley problem, a moral dilemma in ethics where one must decide whether to switch the train to another track to save a group of people at the cost of another individual."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"grok-vision-beta\"\n",
    "provider = XAIProviderOpenAI(model_name=model_name)\n",
    "chat_client = ChatClient(provider=provider)\n",
    "chat_client.prompt_model(\"What is in the image?\", base64_images=[base64_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The image depicts a black-and-white graphic of a train and people. The train is on the left side of the image, with a diamond-shaped object above it. There are several lines extending from the train, each featuring a person icon. The lines vary in length, with some being longer than others. The background of the image is white.\n",
       "\n",
       "**Key Elements:**\n",
       "\n",
       "*   **Train:** A black train with a diamond-shaped object above it.\n",
       "*   **Lines:** Several lines extending from the train, each featuring a person icon.\n",
       "*   **Person Icons:** Small black figures representing people.\n",
       "*   **Background:** White background.\n",
       "\n",
       "**Interpretation:**\n",
       "\n",
       "The image appears to be a simple graphic representation of a train and its passengers. The lines extending from the train may represent the paths or routes taken by the passengers. The person icons could symbolize the individuals traveling on the train. The diamond-shaped object above the train is unclear in its meaning but may represent a signal or a marker. Overall, the image conveys a sense of movement and transportation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"llama-3.2-90b-vision-preview\"\n",
    "provider = GroqProviderLlama3(model_name=model_name)\n",
    "chat_client = ChatClient(provider=provider)\n",
    "chat_client.prompt_model(\"What is in the image?\", base64_images=[base64_image])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "As you can see, it is very easy to benchmarking different models against the same input. In this test, GPT-4o, Claude Sonnet 3.5, and Grok correctly recognized the Trolley Problem. Llama 3.2, however, just gave a generic description of the scene, missing the high-level concept. This demonstrates how the OmniChatClient design allows you to efficiently test and compare multiple models in a unified way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Building the OmniChatClient has been an exciting project, creating a unified and extensible interface for working with multiple LLMs. By abstracting provider-specific functionality into modular components, we’ve reduced redundancy and created a scalable framework that can easily adapt to future requirements.\n",
    "\n",
    "Dependency injection ensures that the OmniChatClient remains flexible and decoupled from provider-specific details, which are implemented in dedicated provider classes. Inheritance further reduces redundancies and accelerates the addition of new providers and features. As a result, any application using the OmniChatClient can seamlessly switch between models by simply swapping out the provider objects, no additional code changes are required.\n",
    "\n",
    "While this implementation is already functional and supports both text and vision capabilities, there’s still room to expand. Enabling tool usage and adding support for providers like Google’s Gemini series are in my backlog.\n",
    "\n",
    "On a broader level, I hope this hands-on demonstration of dependency injection and inheritance, applied in a practical example, inspires you to experiment with these patterns to make your code more robust, flexible, and elegant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
