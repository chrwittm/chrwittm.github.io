<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Wittmann">
<meta name="dcterms.date" content="2024-11-30">

<title>OmniChat - How to chat with any LLM – chrwittm.github.io</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-08e1269e8557b430e2197ccc38ae6770.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-GF3YYKQQNH"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-GF3YYKQQNH', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"express",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":"",
"website_privacy_policy_url":"/privacy.html"
  ,
"language":"en"
  });
});
</script> 
  
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">chrwittm.github.io</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../license.html"> 
<span class="menu-text">License</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../impressum.html"> 
<span class="menu-text">Impressum</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../privacy.html"> 
<span class="menu-text">Privacy</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/chrwittm"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/chrwittm"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://de.linkedin.com/in/chrwittm"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.kaggle.com/christianwittmann"> <i class="bi bi-file-earmark-code" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">OmniChat - How to chat with any LLM</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">chat</div>
                <div class="quarto-category">llm</div>
                <div class="quarto-category">python</div>
                <div class="quarto-category">injection</div>
                <div class="quarto-category">openai</div>
                <div class="quarto-category">llama</div>
                <div class="quarto-category">anthropic</div>
                <div class="quarto-category">x.ai</div>
                <div class="quarto-category">grok</div>
                <div class="quarto-category">groq</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Christian Wittmann </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 30, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>So far, I’ve worked with large language models (LLMs) from OpenAI and Meta (the LLaMA series) in separate implementations. I noticed that the code across these implementations was very similar, with only minor variations for provider-specific details. This realization inspired me to harmonize the codebase into a unified, modular chat client with the current working title “OmniChatClient”. The name “Omni” reflects two key aspects of its design: It seamlessly integrates with several LLM providers, and it supports multimodal inputs (text and vision so far).</p>
<p>In this blog post, I’ll walk you through the steps I took to create the OmniChatClient. The current version works with OpenAI, the LLaMA series (via Groq), Anthropic’s Claude, and X.AI’s Grok, using both the SDKs from OpenAI and Anthropic. By the end, you’ll see how OmniChatClient’s code structure enables flexibility, maintainability, and scalability. I’ll demonstrate how I implemented support for both text and vision inputs. I will save additional features like function calling or adding additional vendor support for later posts.</p>
<p>This blog post focuses on the refactoring process that transformed my earlier implementations into OmniChatClient. If you’re interested in the inner mechanics of the chat client and the chat messages, please refer to my previous blog posts like <a href="https://chrwittm.github.io/posts/2024-02-23-chat-from-scratch/">Building Chat for Jupyter Notebooks from Scratch</a> or <a href="https://chrwittm.github.io/posts/2024-08-02-llm-calculator2-vision/">Building the Apple Calculator in a Jupyter Notebook</a>.</p>
<p>A final note before we start: As usual, if you prefer the interactive notebook version of this blog post, please use the <a href="https://github.com/chrwittm/lm-hackers/blob/main/60-omni-chat/index.ipynb">this version on GitHub</a>.</p>
<section id="architecture" class="level2">
<h2 class="anchored" data-anchor-id="architecture">Architecture</h2>
<p>Before diving into the code, let’s explore the challenge and the approach I took to solve it. At first glance, the APIs of various LLM providers seem very similar: You specify a model name, send <code>user</code>-messages, and receive <code>assistant</code>-messages, all wrapped in a JSON schema. However, subtle differences emerge on closer inspection. For example:</p>
<ul>
<li>OpenAI expects the system prompt to be included in the chat messages, whereas Anthropic requires it to be passed as a separate parameter.</li>
<li>The maximum token limit (<code>max_tokens</code>) is mandatory in Anthropic’s API, but optional for OpenAI and other providers I’ve worked with so far.</li>
</ul>
<p>These differences become even more pronounced when going beyond simple text-based chat. For instance:</p>
<ul>
<li>OpenAI and Anthropic use different formats for passing images into the chat.</li>
<li>Llama 3.2 models only support one image at a time, while OpenAI and Anthropic can handle multiple images.</li>
</ul>
<p>Despite these variations, the overarching process flow remains consistent when viewed at a higher level.</p>
<p>To address these challenges, I decided to decouple provider-specific details from the generic chat client logic. This is achieved through “dependency injection” (for a nice video intro I can recommend <a href="https://www.youtube.com/watch?v=J1f5b4vcxCQ">Dependency Injection, The Best Pattern</a> from <a href="https://www.youtube.com/@CodeAesthetic">CodeAesthetic</a>), where an LLM-specific provider class is injected into the chat client. For message management, I chose an inheritance structure, with a base <code>ChatMessages</code> class handling common functionality and provider-specific subclasses extending it to support provider-specific message formatting requirements.</p>
<section id="injecting-provider-specific-class-into-chat-client" class="level3">
<h3 class="anchored" data-anchor-id="injecting-provider-specific-class-into-chat-client">Injecting Provider-Specific Class into Chat Client</h3>
<p>The <code>ChatClient</code> is a provider-agnostic class responsible for managing the conversation flow. It delegates provider-specific functionality, such as authentication and API interactions, to a class implementing the <code>LLMProviderInterface</code>. This interface defines the contract for provider-specific behavior, ensuring consistent integration regardless of the provider.</p>
<p>To maximize reusability, shared logic is abstracted into the <code>BaseLLMProvider</code>, which implements <code>LLMProviderInterface</code>. Provider-specific classes, such as <code>OpenAIProvider</code> and <code>AnthropicProvider</code>, extend <code>BaseLLMProvider</code> to handle unique API requirements.</p>
<p>Upon instantiating the <code>ChatClient</code>, you inject the desired provider class, enabling the client to remain decoupled from provider-specific details while leveraging the functionality defined by the provider.</p>
<p>The diagram below illustrates the architecture I implemented. For readability, it contains only 2 providers: OpenAI and Anthropic. (You can find a more detailed version later in this post.)</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart BT
    
    LLMProviderInterface --&gt;|Injects| ChatClient 

    subgraph Injection["Dependency Injection"]
        direction BT
        style Injection fill:#FFFFE0
    
        BaseLLMProvider --&gt;|Implements| LLMProviderInterface

        subgraph Inheritance["Inheritance Hierarchy"]
            direction BT
            OpenAIProvider --&gt;|Extends| BaseLLMProvider
            AnthropicProvider --&gt;|Extends| BaseLLMProvider
            style Inheritance fill:#D0E6F5
        end

    end   
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="chatmessages-with-provider-specific-classes" class="level3">
<h3 class="anchored" data-anchor-id="chatmessages-with-provider-specific-classes">ChatMessages with Provider-Specific Classes</h3>
<p>The <code>LLMProviderInterface</code> includes a method called <code>chat_messages_factory</code>, which is responsible for creating an instance of the appropriate <code>ChatMessages</code> class. These classes follow an inheritance hierarchy to handle both common functionality and provider-specific message formatting.</p>
<p>The base <code>ChatMessages</code> class provides shared logic for managing conversation history and appending messages in a generic format. Provider-specific subclasses, such as <code>OpenAIChatMessages</code> and <code>AnthropicChatMessages</code>, extend this base class to handle unique requirements, such as formatting multimodal inputs (e.g., images).</p>
<p>This design allows the <code>ChatClient</code> to remain agnostic of provider-specific message handling, while the injected provider determines which <code>ChatMessages</code> subclass to use.</p>
<p>The diagram below illustrates the inheritance hierarchy of <code>ChatMessages</code> (again, it is limited to OpenAI and Anthropic for readability):</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart BT
    OpenChatMessages --&gt;|Extends| ChatMessages
    AnthropicChatMessages --&gt;|Extends| ChatMessages
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="the-full-architecture" class="level3">
<h3 class="anchored" data-anchor-id="the-full-architecture">The full architecture</h3>
<p>Now that we’ve discussed the individual components, let’s put everything together to see the big picture. The diagram below illustrates how the key classes interact to create a flexible, multi-provider chat system.</p>
<p>If this looks overwhelming at first, don’t worry! We’ll construct this step-by-step throughout the blog post. You can always revisit this diagram later once you’ve gone through the detailed explanations.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">classDiagram
    %% ChatClient and Provider Interface
    class ChatClient {
        Responsible for managing the conversation with LLM
        - _provider: LLMProviderInterface
        - _chat_messages: ChatMessages
        +__init__(provider: LLMProviderInterface)
        +prompt_model(prompt: str, base64_images: Optional[List[str]])
    }

    class LLMProviderInterface {
        Defines provider-specific implementation points
        +__init__(model_name: str, **kwargs)
        +chat_messages_factory() ChatMessages
        +_get_model_response(chat_messages: ChatMessages)
    }

    class ChatMessages {
        Manages the conversation history and message structure
        - _messages: List[Dict]
        +append_user_message(content: str, base64_images: Optional[List[str]])
        +append_assistant_message(content: str)
    }

    %% Relationships
    ChatClient ..&gt; LLMProviderInterface : Dependency Injection
    LLMProviderInterface o-- ChatMessages : Factory Creates
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
</section>
<section id="creating-the-text-based-omnichatclient" class="level2">
<h2 class="anchored" data-anchor-id="creating-the-text-based-omnichatclient">Creating the text-based OmniChatClient</h2>
<p>Let’s start simple by implementing a text-only version of the OmniChatClient. This will lay the foundation for adding more advanced functionality like vision support in later iterations.</p>
<div id="cell-10" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dotenv <span class="im">import</span> load_dotenv</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>load_dotenv()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>True</code></pre>
</div>
</div>
<section id="chatmessages" class="level3">
<h3 class="anchored" data-anchor-id="chatmessages">ChatMessages</h3>
<p>First, we need a class to manage the conversation history and structure messages. Below is a simplified version of the ChatMessages class, similar to what I introduced in <a href="https://chrwittm.github.io/posts/2024-02-23-chat-from-scratch/">Building Chat for Jupyter Notebooks from Scratch</a>. Provider-specific classes will extend this base class only when needed.</p>
<div id="cell-12" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display, Markdown</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ChatMessages:</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Initializes the Chat."""</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._messages <span class="op">=</span> []</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _append_message(<span class="va">self</span>, role, content):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Appends a message with specified role and content to messages list."""</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._messages.append({<span class="st">"role"</span>: role, <span class="st">"content"</span>: content})</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> append_system_message(<span class="va">self</span>, content):</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Appends a system message with specified content to messages list."""</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> content:</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># No empty system message</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._append_message(<span class="st">"system"</span>, content)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> append_user_message(<span class="va">self</span>, content<span class="op">=</span><span class="va">None</span>, base64_images<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Appends a user message with specified content"""</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._append_message(<span class="st">"user"</span>, content)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> append_assistant_message(<span class="va">self</span>, content):</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Appends an assistant message with specified content."""</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._append_message(<span class="st">"assistant"</span>, content)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_messages(<span class="va">self</span>):</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Returns a shallow copy of the messages list."""</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._messages[:]</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_debug_view(<span class="va">self</span>):</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Returns the debug view of the chat messages formatted as Markdown."""</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        debug_view <span class="op">=</span> []</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> message <span class="kw">in</span> <span class="va">self</span>._messages:</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>            role <span class="op">=</span> message.get(<span class="st">'role'</span>)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>            content <span class="op">=</span> message.get(<span class="st">'content'</span>, <span class="st">''</span>)</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>            debug_view.append(<span class="ss">f"**</span><span class="sc">{</span>role<span class="sc">}</span><span class="ss">**: </span><span class="sc">{</span>content<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> Markdown(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>.join(debug_view))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="llmproviderinterface" class="level3">
<h3 class="anchored" data-anchor-id="llmproviderinterface">LLMProviderInterface</h3>
<p>Next, we define the LLMProviderInterface, which serves as a contract for all provider-specific classes. These classes will handle authentication, API interactions, and any provider-specific logic.</p>
<div id="cell-14" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LLMProviderInterface:</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> initialize_client(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">        Abstract method for initializing the model prover client including authentication.</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">        This method is called from the provider's constructor.</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>()</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> initialize_system_prompt(<span class="va">self</span>, system_prompt):</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">        Abstract method for initializing the system prompt.</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co">        :param system_prompt: The system prompt for the ChatClient.</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>()</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_model_response(<span class="va">self</span>, chat_messages: ChatMessages):</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co">        Abstract method for fetching the model response.</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co">        :param chat_messages: ChatMessages object containing the conversation history.</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>()</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> chat_messages_factory(<span class="va">self</span>) <span class="op">-&gt;</span> ChatMessages:</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Abstract factory method to create the appropriate ChatMessages class."""</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="basellmprovider" class="level3">
<h3 class="anchored" data-anchor-id="basellmprovider">BaseLLMProvider</h3>
<p>The <code>BaseLLMProvider</code> implements shared functionality like client initialization and serves as the foundation for provider-specific classes.</p>
<div id="cell-16" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BaseLLMProvider(LLMProviderInterface):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        model_name: <span class="bu">str</span>,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        system_prompt: Optional[<span class="bu">str</span>] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        max_tokens: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        temperature: Optional[<span class="bu">float</span>] <span class="op">=</span> <span class="fl">0.0</span>,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model_name <span class="op">=</span> model_name</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._max_tokens <span class="op">=</span> max_tokens</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._temperature <span class="op">=</span> temperature</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.initialize_system_prompt(system_prompt)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.initialize_client()</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> initialize_system_prompt(<span class="va">self</span>, system_prompt <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._system_prompt <span class="op">=</span> system_prompt</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> initialize_client(<span class="va">self</span>):</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>()</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_model_response(<span class="va">self</span>, chat_messages, tools<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>()</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> chat_messages_factory(<span class="va">self</span>):</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ChatMessages()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="chatclient" class="level3">
<h3 class="anchored" data-anchor-id="chatclient">ChatClient</h3>
<p>Finally, the <code>ChatClient</code> class orchestrates the conversation flow. Notice how provider-specific logic is delegated to the injected provider, ensuring the client remains provider-agnostic.</p>
<div id="cell-18" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Markdown</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ChatClient:</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, provider: LLMProviderInterface):</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Initializes the Chat with the system message."""</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._provider <span class="op">=</span> provider</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._chat_messages <span class="op">=</span> provider.chat_messages_factory()</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _get_model_response(<span class="va">self</span>):</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Delegates response fetching to the LLM-provider class."""</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._provider.get_model_response(</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>            chat_messages<span class="op">=</span><span class="va">self</span>._chat_messages,</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> prompt_model(<span class="va">self</span>, prompt<span class="op">=</span><span class="va">None</span>, base64_images<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""This method handles the user prompt, delegates interaction with the model through the provider, </span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co">        and returns the model response."""</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._chat_messages.append_user_message(prompt)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        content <span class="op">=</span> <span class="va">self</span>._get_model_response()</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._chat_messages.append_assistant_message(content)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> Markdown(content)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With this foundation, we can now implement provider-specific classes (e.g., for OpenAI and Anthropic). These classes will inherit from BaseLLMProvider and implement methods like <code>initialize_client</code> and <code>get_model_response</code> to handle provider-specific details.</p>
</section>
</section>
<section id="provider-implementations" class="level2">
<h2 class="anchored" data-anchor-id="provider-implementations">Provider Implementations</h2>
<section id="openai-implementation" class="level3">
<h3 class="anchored" data-anchor-id="openai-implementation">OpenAI Implementation</h3>
<p>The <a href="https://platform.openai.com/docs/api-reference/introduction">OpenAI</a> provider is implemented using the <a href="https://github.com/openai/openai-python">OpenAI SDK</a>. This provider class handles client initialization, constructs the necessary parameters for API calls, and manages the chat message structure.</p>
<p>Noteworthy points:</p>
<ul>
<li>The system prompt is included directly in the chat messages. This behavior is specific to OpenAI and differs from, for example, Anthropic’s implementation.</li>
<li>Optional parameters like <code>max_tokens</code> and <code>temperature</code> are conditionally added to the API request, ensuring flexibility without hardcoding defaults.</li>
</ul>
<div id="cell-21" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> openai <span class="im">import</span> OpenAI</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OpenAIProvider(BaseLLMProvider):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> initialize_client(<span class="va">self</span>):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        api_key <span class="op">=</span> os.getenv(<span class="st">"OPENAI_API_KEY"</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> api_key:</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"OPENAI_API_KEY is not set in the environment variables."</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.client <span class="op">=</span> OpenAI()</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_model_response(<span class="va">self</span>, chat_messages):</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Mandatory parameters</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>            params <span class="op">=</span> {</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>                <span class="st">"model"</span>: <span class="va">self</span>.model_name,</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>                <span class="st">"messages"</span>: chat_messages.get_messages()</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Optional parameters</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>._max_tokens <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>                params[<span class="st">"max_tokens"</span>] <span class="op">=</span> <span class="va">self</span>._max_tokens</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>._temperature <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>                params[<span class="st">"temperature"</span>] <span class="op">=</span> <span class="va">self</span>._temperature</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>            response <span class="op">=</span> <span class="va">self</span>.client.chat.completions.create(<span class="op">**</span>params)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>                content <span class="op">=</span> response.choices[<span class="dv">0</span>].message.content</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span> (<span class="pp">AttributeError</span>, <span class="pp">IndexError</span>) <span class="im">as</span> e:</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="ss">f"Malformed response structure: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ss">. Response: </span><span class="sc">{</span>response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> content</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="ss">f"Failed to fetch model response. Params: </span><span class="sc">{</span>params<span class="sc">}</span><span class="ss">. Error: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> chat_messages_factory(<span class="va">self</span>):</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Returns the standard ChatMessages implementation for OpenAI."""</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>        chat_messages <span class="op">=</span> ChatMessages()</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>        chat_messages.append_system_message(<span class="va">self</span>._system_prompt)</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> chat_messages</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s test the OpenAI provider with a simple prompt:</p>
<div id="cell-23" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"gpt-4o"</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>system_prompt <span class="op">=</span> <span class="st">"Answer in a very concise and accurate way"</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>provider <span class="op">=</span> OpenAIProvider(model_name<span class="op">=</span>model_name, system_prompt<span class="op">=</span>system_prompt)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>chat_client <span class="op">=</span> ChatClient(provider<span class="op">=</span>provider)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>chat_client.prompt_model(<span class="st">"Hello, who are you?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="32">
<p>Hello, I’m an AI language model created by OpenAI, here to assist you with information and answer your questions.</p>
</div>
</div>
</section>
<section id="anthropic-implementation" class="level3">
<h3 class="anchored" data-anchor-id="anthropic-implementation">Anthropic implementation</h3>
<p>The <a href="https://docs.anthropic.com/en/api/getting-started">Anthropic</a> provider is implemented using the <a href="https://github.com/anthropics/anthropic-sdk-python">Anthropic SDK</a>. This class manages client initialization, validates required parameters, and adjusts the message structure for Anthropic’s API.</p>
<p>Noteworthy points:</p>
<ul>
<li><code>max_tokens</code> is a required field for calling Anthropic’s Claude model. This is validated during provider initialization to avoid runtime errors.</li>
<li>Unlike the OpenAI implementation, the system prompt is passed directly to the <code>client</code> as a separate parameter, instead of embedding it in the chat messages.</li>
</ul>
<div id="cell-25" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> anthropic <span class="im">import</span> Anthropic</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AnthropicProvider(BaseLLMProvider):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        max_tokens <span class="op">=</span> kwargs.get(<span class="st">"max_tokens"</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> max_tokens <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"AnthropicProvider requires 'max_tokens' to be specified."</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">*</span>args, <span class="op">**</span>kwargs)  <span class="co"># Pass all arguments to the base constructor</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> initialize_system_prompt(<span class="va">self</span>, system_prompt <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().initialize_system_prompt(system_prompt)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>._system_prompt <span class="op">==</span> <span class="va">None</span>:</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._system_prompt <span class="op">=</span> <span class="st">""</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> initialize_client(<span class="va">self</span>):</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        api_key <span class="op">=</span> os.getenv(<span class="st">"ANTHROPIC_API_KEY"</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> api_key:</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"ANTHROPIC_API_KEY is not set in the environment variables."</span>)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.client <span class="op">=</span> Anthropic()</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_model_response(<span class="va">self</span>, chat_messages):</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Mandatory parameters</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>            params <span class="op">=</span> {</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>                <span class="st">"model"</span>: <span class="va">self</span>.model_name,</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>                <span class="st">"system"</span>: <span class="va">self</span>._system_prompt,</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>                <span class="st">"messages"</span>: chat_messages.get_messages(),</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>                <span class="st">"max_tokens"</span>: <span class="va">self</span>._max_tokens</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Optional parameters</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>._temperature <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>                params[<span class="st">"temperature"</span>] <span class="op">=</span> <span class="va">self</span>._temperature</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>            response <span class="op">=</span> <span class="va">self</span>.client.messages.create(<span class="op">**</span>params)</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>                content <span class="op">=</span> <span class="st">""</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> content_line <span class="kw">in</span> response.content:</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> content_line.<span class="bu">type</span> <span class="op">==</span> <span class="st">"text"</span>:</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>                        content <span class="op">=</span> content_line.text  </span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> content</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span> (<span class="pp">AttributeError</span>, <span class="pp">IndexError</span>) <span class="im">as</span> e:</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="ss">f"Malformed response structure: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ss">. Response: </span><span class="sc">{</span>response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="ss">f"Failed to fetch model response. Params: </span><span class="sc">{</span>params<span class="sc">}</span><span class="ss">. Error: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Again, let’s run a simple test. Notice how the code remains compact and modular: Only the model name and provider instantiation change. The OmniChatClient abstracts away the different APIs and message structures through the injected provider.</p>
<div id="cell-27" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"claude-3-5-sonnet-20241022"</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>provider <span class="op">=</span> AnthropicProvider(model_name<span class="op">=</span>model_name, system_prompt<span class="op">=</span>system_prompt, max_tokens<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>chat_client <span class="op">=</span> ChatClient(provider<span class="op">=</span>provider)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>chat_client.prompt_model(<span class="st">"Hello, who are you?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="34">
<p>I’m Claude, an AI assistant created by Anthropic. I aim to be direct and truthful in our conversations.</p>
</div>
</div>
</section>
<section id="x.ai-implementation" class="level3">
<h3 class="anchored" data-anchor-id="x.ai-implementation">X.ai Implementation</h3>
<p>This is where the effort we’ve invested in building a modular architecture starts to pay off. With the implementation for <a href="https://docs.x.ai/docs">X.ai</a>, we can reuse much of the code we’ve already written for OpenAI and Anthropic, saving time and effort.</p>
<p>To interact with Grok, X.ai allows you to use <a href="https://docs.x.ai/api/integrations">either the OpenAI or Anthropic SDKs</a>. Since we’ve already implemented both, all we need to do is update the endpoint and authorization details.</p>
<section id="using-the-openai-sdk-for-x.ai" class="level4">
<h4 class="anchored" data-anchor-id="using-the-openai-sdk-for-x.ai">Using the OpenAI SDK for X.ai</h4>
<p>Here’s all the code needed to enable Grok support via the OpenAI SDK:</p>
<div id="cell-29" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> openai <span class="im">import</span> OpenAI</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> XAIProviderOpenAI(OpenAIProvider):</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> initialize_client(<span class="va">self</span>):</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>        api_key <span class="op">=</span> os.getenv(<span class="st">"XAI_API_KEY"</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> api_key:</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"XAI_API_KEY is not set in the environment variables."</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        base_url <span class="op">=</span> os.getenv(<span class="st">"XAI_BASE_URL"</span>, <span class="st">"https://api.x.ai/v1"</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.client <span class="op">=</span> OpenAI(</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>            api_key<span class="op">=</span>api_key,    </span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>            base_url<span class="op">=</span>base_url,</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here is a quick test. The previous prompt returned a pretty shy answer, so I updated it to a more grok-like version.</p>
<div id="cell-31" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"grok-beta"</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>provider <span class="op">=</span> XAIProviderOpenAI(model_name<span class="op">=</span>model_name, system_prompt<span class="op">=</span>system_prompt)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>chat_client <span class="op">=</span> ChatClient(provider<span class="op">=</span>provider)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>chat_client.prompt_model(<span class="st">"State your name and identity!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="36">
<p>I am Grok, an AI developed by xAI, here to provide helpful and truthful answers.</p>
</div>
</div>
</section>
<section id="using-the-anthropic-sdk-for-x.ai" class="level4">
<h4 class="anchored" data-anchor-id="using-the-anthropic-sdk-for-x.ai">Using the Anthropic SDK for X.ai</h4>
<p>Similarly, we can use the Anthropic SDK to interact with Grok. Here’s the implementation:</p>
<div id="cell-33" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> anthropic <span class="im">import</span> Anthropic</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> XAIProviderAnthropic(AnthropicProvider):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> initialize_client(<span class="va">self</span>):</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        api_key <span class="op">=</span> os.getenv(<span class="st">"XAI_API_KEY"</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> api_key:</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"XAI_API_KEY is not set in the environment variables."</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        base_url <span class="op">=</span> <span class="st">"https://api.x.ai"</span> </span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.client <span class="op">=</span> Anthropic(</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>            api_key<span class="op">=</span>api_key,    </span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>            base_url<span class="op">=</span>base_url,</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-34" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"grok-beta"</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>provider <span class="op">=</span> XAIProviderAnthropic(model_name<span class="op">=</span>model_name, system_prompt<span class="op">=</span>system_prompt, max_tokens<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>chat_client <span class="op">=</span> ChatClient(provider<span class="op">=</span>provider)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>chat_client.prompt_model(<span class="st">"State your name and identity!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="38">
<p>I am Grok, an AI developed by xAI, here to provide helpful and truthful answers.</p>
</div>
</div>
</section>
</section>
<section id="llama-implementation" class="level3">
<h3 class="anchored" data-anchor-id="llama-implementation">Llama Implementation</h3>
<p>To access Llama 3.2, I chose <a href="https://groq.com/">Groq</a> as the provider. Groq offers a free tier and provides access to Llama 3.2 with vision capabilities, a practical workaround given that downloading Llama 3 in the EU is restricted under its licensing terms.</p>
<p>Here’s the implementation for the Llama 3 provider class base on Groq:</p>
<div id="cell-36" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> groq <span class="im">import</span> Groq</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GroqProviderLlama3(BaseLLMProvider):</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> initialize_client(<span class="va">self</span>):</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        api_key <span class="op">=</span> os.getenv(<span class="st">"GROQ_API_KEY"</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> api_key:</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"GROQ_API_KEY is not set in the environment variables."</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.client <span class="op">=</span> Groq(</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>            api_key<span class="op">=</span>os.environ.get(<span class="st">"GROQ_API_KEY"</span>),</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_model_response(<span class="va">self</span>, chat_messages):</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Mandatory parameters</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>            params <span class="op">=</span> {</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>                <span class="st">"model"</span>: <span class="va">self</span>.model_name,</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>                <span class="st">"messages"</span>: chat_messages.get_messages()</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Optional parameters</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>._max_tokens <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>                params[<span class="st">"max_tokens"</span>] <span class="op">=</span> <span class="va">self</span>._max_tokens</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>._temperature <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>                params[<span class="st">"temperature"</span>] <span class="op">=</span> <span class="va">self</span>._temperature</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>            response <span class="op">=</span> <span class="va">self</span>.client.chat.completions.create(<span class="op">**</span>params)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>                content <span class="op">=</span> response.choices[<span class="dv">0</span>].message.content</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span> (<span class="pp">AttributeError</span>, <span class="pp">IndexError</span>) <span class="im">as</span> e:</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="ss">f"Malformed response structure: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ss">. Response: </span><span class="sc">{</span>response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> content</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="ss">f"Failed to fetch model response. Params: </span><span class="sc">{</span>params<span class="sc">}</span><span class="ss">. Error: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-37" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"llama-3.2-90b-vision-preview"</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>provider <span class="op">=</span> GroqProviderLlama3(model_name<span class="op">=</span>model_name, system_prompt<span class="op">=</span>system_prompt)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>chat_client <span class="op">=</span> ChatClient(provider<span class="op">=</span>provider)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>chat_client.prompt_model(<span class="st">"Hello, who are you?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="40">
<p>I’m an artificial intelligence model known as Llama. Llama stands for “Large Language Model Meta AI.”</p>
</div>
</div>
</section>
</section>
<section id="what-we-have-done-so-far" class="level2">
<h2 class="anchored" data-anchor-id="what-we-have-done-so-far">What we have done so far</h2>
<p>We have built a modular and extensible architecture for the OmniChatClient. At its core is the <code>BaseLLMProvider</code>, which implements the shared functionality defined by the <code>LLMProviderInterface</code>. Each provider-specific class extends the <code>BaseLLMProvider</code> to handle unique API requirements and interactions.</p>
<p>The inheritance hierarchy below illustrates how different providers OpenAI, Anthropic, Groq, and X.AI—are integrated: - <code>OpenAIProvider</code>, <code>AnthropicProvider</code>, and <code>GroqProviderLlama3</code> extend <code>BaseLLMProvider</code>. - X.AI reuses the existing OpenAI and Anthropic implementations by inheriting from their respective providers.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart BT
    
    BaseLLMProvider --&gt;|Implements| LLMProviderInterface

    subgraph Inheritance["Inheritance Hierarchy"]
        direction BT
        style Inheritance fill:#FFFFE0
        OpenAIProvider --&gt;|Extends| BaseLLMProvider
        AnthropicProvider --&gt;|Extends| BaseLLMProvider
        GroqProviderLlama3 --&gt;|Extends| BaseLLMProvider
        XAIProviderOpenAI --&gt;|Inherits from| OpenAIProvider
        XAIAnthropicProvider --&gt;|Inherits from| AnthropicProvider
    end
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="adding-vision-capabilities" class="level2">
<h2 class="anchored" data-anchor-id="adding-vision-capabilities">Adding Vision Capabilities</h2>
<p>To add vision capabilities, we first update the <code>append_user_message</code> method in the <code>ChatMessages</code> class to accept images. Since each provider has a different format for handling images, the actual implementation is done in the provider-specific child classes.</p>
<p>To update the method, I use <a href="https://fastcore.fast.ai/basics.html#patch"><code>@patch</code>from <code>fastcore</code></a>, which is a nice tool which allows to only add functionality as it is needed in a notebook.</p>
<div id="cell-41" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastcore.utils <span class="im">import</span> <span class="op">*</span> <span class="co">#for importing patch</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="at">@patch</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> append_user_message(<span class="va">self</span>:ChatMessages, content<span class="op">=</span><span class="va">None</span>, base64_images<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Appends a user message with specified content</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> base64_images:</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">"Vision is not supported by this model."</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> content:</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Content cannot be empty or None."</span>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>._append_message(<span class="st">"user"</span>, content)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Consequently, we also need to update the <code>prompt_model</code> method of the <code>ChatClient</code> to allow for vision-based prompts:</p>
<div id="cell-43" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="at">@patch</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prompt_model(<span class="va">self</span>:ChatClient, prompt<span class="op">=</span><span class="va">None</span>, base64_images<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Sends a message to the model, including an optional prompt and multiple base64 images.</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :param prompt: The text prompt for the model.</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co">    :param base64_images: A list of base64-encoded image strings.</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co">    :return: The model's response in Markdown format.</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> base64_images:</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append user message with multiple images</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._chat_messages.append_user_message(content<span class="op">=</span>prompt, base64_images<span class="op">=</span>base64_images)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> prompt:</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append user message with only text</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._chat_messages.append_user_message(content<span class="op">=</span>prompt)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    content <span class="op">=</span> <span class="va">self</span>._get_model_response()</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>._chat_messages.append_assistant_message(content)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Markdown(content)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can implement the different formats in the vendor-specific chat message classes. Pay attention to the subtle differences in how each provider handles vision inputs.</p>
<div id="cell-45" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OpenAIChatMessages(ChatMessages):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> append_user_message(<span class="va">self</span>, content<span class="op">=</span><span class="va">None</span>, base64_images<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co">        Appends a user message with specified content and multiple images to messages list.</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co">        As per https://platform.openai.com/docs/guides/vision</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> base64_images:</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>            content_parts <span class="op">=</span> [{<span class="st">"type"</span>: <span class="st">"text"</span>, <span class="st">"text"</span>: content}] <span class="cf">if</span> content <span class="cf">else</span> []</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> base64_image <span class="kw">in</span> base64_images:</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>                content_parts.append({</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"type"</span>: <span class="st">"image_url"</span>,</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"image_url"</span>: {<span class="st">"url"</span>: <span class="ss">f"data:image/png;base64,</span><span class="sc">{</span>base64_image<span class="sc">}</span><span class="ss">"</span>}</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>                })</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._messages.append({<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: content_parts})</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> content:</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._append_message(<span class="st">"user"</span>, content)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Content cannot be empty or None."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-46" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AnthropicChatMessages(ChatMessages):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> append_user_message(<span class="va">self</span>, content<span class="op">=</span><span class="va">None</span>, base64_images<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">        Appends a user message with specified content and multiple images to messages list.</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">        As per https://docs.anthropic.com/en/api/messages</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> base64_images:</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>            content_parts <span class="op">=</span> [{<span class="st">"type"</span>: <span class="st">"text"</span>, <span class="st">"text"</span>: content}] <span class="cf">if</span> content <span class="cf">else</span> []</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> base64_image <span class="kw">in</span> base64_images:</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>                content_parts.append({</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"type"</span>: <span class="st">"image"</span>,</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"source"</span>: {</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"type"</span>: <span class="st">"base64"</span>,</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"media_type"</span>: <span class="st">"image/png"</span>,</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"data"</span>: base64_image</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>                    }</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>                })</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._messages.append({<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: content_parts})</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> content:</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._append_message(<span class="st">"user"</span>, content)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Content cannot be empty or None."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, we need to update the <code>chat_messages_factory</code> method in all provider classes, so that they use the corresponding chat messages implementation.</p>
<div id="cell-48" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="at">@patch</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chat_messages_factory(<span class="va">self</span>:OpenAIProvider):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the standard ChatMessages implementation for OpenAI."""</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    chat_messages <span class="op">=</span> OpenAIChatMessages()</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    chat_messages.append_system_message(<span class="va">self</span>._system_prompt)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> chat_messages</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="at">@patch</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chat_messages_factory(<span class="va">self</span>:AnthropicProvider):</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the Anthropic-specific ChatMessages implementation."""</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> AnthropicChatMessages()</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="at">@patch</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chat_messages_factory(<span class="va">self</span>:GroqProviderLlama3):</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""OpenAI Format also works for Llama."""</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> OpenAIChatMessages()</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="co">#@patch</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a><span class="co">#def chat_messages_factory(self:XAIProviderAnthropic):</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="co">#    """Returns the Base ChatMessages implementation."""</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a><span class="co">#    return ChatMessages()</span></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a><span class="co">#@patch</span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a><span class="co">#def chat_messages_factory(self:XAIProviderOpenAI):</span></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a><span class="co">#    """Returns the Base ChatMessages implementation."""</span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a><span class="co">#    return ChatMessages()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="testing-vision" class="level3">
<h3 class="anchored" data-anchor-id="testing-vision">Testing Vision</h3>
<p>Let’s test the vision capabilities of our implementation using the famous Trolley Problem.</p>
<section id="preparing-the-image" class="level4">
<h4 class="anchored" data-anchor-id="preparing-the-image">Preparing the Image</h4>
<p>First, we encode the image in base64 and have a look:</p>
<div id="cell-50" class="cell" data-execution_count="46">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> base64</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Image, display</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> encode_image(image_path):</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""Encodes an image file in base64"""</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">with</span> <span class="bu">open</span>(image_path, <span class="st">"rb"</span>) <span class="im">as</span> image_file:</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> base64.b64encode(image_file.read()).decode(<span class="st">'utf-8'</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> render_base64_image(base64_string):</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Render a Base64-encoded image in a Jupyter Notebook."""</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    display(Image(data<span class="op">=</span>base64.b64decode(base64_string)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-51" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">"Trolley_Problem.png"</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>base64_image <span class="op">=</span> encode_image(image_path)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>render_base64_image(base64_image)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="running-the-models" class="level4">
<h4 class="anchored" data-anchor-id="running-the-models">Running the Models</h4>
<p>Now, let’s ask each model to describe the image:</p>
<div id="cell-53" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"gpt-4o"</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>provider <span class="op">=</span> OpenAIProvider(model_name<span class="op">=</span>model_name)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>chat_client <span class="op">=</span> ChatClient(provider<span class="op">=</span>provider)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>chat_client.prompt_model(<span class="st">"What is in the image?"</span>, base64_images<span class="op">=</span>[base64_image])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="48">
<p>The image depicts the classic “trolley problem,” a philosophical thought experiment. It shows a trolley on a track heading towards five people. There is a lever that can be pulled to switch the trolley onto another track, where it would hit one person instead. The scenario is used to discuss ethical decision-making and moral dilemmas.</p>
</div>
</div>
<div id="cell-54" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"claude-3-5-sonnet-20241022"</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>provider <span class="op">=</span> AnthropicProvider(model_name<span class="op">=</span>model_name, system_prompt<span class="op">=</span>system_prompt, max_tokens<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>chat_client <span class="op">=</span> ChatClient(provider<span class="op">=</span>provider)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>chat_client.prompt_model(<span class="st">"What is in the image?"</span>, base64_images<span class="op">=</span>[base64_image])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="49">
<p>This is the classic “Trolley Problem” ethical dilemma illustrated in a simple diagram. It shows a trolley/tram on tracks with two possible paths: one track leads to five people, while the other track has one person. There’s also a figure shown near a lever that could switch the trolley’s direction.</p>
</div>
</div>
<div id="cell-55" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"grok-vision-beta"</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>provider <span class="op">=</span> XAIProviderOpenAI(model_name<span class="op">=</span>model_name)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>chat_client <span class="op">=</span> ChatClient(provider<span class="op">=</span>provider)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>chat_client.prompt_model(<span class="st">"What is in the image?"</span>, base64_images<span class="op">=</span>[base64_image])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="50">
<p>The image depicts a simplified diagram of a train or tram on tracks with a switch mechanism. There are two tracks: one leading straight ahead and another branching off to the right.</p>
<ul>
<li>On the left side, there is a train or tram on the left track.</li>
<li>In the middle, there is a person standing near the switch mechanism.</li>
<li>On the right side, there are two groups of people:
<ul>
<li>One group is on the straight track ahead.</li>
<li>Another group is on the track that branches off to the right.</li>
</ul></li>
</ul>
<p>This diagram is often used to illustrate the trolley problem, a moral dilemma in ethics where one must decide whether to switch the train to another track to save a group of people at the cost of another individual.</p>
</div>
</div>
<div id="cell-56" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"llama-3.2-90b-vision-preview"</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>provider <span class="op">=</span> GroqProviderLlama3(model_name<span class="op">=</span>model_name)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>chat_client <span class="op">=</span> ChatClient(provider<span class="op">=</span>provider)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>chat_client.prompt_model(<span class="st">"What is in the image?"</span>, base64_images<span class="op">=</span>[base64_image])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="51">
<p>The image depicts a black-and-white graphic of a train and people. The train is on the left side of the image, with a diamond-shaped object above it. There are several lines extending from the train, each featuring a person icon. The lines vary in length, with some being longer than others. The background of the image is white.</p>
<p><strong>Key Elements:</strong></p>
<ul>
<li><strong>Train:</strong> A black train with a diamond-shaped object above it.</li>
<li><strong>Lines:</strong> Several lines extending from the train, each featuring a person icon.</li>
<li><strong>Person Icons:</strong> Small black figures representing people.</li>
<li><strong>Background:</strong> White background.</li>
</ul>
<p><strong>Interpretation:</strong></p>
<p>The image appears to be a simple graphic representation of a train and its passengers. The lines extending from the train may represent the paths or routes taken by the passengers. The person icons could symbolize the individuals traveling on the train. The diamond-shaped object above the train is unclear in its meaning but may represent a signal or a marker. Overall, the image conveys a sense of movement and transportation.</p>
</div>
</div>
</section>
<section id="observations" class="level4">
<h4 class="anchored" data-anchor-id="observations">Observations</h4>
<p>As you can see, it is very easy to benchmarking different models against the same input. In this test, GPT-4o, Claude Sonnet 3.5, and Grok correctly recognized the Trolley Problem. Llama 3.2, however, just gave a generic description of the scene, missing the high-level concept. This demonstrates how the OmniChatClient design allows you to efficiently test and compare multiple models in a unified way.</p>
</section>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Building the OmniChatClient has been an exciting project, creating a unified and extensible interface for working with multiple LLMs. By abstracting provider-specific functionality into modular components, we’ve reduced redundancy and created a scalable framework that can easily adapt to future requirements.</p>
<p>Dependency injection ensures that the OmniChatClient remains flexible and decoupled from provider-specific details, which are implemented in dedicated provider classes. Inheritance further reduces redundancies and accelerates the addition of new providers and features. As a result, any application using the OmniChatClient can seamlessly switch between models by simply swapping out the provider objects, no additional code changes are required.</p>
<p>While this implementation is already functional and supports both text and vision capabilities, there’s still room to expand. Enabling tool usage and adding support for providers like Google’s Gemini series are in my backlog.</p>
<p>On a broader level, I hope this hands-on demonstration of dependency injection and inheritance, applied in a practical example, inspires you to experiment with these patterns to make your code more robust, flexible, and elegant.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/chrwittm\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="chrwittm/chrwittm.github.io.comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
© <span id="y"></span> Christian Wittmann
<script>
  document.getElementById('y').textContent = new Date().getFullYear();
</script>
</div>   
    <div class="nav-footer-center">

<div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
<p><a href="../../impressum.html">Impressum</a> · <a href="../../privacy.html">Privacy</a></p>
</div>
  </div>
</footer>




</body></html>