{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f53a81bc",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Building a Harmonized-API 'Hello World' with AI Core\"\n",
    "author: \"Christian Wittmann\"\n",
    "date: \"2026-01-27\"\n",
    "categories: [sap, orchestration, llm]\n",
    "image: \"orchestration-hello-world.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368fe916",
   "metadata": {},
   "source": [
    "The [Harmonized API](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/harmonized-api) of the Orchestration Service is probably one of SAP's best kept secrets. It allows you to talk to [all the language models on AI core](https://me.sap.com/notes/3437766) in a harmonized format across model families. This means that you can simply swap out the model name from `gpt-4o` to `anthropic--claude-4.5-sonnet` without touching anything else of your code. This enables you to simply compare model performance or even build redundancies into your use cases.\n",
    "\n",
    "By the end of this tutorial (which is also available as a [Jupyter notebook on GitHub](https://github.com/chrwittm/sap-orchestration/blob/main/hello-word/hello-world.ipynb)), you'll have a working setup for talking to SAP's Harmonized API of the Orchestration Service using the Generative AI Hub SDK on AI Core. Honestly, the hardest part is just stating what we're using ðŸ˜‰, the actual content is rather simple ðŸ¤“. Let's cut through the jargon and build something cool.\n",
    "\n",
    "> Note: Throughout this blog post, I'll assume that you have access to a BTP subaccount with instances of the [AI Core service](https://discovery-center.cloud.sap/serviceCatalog/sap-ai-core) (extended plan) and the [AI Launchpad service](https://discovery-center.cloud.sap/serviceCatalog/sap-ai-launchpad) (standard plan)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f9762b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<style>\n",
    "  figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    text-align: center;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"orchestration-hello-world.png\" alt=\"SAP Orchestration Service\" style=\"width:50%;\">\n",
    "    <figcaption>SAP Orchestration Service</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff689e84",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before we can talk to the Orchestration Service, we need to install the [SAP Cloud SDK for AI](https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/README_sphynx.html#installation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b55d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"sap-ai-sdk-gen[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583befa1",
   "metadata": {},
   "source": [
    "Next we need to focus on authentication. As explained [in the docs](https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/README_sphynx.html#installation), there are multiple ways to do this.\n",
    "\n",
    "To keep this notebook-friendly with minimal setup, I will use a `.env`-file. You can extract all necessary values from the BTP service key of your AI Core Service Instance. \n",
    "\n",
    "Just create the `.env`-file in the same directory as the Jupyter notebook with the following format:\n",
    "\n",
    "```bash\n",
    "AICORE_AUTH_URL=https://********.authentication.********.hana.ondemand.com\n",
    "AICORE_CLIENT_ID=********\n",
    "AICORE_CLIENT_SECRET=********\n",
    "AICORE_RESOURCE_GROUP=********\n",
    "AICORE_BASE_URL=https://api.ai.********.hana.ondemand.com/v2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce4c44",
   "metadata": {},
   "source": [
    "All we need to do is load the `.env`. If you stick to the naming conventions, the SDK will automatically use it correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a19bc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Keeping rest of the cell for explicit checking, if you want to experiment\n",
    "\n",
    "# Access the variables\n",
    "#aicore_auth_url = os.getenv(\"AICORE_AUTH_URL\")\n",
    "#aicore_client_id = os.getenv(\"AICORE_CLIENT_ID\")\n",
    "#aicore_client_secret = os.getenv(\"AICORE_CLIENT_SECRET\")\n",
    "#aicore_resource_group = os.getenv(\"AICORE_RESOURCE_GROUP\")\n",
    "#aicore_base_url = os.getenv(\"AICORE_BASE_URL\")\n",
    "\n",
    "# Print them to check\n",
    "#print(f\"AICORE_AUTH_URL: {aicore_auth_url}\")\n",
    "#print(f\"AICORE_CLIENT_ID: {aicore_client_id}\")\n",
    "#print(f\"AICORE_CLIENT_SECRET: {aicore_client_secret}\")\n",
    "#print(f\"AICORE_RESOURCE_GROUP: {aicore_resource_group}\")\n",
    "#print(f\"AICORE_BASE_URL: {aicore_base_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d095b0",
   "metadata": {},
   "source": [
    "Once done, we can talk to the Orchestration Service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8276e57",
   "metadata": {},
   "source": [
    "## Building Hello World\n",
    "\n",
    "For all the elements of the API, the SAP Cloud SDK for AI has a dedicated class which abstracts the model specifics away.\n",
    "\n",
    "For example, the different message types used in LLM communication are represented by the classes `SystemMessage`, `UserMessage`, and `AssistantMessage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e58e4ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.message import SystemMessage, UserMessage\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(\"Act like the very first program of a coding tutorial.\"),\n",
    "    UserMessage(\"What do you respond upon execution?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af70cefb",
   "metadata": {},
   "source": [
    "The messages need to be wrapped in a template. Even if we could do a lot more with the template (placeholders, structured outputs, tool definitions), let's treat it as a simple wrapper for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1c43405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.template import Template\n",
    "\n",
    "template = Template(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686064d5",
   "metadata": {},
   "source": [
    "Even though the code is model-independent, we need to specify which model we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a5f5702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.llm import LLM\n",
    "\n",
    "llm = LLM(name=\"gpt-4o\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17ac5ab",
   "metadata": {},
   "source": [
    "When we combine the template and the LLM, this is called a configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13a3a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.config import OrchestrationConfig\n",
    "\n",
    "config = OrchestrationConfig(template=template, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35407c8b",
   "metadata": {},
   "source": [
    "Finally, we can pass the configuration to the orchestration service to send the prompt to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73077ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.service import OrchestrationService\n",
    "\n",
    "orchestration_service = OrchestrationService(config=config)\n",
    "result = orchestration_service.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b5d36",
   "metadata": {},
   "source": [
    "The result is a typical OpenAI-style nested object. Here's how we can extract the model response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "258f7d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "print(result.orchestration_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0bf80",
   "metadata": {},
   "source": [
    "We have successfully established the communication with the orchestration service ðŸŽ‰. Let's try swapping out the model next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2a554",
   "metadata": {},
   "source": [
    "## How to simply swap models\n",
    "\n",
    "Let's wrap our orchestration call in a helper function to easily compare models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d180db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_orchestration_service(system_prompt: str, user_prompt: str, model_name: str) -> str:\n",
    "    \"\"\"Simple wrapper to call the Orchestration Service.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(system_prompt),\n",
    "        UserMessage(user_prompt)\n",
    "    ]\n",
    "    \n",
    "    config = OrchestrationConfig(\n",
    "        template=Template(messages),\n",
    "        llm=LLM(name=model_name)\n",
    "    )\n",
    "    \n",
    "    result = OrchestrationService(config=config).run()\n",
    "    return result.orchestration_result.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b065674",
   "metadata": {},
   "source": [
    "Now let's ask three different models the same question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d47c95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Answer in a concise way.\"\n",
    "user_prompt = \"Who are you? Which model do you use?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f52af07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm an AI language model created by OpenAI, based on the GPT-4 architecture.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_orchestration_service(system_prompt, user_prompt, \"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bc99c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm an AI assistant created by Anthropic to be helpful, harmless, and honest. I don't have information about my specific model or training.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_orchestration_service(system_prompt, user_prompt, \"anthropic--claude-3.5-sonnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65b73d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am a large language model, trained by Google. I use Google's Gemini model family.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_orchestration_service(system_prompt, user_prompt, \"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d24f1",
   "metadata": {},
   "source": [
    "Notice how each model proudly announces its creator, yet our code didn't change at all. That's one of the key advantages of using the harmonized API compared to the model-specific chat completion API.\n",
    "\n",
    "Before we close this hello world example, one final question remains: Which models can you actually use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd78d142",
   "metadata": {},
   "source": [
    "## Available Models\n",
    "\n",
    "The easiest way to find out which models are supported is to simply read the [documentation](https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/README_sphynx.html#supported-models) - Who would have thought? ðŸ˜‰ Nonetheless, for the most up-to-date information you should check [note 3437766](https://me.sap.com/notes/3437766), which lists the availability of Generative AI Models.\n",
    "\n",
    "At the time of writing, Claude Opus 4.5 was not listed in the docs, but the note listed it as available and it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47670023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm Claude, an AI assistant made by Anthropic.\\n\\nI am the modelâ€”I'm Claude, specifically from Anthropic's Claude model family. I don't have access to my exact version number in this conversation, but I'm one of the Claude models (such as Claude 3.5 Sonnet, Claude 3 Opus, etc.).\\n\\nIs there something specific you'd like to know about my capabilities?\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_orchestration_service(system_prompt, user_prompt, \"anthropic--claude-4.5-opus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d568ad",
   "metadata": {},
   "source": [
    "When trying out different models via the harmonized API, it is important to note that you do not need a deployment in AI Core to access the model. This may sound surprising, but trust me, I didn't create a deployment for Claude Opus 4.5 in AI core, yet it works. This is one of the key benefits of the orchestration service with the harmonized API: SAP manages the model deployments centrally, so you can simply switch model names without provisioning anything yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea57b52e",
   "metadata": {},
   "source": [
    "As we can see, the documentation is sometimes lagging behind reality in the system. Therefore, I was curious if we could just ask AI Core which models are available, and here's what I found. Since there is no good property to filter on to only get the LLMs, I relied on the name and description to filter the list for non-deprecated LLMs. Just keep in mind that the following list is a snapshot and depending on when you run this, the list will look different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0b7f5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohere: cohere--command-a-reasoning\n",
      "Google: gemini-2.0-flash\n",
      "Google: gemini-2.0-flash-lite\n",
      "Google: gemini-2.5-pro\n",
      "Google: gemini-2.5-flash\n",
      "Google: gemini-2.5-flash-lite\n",
      "OpenAI: gpt-5\n",
      "OpenAI: gpt-5-nano\n",
      "OpenAI: gpt-5-mini\n",
      "OpenAI: gpt-4o\n",
      "OpenAI: gpt-4o-mini\n",
      "OpenAI: gpt-4.1\n",
      "OpenAI: gpt-4.1-nano\n",
      "OpenAI: gpt-4.1-mini\n",
      "OpenAI: o3-mini\n",
      "OpenAI: o3\n",
      "OpenAI: o4-mini\n",
      "Perplexity: sonar-pro\n",
      "Perplexity: sonar\n",
      "Mistral AI: mistralai--mistral-large-instruct\n",
      "Mistral AI: mistralai--mistral-small-instruct\n",
      "Mistral AI: mistralai--mistral-medium-instruct\n",
      "Amazon: amazon--nova-pro\n",
      "Amazon: amazon--nova-lite\n",
      "Amazon: amazon--nova-micro\n",
      "Anthropic: anthropic--claude-3-haiku\n",
      "Anthropic: anthropic--claude-3.5-sonnet\n",
      "Anthropic: anthropic--claude-3.7-sonnet\n",
      "Anthropic: anthropic--claude-4-sonnet\n",
      "Anthropic: anthropic--claude-4.5-sonnet\n",
      "Anthropic: anthropic--claude-4.5-opus\n",
      "Anthropic: anthropic--claude-4.5-haiku\n"
     ]
    }
   ],
   "source": [
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "\n",
    "client = AICoreV2Client.from_env()\n",
    "for m in client.model.query().resources:\n",
    "    # Filter out embedding models, rerankers, and deprecated models\n",
    "    name = m.model.lower()\n",
    "    desc = m.description.lower()\n",
    "    if any(x in name or x in desc for x in ['embed', 'rerank', 'sap-abap', 'sap-rpt', 'gpt-35']):\n",
    "        continue\n",
    "    print(f\"{m.provider}: {m.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b988ac2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "After installing the SAP Cloud SDK for AI and setting up authentication, we managed to talk to the Orchestration Service via the Harmonized API with just a few lines of code.\n",
    "\n",
    "The key advantage over the model-specific chat completion API is that you can swap models from different vendors(!) by simply changing a string, no code changes required. Whether it's `gpt-4o`, `anthropic--claude-3.5-sonnet`, or `gemini-2.5-flash`, the same code just works. This opens up easy benchmarking across model families, A/B testing, and even building redundancy into your applications.\n",
    "\n",
    "We also discovered another benefit: You don't need to manage deployments yourself. Instead, SAP handles all model deployments centrally. This makes life easy for you as a developer: No need to coordinate with your admin or wait for provisioning. You can experiment with new models the moment they're available. ðŸ¤“\n",
    "\n",
    "With this \"Hello World\", you now have working code to start experimenting with. Try it out for yourself. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
