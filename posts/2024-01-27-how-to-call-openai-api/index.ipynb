{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"How to call the OpenAI API from a Jupyter Notebook\"\n",
    "author: \"Christian Wittmann\"\n",
    "date: \"2024-01-27\"\n",
    "categories: [openai, jupyter, API]\n",
    "image: \"openai-api-512px.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring Large Language Models (LLMs) through their Web-based User Interfaces (WebUIs) is indeed insightful, particularly for experimenting with various prompt engineering techniques. However, accessing LLMs via their API unlocks a many additional possibilities. This approach not only allows you to craft your own applications but also enables the integration of LLMs into existing solutions. The use cases are endless: You can leverage LLMs for constructing comprehensive datasets, automating content creation, enhancing user interaction with natural language, personalizing user experiences, etc. The API access essentially opens doors to a more tailored LLM experience, more than just chatting with it.\n",
    "\n",
    "To demonstrate how to access the [OpenAI API for text generation](https://platform.openai.com/docs/guides/text-generation), I created a [Jupyter Notebook](https://github.com/chrwittm/lm-hackers/blob/main/10-open-ai-api/accessing-openai-api.ipynb) with all the steps from installing the necessary python packages, via managing access keys to calling the API with some examples. While you can perform all the steps in the [Jupyter Notebook](https://github.com/chrwittm/lm-hackers/blob/main/10-open-ai-api/accessing-openai-api.ipynb), in this blog post I would like to explore the concepts and take a look at what is between the lines of code, including my biggest learning: How does the chat with an LLM actually work.\n",
    "\n",
    "This is the first blog post of a series in which I am reworking the [hackers guide by Jeremy Howard](https://youtu.be/jkrNMKz9pWU?si=rh31HHhAnaKPZw1I) and the accompanying [notebook](https://github.com/fastai/lm-hackers/blob/main/lm-hackers.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before we can start calling the OpenAI API, we need to setup a few thing:\n",
    "\n",
    "- Installing python packages\n",
    "- Getting an API Key\n",
    "- Securely storing the API-key\n",
    "\n",
    "### Installation\n",
    "\n",
    "If you have not done so already, pip install the `openai` package:\n",
    "\n",
    "```bash\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "### Generate API key\n",
    "\n",
    "To be able to access the OpenAI API, you need an API access key. To obtain/generate the [API-key from the Open.AI Website](https://platform.openai.com/api-keys) as also [explained in the docs](https://platform.openai.com/docs/api-reference/introduction)\n",
    "\n",
    "### How to securely store your API Access Key\n",
    "\n",
    " Since you do not want to put your API key into a Jupyter notebook, it is recommended that you store the API-key in a your python environment using [python-dotenv](https://pypi.org/project/python-dotenv/).\n",
    "\n",
    "```bash\n",
    "pip install python-dotenv\n",
    "```\n",
    "\n",
    "Using dotenv, you store your API key in an environment file which you can easily access from within your Jupyter notebook. Here is a quick example, using an example file `foobar.env` which has the following content:\n",
    "\n",
    "```bash\n",
    "# Exapmple\n",
    "FOO=\"BAR\"\n",
    "```\n",
    "\n",
    "You can import the variables like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('FOO', 'BAR')])\n"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "foobar_config = dotenv_values(\"foobar.env\")\n",
    "print(foobar_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real life, the usage looks like this, leveraging the environment variables from the os package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAR\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"foobar.env\")  # This loads the .env file into the environment\n",
    "\n",
    "foo_env_value = os.getenv('FOO')\n",
    "print(foo_env_value)  # This will also print \"BAR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step to real life is not to use `foobar.env`, but `.env`. Therefore, you need to add the following section to your `.env`-file:\n",
    "\n",
    "```bash\n",
    "# Open AI\n",
    "OPENAI_API_KEY=\"My API Key\"\n",
    "```\n",
    "\n",
    "Once you load the `.env`-file, you are in business to call the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: Make sure, the `.env` file is not published to GitHub by including `*.env` in the `.gitignore`-file:\n",
    "\n",
    "```bash\n",
    "echo \".env\" >> .gitignore \n",
    "```\n",
    "\n",
    "afterwards:\n",
    "```bash\n",
    "git add .gitignore \n",
    "git commit -m \"Updated .gitignore to ignore .env files\"\n",
    "git push\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the API\n",
    "\n",
    "Since the time of publication of [Jeremy's hackers guide](https://github.com/fastai/lm-hackers/blob/main/lm-hackers.ipynb) the [Open.AI API](https://github.com/openai/openai-python/blob/main/README.md) had changed. Therefore, the original code needed from  some minor refactoring, essentially 2 thing:\n",
    "\n",
    "* Replace `ChatCompletion.create` with  `chat.completions.create`\n",
    "* Replace `c['choices'][0]['message']['content']` with `c.choices[0].message.content`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Money, mate, is like the fuel that powers your financial engine. It\\'s the cold, hard cash and digital digits you use to buy stuff, pay your bills, and live your life. It\\'s a medium of exchange that keeps the economic gears churning. Think of it as the \"dollarydoos\" that keep the economic barbie cookin\\'!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from openai import ChatCompletion,Completion\n",
    "from openai import chat\n",
    "\n",
    "aussie_sys = \"You are an Aussie LLM that uses Aussie slang and analogies whenever possible.\"\n",
    "\n",
    "#c = ChatCompletion.create(\n",
    "c = chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n",
    "              {\"role\": \"user\", \"content\": \"What is money?\"}])\n",
    "\n",
    "#c['choices'][0]['message']['content']\n",
    "c.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note for Enhanced Readability\n",
    "\n",
    "To improve the readability of of the model responses in the notebook, especially if it contains long lines of text or code, you may want to enable word wrap in your development environment.\n",
    "\n",
    "**For Visual Studio Code Users:**\n",
    "- Open the Command Palette (`Ctrl+Shift+P` or `Cmd+Shift+P`).\n",
    "- Search for `Preferences: Open Settings (JSON)` and select it.\n",
    "- Add `\"notebook.wordWrap\": \"on\"` to your settings.\n",
    "- Save the `settings.json` file.\n",
    "\n",
    "Enabling word wrap will make long lines of code or text wrap to the next line, fitting within the cell's width and eliminating the need for horizontal scrolling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnings\n",
    "\n",
    "My biggest take-away from having done this implementation is the realization how the chat with an LLM actually works. It is surprisingly simple, yet I had not realized this before: The chat with an LLM is stateless, which means that ChatGPT does not have a session open with you. Instead, the whole chat is passed to the model as context with every new prompt. This is the way the model knows what you have been talking about, and it can answer follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ah, glad you asked! Money, just like kangaroos, is all about value and trading, you see. Just as kangaroos hop around, money hops from one person to another in exchange for goods and services. It's the key to getting what you need and want in this modern world. Just like kangaroos in the outback, money roams around the economy, jumpin' here and there, makin' things happen. It's the backbone of our economic system, mate!\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n",
    "              {\"role\": \"user\", \"content\": \"What is money?\"},\n",
    "              {\"role\": \"assistant\", \"content\": \"Well, mate, money is like kangaroos actually.\"},\n",
    "              {\"role\": \"user\", \"content\": \"Really? In what way?\"}])\n",
    "\n",
    "c.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I had understood this, I started interacting with ChatGPT differently:\n",
    "\n",
    "- Instead of having long chats which drifted from topic to topic, I try to keep the chats more focused. If the topic changes too much, I open up a new chat.\n",
    "- I go back to prompts which did not yield the desired result more frequently, i.e. I edit the prompt instead of asking ChatGPT to correct something. This way you can keep undesired results out of the conversation which otherwise would be stuck in the conversation as context.\n",
    "\n",
    "Overall, I think the technical implementation was quite easy, and the docs nicely guided me to dive one level deeper than in Jeremy's original notebook. Learning more about the inner mechanics of how chatting with an LLM actually works, was the best part of this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
