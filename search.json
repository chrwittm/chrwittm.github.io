[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "‚ÄúThe Machine Learning Journey‚Äù ;)\nI am mainly writing for myself, trying to explain/clarify concepts to myself while writing,\ntherefore implementing the concept of ‚ÄúThe Best Way to Learn Is to Teach‚Äù.\nIf you stumbled across this blog and you even find it useful, that would make me happy :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christian‚Äôs Machine Learning Journey",
    "section": "",
    "text": "fast.ai\n\n\nkaggle\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblogging\n\n\nquarto\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nkaggle\n\n\nmnist\n\n\nfast.ai\n\n\nvision\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nkaggle\n\n\ntitanic\n\n\nfast.ai\n\n\nml\n\n\ntabular\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmath\n\n\npython\n\n\nnumpy\n\n\npytorch\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblogging\n\n\nquarto\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblogging\n\n\nquarto\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfast.ai\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfast.ai\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblogging\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-10-01-hello-world/index.html",
    "href": "posts/2022-10-01-hello-world/index.html",
    "title": "Hello World!",
    "section": "",
    "text": "Hello World!"
  },
  {
    "objectID": "posts/2022-10-03-bear-detector-2022/index.html",
    "href": "posts/2022-10-03-bear-detector-2022/index.html",
    "title": "Fast.AI with Bears, Cats and Dogs",
    "section": "",
    "text": "After having reworked lesson 2, here are my first trained models:\n\nBear Detector on HuggingFace\nBear Detector on GitHub Pages\nCat-vs-Dog-Classifier on HuggingFace\nCat-vs-Dog-Classifier on GitHub Pages\n\nFor the complete summary and the source code, check out my GitHub.\n\n\n\nBear Detector 2022"
  },
  {
    "objectID": "posts/2022-10-13-visualizing-gradient-descent-in-3d/index.html",
    "href": "posts/2022-10-13-visualizing-gradient-descent-in-3d/index.html",
    "title": "Visualizing Gradient Descent in 3D",
    "section": "",
    "text": "If you want to understand Machine Learning you have to understand gradient descent, we have all heard that before ;). Since I am a visual person, I tried to not only think through the concept, but also to visualize it.\nBased on Jeremy‚Äôs great notebook ‚ÄúHow does a neural net really work?‚Äù, I created a notebook which visualizes gradient descent in 3D. There are two version:"
  },
  {
    "objectID": "posts/2022-10-13-visualizing-gradient-descent-in-3d/index.html#the-backstory",
    "href": "posts/2022-10-13-visualizing-gradient-descent-in-3d/index.html#the-backstory",
    "title": "Visualizing Gradient Descent in 3D",
    "section": "The backstory",
    "text": "The backstory\nGradient descent is one of the topics of lesson 3 of the 2022-Fast.AI-Course. On a high level, it is pretty straight forward:\n\nCalculate the predictions and the loss (forward-pass)\nInitialize and calculate the gradients (i.e.¬†derivatives of the parameters, i.e.¬†how does changing the parameters change the loss) (backward-pass)\nUpdate the parameters (via the learning rate)\nRestart\n\nLooking at the python code, however, it is very compact, and a lot of magic is going on. Trying to unpack this and to get a solid and intuit understanding of gradient descent, I tried to not only think through the concept, but also to visualize it.\nI started playing with Jeremy‚Äôs notebook, and what started out as a rough idea turned into the notebooks on Kaggle and GitHub.\nI learned a lot about gradient descent and python (especially plotting) along the way, and I hope you find the visualizations useful."
  },
  {
    "objectID": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html",
    "href": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html",
    "title": "Creating a Blog Post using a Jupyter Notebook",
    "section": "",
    "text": "This is somehow a ‚ÄúHello World‚Äù-notebook, since its only purpose is to demonstrate how you can use a jupyter notebook to write a blog post using Quarto.\nSomehow I did not find the key ingredient in the Quarto docs, but in this blog post: To get the necessary header data into the jupyter notebook, you need to add a RAW-cell at the top which contains the metadata. This is how this cell looks like in this notebook (and here is an hello-world example):"
  },
  {
    "objectID": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html#jupyter-notebook-.ipynb-vs.-quarto-.qmd",
    "href": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html#jupyter-notebook-.ipynb-vs.-quarto-.qmd",
    "title": "Creating a Blog Post using a Jupyter Notebook",
    "section": "Jupyter Notebook (.ipynb) vs.¬†Quarto (.qmd)",
    "text": "Jupyter Notebook (.ipynb) vs.¬†Quarto (.qmd)\nFor my current use case of blogging I prefer the jupyter notebooks, and I will most likely write all future blog posts in jupyter notebooks because of the following:\n\nMy spell checking addon for VS Code does not support .qmd files.\nWith jupyter notebook there is no need to render the files, rendering is instant in jupyter notebook when you execute the cell.\nJupyter notebook is the same environment when I code, no need to adjust (even if only slightly)\n\nOf course, all of this is very personal and a current snapshot of preferences (as a beginner) - let‚Äôs see if this solidifies or changes."
  },
  {
    "objectID": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html#how-is-code-rendered",
    "href": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html#how-is-code-rendered",
    "title": "Creating a Blog Post using a Jupyter Notebook",
    "section": "How is code rendered?",
    "text": "How is code rendered?\nLet‚Äôs try out a little bit of code:\n\nHello World\n\nprint(\"Hello World!\")\n\nHello World!\n\n\n\n\nCalculations\n\na = 1\nb = 2\nc = a+b\nprint(c)\n\n3\n\n\n\n\nPlotting\n\nimport matplotlib.pyplot as plt\n    \nx = [1,2,3]\ny = [2,4,1]\n    \nplt.plot(x, y)\n    \nplt.xlabel('x - axis')\nplt.ylabel('y - axis')\nplt.title('Sample graph')\n    \nplt.show()"
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html",
    "title": "How I created this Blog",
    "section": "",
    "text": "Blogging is an essential part of the Fast.AI methodology, therefore, I decided to follow the advice and start my blog to document my ‚ÄúMachine Learning Journey‚Äù. Here are the steps it took to create this blog.\nThe previous goto solution ‚ÄúFastpages‚Äù has been depreciated in favor of Quarto. There is a good tutorial, but honestly I found it a bit intimidating because it is not a simple step-by-step guide, also the Creating a Blog page did no quite fit this category (for me). Therefore, without being an expert at this, let me share what I did to create this blog.\nSo if your goal is to create a simple blog based on Quarto, just hop on and follow along :).\nA little side-note: This is the fourth blog post I write with Quarto, and by now I feel that some rough edged have been removed, mostly because I realized that really all the blogging can be done in jupyter notebook! Therefore, learning, running my own experiments and blogging happen in the same environment and with a lot of reuse. Knowing that my more things can be done, the only goal of this blog post is to get you up and running with a basic setup and explain the possibility to blog via jupyter notebooks."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-1-create-a-new-github-repo",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-1-create-a-new-github-repo",
    "title": "How I created this Blog",
    "section": "Step 1: Create a new GitHub repo",
    "text": "Step 1: Create a new GitHub repo\nYour blog will reside in a GitHub repo, and it will leverage Github Pages. Follow steps 1 and 2 on the GitHub Pages homepage for the initial setup.\n\nNote: The default recommendation for the repo is <your username>.github.io. I took that recommendation, but anything else should work as well.\n\nAs a result you have an empty repo with just a readme.md file. Here‚Äôs how it still looks in my repo.\nFor cloning the repo to my local machine, I did:\ngit clone git@github.com:chrwittm/chrwittm.github.io.git\nTwo final activities are needed to finalize the setup of your repo:\n\nCreate a new branch called gh-pages. To do this, go to your branches (for me that is https://github.com/chrwittm/chrwittm.github.io/branches), and create the new branch by clicking the ‚ÄúNew branch‚Äù-button in the top right.\nSet the new branch as the branch for GitHub Pages. In your repo, navigate to Settings -> Pages. (In my repo that takes me to https://github.com/chrwittm/chrwittm.github.io/settings/pages.) Change main to gh-pages.\n\n\nFor more info on setting the branch, please refer to the Quarto Docs."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-2-install-quarto",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-2-install-quarto",
    "title": "How I created this Blog",
    "section": "Step 2: Install Quarto",
    "text": "Step 2: Install Quarto\nI am currently working on a Windows 10 machine and I usually work both in WSL (Ubuntu) (e.g.¬†for Jupyter and anything related to Fast.AI development) and in Windows with VS Code (e.g.¬†for writing this blog).\n\nInstalling Quarto in WSL\nFrom previous activities with nbdev, I already had Quarto installed. If I re-traced my steps correctly, here is what I did (as suggested here and here):\nmamba install -c fastchan nbdev\nnbdev_install_quarto\n\n\nInstalling Quarto for Windows (optional)\nOptional: Once I discovered that I can do everything in jupyter, I would label this step as optional, because I only used the Windows installation of Quarto to render previews of .qmd-files - which I do not need anymore when everything is done in jupyter notebooks.\nGo to this page, download and install Quarto.\n\n\nSetup Addons for VS Code (optional)\nOptional: Once I discovered that I can do everything in jupyter, I would label this step as optional, because I only used the Windows installation of Quarto to render previews of .qmd-files - which I do not need anymore when everything is done in jupyter notebooks.\nI also installed the Quarto extension for VS Code."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-3-initial-setup-to-publish-hello-world",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-3-initial-setup-to-publish-hello-world",
    "title": "How I created this Blog",
    "section": "Step 3: Initial setup to publish ‚ÄúHello World‚Äù",
    "text": "Step 3: Initial setup to publish ‚ÄúHello World‚Äù\nBy now we are really close to publishing the ‚ÄúHello World‚Äù-version of our blog: In the command line, go to the directory of your repo, and run the following commands, and the example content for the Quarto blog should be published to your repo.\nquarto create-project --type website:blog\nquarto publish gh-pages\nOnce done, you can open your blog at: <https://\"your username\".github.io/>\nFor some more background on what is happening with these two commands, please refer to this this page (choose ‚ÄúTerminal‚Äù) and this page."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-4-create-your-first-blog-post",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-4-create-your-first-blog-post",
    "title": "How I created this Blog",
    "section": "Step 4: Create your first Blog Post",
    "text": "Step 4: Create your first Blog Post\nNow it is time to create your first own blog post.\nIn the posts-directory, create a new folder, for example hello-world. Within this folder, create a notebook called index.ipynb. Add some hello-world content and a RAW-section as the first cell with this content (here is an example):\n---\ntitle: \"Hello World\"\nauthor: \"Your Name\"\ndate: \"2022-01-01\"\n---\nRepublish your blog:\nquarto publish gh-pages\nCongratulations, you just published your first blog post!\nFor a little more detailed version of the hello world blog post, please refer to my other hello world post."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-5-avoiding-disaster",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-5-avoiding-disaster",
    "title": "How I created this Blog",
    "section": "Step 5: Avoiding Disaster",
    "text": "Step 5: Avoiding Disaster\nWhen you run quarto publish gh-pages, your blog posts are rendered, and the rendered versions are pushed to git in branch gh-pages. Your actual notebooks are not uploaded to GitHub. Also any config you make to the blog etc. is uploaded in the rendered versions only. So if something were to happen to your local files, your work would be lost. (Such a disaster almost happened to me but the OneDrive file history saved me.)\nTherefore, I would recommend to also upload the ‚Äúsource‚Äù-files to GitHub (in the main branch):\ngit add posts/\ngit add _quarto.yml\ngit add about.qmd\ngit add index.qmd\ngit add profile.png\ngit add styles.css\ngit add .gitignore\ngit commit -m \"uploaded source files\"\ngit push\nAs a result, the source files are also stored on GitHub."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#steps-6-to-n-additional-setup",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#steps-6-to-n-additional-setup",
    "title": "How I created this Blog",
    "section": "Steps 6 to n: Additional setup",
    "text": "Steps 6 to n: Additional setup\nThere are many more things that can be done with the blog, but to keep things down to basics, let me just mention a few topics which will make the blog look like your own blog.\nAdditionally, let me mention one other blog post as a reference which I found only when looking into more detailed setup topics like comments and analytics. Albert Rapp‚Äôs blog post The ultimate guide to starting a Quarto blog truly is a great guide for setting up your Quarto blog.\n\nStep 6.1: Remove example content\nNow that the hello world blog post is published, you can remove the default content. I just turned the two example blog posts into drafts by adding the following line in their headers:\ndraft: true\n\n\nStep 6.2: Update remaining example content\nUpdate the following files and add/change the content, so that the blog looks like it is your blog:\n_quarto.yml\nabout.qmd\nindex.qmd"
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#conclusion",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#conclusion",
    "title": "How I created this Blog",
    "section": "Conclusion",
    "text": "Conclusion\nSetting up the blog was not really hard, but it took some time for me. Hopefully, this guide contains some shortcuts for you. Happy blogging!"
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html",
    "href": "posts/2022-10-28-matrix-multiplication/index.html",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "Since matrix multiplication is a big thing for deep learning and visualizations like http://matrixmultiplication.xyz/ were a bit to fast for me to properly re-understand what I learned in highschool, I decided dive in more systematically without falling into the trap of learning lots of math before continuing with deep learning. This will be short and sweet:\nThis has been done a million times before already, but nonetheless, let me explain what I learned along the way."
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#takeaways-from-khan-academy",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#takeaways-from-khan-academy",
    "title": "Matrix Multiplication",
    "section": "Takeaways from Khan Academy",
    "text": "Takeaways from Khan Academy\nOne thing I was struggling with was to intuit the dimensions of the target matrix.\nLet‚Äôs assume two matrixes: \\(A = (m \\times n)\\) and \\(B = (n \\times k)\\)\nThis picture from Khan Academy sums it all up for me:\n Illustration by Khan Academy CC BY-NC-SA 3.0 US  Note: All Khan Academy content is available for free at (www.khanacademy.org)‚Äú\nTherefore, a matrix multiplication is defined if the number of columns of matrix \\(A\\) matches the number of rows of matrix \\(B\\).\nThe resulting matrix \\(C = A \\times B\\) has the same number of rows as matrix \\(A\\) and the same number of columns as matrix \\(B\\)."
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#implementing-matrix-multiplication-in-python-from-scratch",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#implementing-matrix-multiplication-in-python-from-scratch",
    "title": "Matrix Multiplication",
    "section": "Implementing Matrix Multiplication in Python from scratch",
    "text": "Implementing Matrix Multiplication in Python from scratch\nOnce that was done, I decided to implement matrix multiplication in python. I found this tutorial which provided me with the task and some guidance along the way, especially on a few things in python.\nAs a starting point, here are 2 matrixes that we want to multiply (example from tutorial sightly adjusted):\n\nimport numpy as np\nnp.random.seed(27)\nA = np.random.randint(1,10,size = (4,3))\nB = np.random.randint(1,10,size = (3,2))\nprint(f\"Matrix A:\\n {A}\\n\")\nprint(f\"Matrix B:\\n {B}\\n\")\n\nMatrix A:\n [[4 9 9]\n [9 1 6]\n [9 2 3]\n [2 2 5]]\n\nMatrix B:\n [[7 4]\n [4 1]\n [6 4]]\n\n\n\nThis is the final result, we want to re-implement from scratch:\n\nA@B\n\narray([[118,  61],\n       [103,  61],\n       [ 89,  50],\n       [ 52,  30]])\n\n\n\nIndexing in Python\nMaybe this is too obvious for many, but I find it worth noting, that the sequence in which python addresses arrays (or tensors) is first by row, than by column. What do I mean by saying that?\nWhen you want to index into an array, you do this by array_name[row:column], for example A[1,2] return 6, it is the second line (which is index 1 when starting to count at 0), and the third column (which is index 1 when starting to count at 0):\n\nA[1,2]\n\n6\n\n\nIs there a way to not only remember this, but to also understand this? Yes, I think so: The most basic array (tensor) is a list (rank 1 tensor), which we can think of as one row of numbers. Therefore, the first index represents the row. You can think of a 2-dimensional array (a rank 2 tensor) as adding the columns to a row of numbers (by adding more rows), therefore the second index represents the columns. Hence to access an element in a 2D-array (rank-2 tensor), this is done by array_name[row:column].\nWhy do we think about indexing? First, to determine if a matrix multiplication is defined, we need to find the dimensions of the matrixes, and later on we need to access the matrix content for the calculation.\nTo access a complete row or column, we use:\n\nFor a row: array_name[row, : ] or the short form array_name[row]\nFor a column: array_name[ : ,column]\n\nThis means: We access a specific row or column by index, and from the other dimension, we access all elements. For example:\n\n# accessing the first row of matrix A\n\nA[0] #same as A[0,:]\n\narray([4, 9, 9])\n\n\n\n# accessing the first column of matrix B\n\nB[:,0]\n\narray([7, 4, 6])"
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#constructing-a-target-matrix-of-zeros",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#constructing-a-target-matrix-of-zeros",
    "title": "Matrix Multiplication",
    "section": "Constructing a target matrix of zeros",
    "text": "Constructing a target matrix of zeros\nThe \\(C\\) target matrix has the same number of rows as A and the same number of columns of B, so in our example that is a matrix with 4 rows and 2 columns:\n\nnp.zeros((4, 2), dtype = int)\n\narray([[0, 0],\n       [0, 0],\n       [0, 0],\n       [0, 0]])\n\n\nThe number of rows is the length of a column, therefore, to get the number of rows of matrix A, we can write:\n\nlen(A[:,0]) #i.e. the length of the first column\n\n4\n\n\nSimilarly, the number of elements in a row if the number of columns, Therefore, the number of columns of B is:\n\nlen(B[0]) #the number of entries in the first row\n\n2\n\n\nWhile to above is correct, there is a more elegant way to write this. Each array (tensor) has an attribute .shape which tells us how many rows and columns an array has (notice the sequence in the tuple: (row,column)):\n\nprint(A.shape)\nprint(B.shape)\n\n(4, 3)\n(3, 2)\n\n\nTherefore, we can re-write:\n\nprint(f'Number of rows in matrix A: {A.shape[0]}') \nprint(f'Number of columns in matrix B: {B.shape[1]}')\n\nNumber of rows in matrix A: 4\nNumber of columns in matrix B: 2\n\n\nNow we can generically construct the target matrix \\(C\\):\n\nC = np.zeros((A.shape[0], B.shape[1]), dtype = int)\nC.shape\n\n(4, 2)"
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#exercise-implement-matrix-multiplication-with-numpy-arrays",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#exercise-implement-matrix-multiplication-with-numpy-arrays",
    "title": "Matrix Multiplication",
    "section": "Exercise: Implement Matrix Multiplication with numpy arrays",
    "text": "Exercise: Implement Matrix Multiplication with numpy arrays\nImplement a function multiply_matrix(A,B) which does the following:\n\nAccept two matrices, A and B, as inputs.\nCheck if matrix multiplication between A and B is valid, if not raise an error.\nIf valid, multiply the two matrices A and B, and return the product matrix C.\n\n\ndef multiply_matrix(A,B):\n    \n    if A.shape[1] != B.shape[0]:\n        raise ValueError('Number of columns of A and number of rows of B do not match')\n    \n    C = np.zeros((A.shape[0], B.shape[1]), dtype=int)\n\n    for row in range(C.shape[0]):\n        for column in range(C.shape[1]):\n            for step in range(A.shape[1]):\n                C[row, column] += A[row, step] * B[step, column]\n    \n    return C\n\nC1 = multiply_matrix(A, B)\nC1\n\narray([[118,  61],\n       [103,  61],\n       [ 89,  50],\n       [ 52,  30]])\n\n\n\nC2 = A@B\nassert np.array_equal(C1, C2)"
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#exercise-implement-matrix-multiplication-with-tensors",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#exercise-implement-matrix-multiplication-with-tensors",
    "title": "Matrix Multiplication",
    "section": "Exercise: Implement Matrix Multiplication with tensors",
    "text": "Exercise: Implement Matrix Multiplication with tensors\nJust for the fun of it, let‚Äôs re-implement the same with pytorch tensors. It turns out it same, same, but a little different:\n\nimport torch\n\ntorch.manual_seed(27) #https://pytorch.org/docs/stable/notes/randomness.html\nX = torch.randint(1,10,size = (4,3)) #https://pytorch.org/docs/stable/generated/torch.randint.html\nY = torch.randint(1,10,size = (3,2))\nprint(f\"Matrix X:\\n {X}\\n\")\nprint(f\"Matrix Y:\\n {Y}\\n\")\n\nMatrix X:\n tensor([[1, 1, 5],\n        [8, 8, 6],\n        [1, 7, 1],\n        [4, 4, 1]])\n\nMatrix Y:\n tensor([[7, 6],\n        [3, 7],\n        [9, 5]])\n\n\n\n\nZ = torch.zeros((4, 2), dtype = int)\nZ\n\ntensor([[0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0]])\n\n\n\nZ.shape\n\ntorch.Size([4, 2])\n\n\n\ndef multiply_matrix_torch(A,B):\n    \n    if A.shape[1] != B.shape[0]:\n        raise ValueError('Number of columns of A and number of rows of B do not match')\n    \n    C = torch.zeros((A.shape[0], B.shape[1]), dtype=int)\n\n    for row in range(C.shape[0]):\n        for column in range(C.shape[1]):\n            for step in range(A.shape[1]):\n                C[row, column] += A[row, step] * B[step, column]\n    \n    return C\n\nZ1 = multiply_matrix_torch(X, Y)\nZ1\n\ntensor([[ 55,  38],\n        [134, 134],\n        [ 37,  60],\n        [ 49,  57]])\n\n\n\nZ2 = X@Y\nassert torch.equal(Z1, Z2) == True\n\nThat concludes the ‚Äúexploration‚Äù of matrix multiplication, I learned a lot along the way :)."
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html",
    "href": "posts/2022-11-05-kaggle-titanic/index.html",
    "title": "My First Kaggle Competition: Titanic",
    "section": "",
    "text": "For more practical experience with gradient descent, I decided to participate in the Titanic Competition. Here is how I did it and what I learned.\nI took the following approach:"
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html#installing-kaggle",
    "href": "posts/2022-11-05-kaggle-titanic/index.html#installing-kaggle",
    "title": "My First Kaggle Competition: Titanic",
    "section": "Installing Kaggle",
    "text": "Installing Kaggle\nGetting ready for the Kaggle competition requires registering for the competition (a few clicks on the kaggle website), and installing kaggle on your local machine. The following is based on the Live-Coding Session 7 and the related official topic in the forums.\nThe first step is to install kaggle:\npip install --user kaggle\nAs a result, the following warning is displayed: The script kaggle is installed in '/home/<your user>/.local/bin' which is not on PATH. This means that the you need to add the path to the PATH-variable. This is done by adding the following line to the .bashrc-file and restarting the terminal:\nPATH=~/.local/bin:$PATH\n\nNote: To display the current PATH-variable use: echo $PATH\n\nAs a result, typing the kaggle-command on the command line works, but the next error shows up (as expected): OSError: Could not find kaggle.json. Make sure it's located in /home/chrwittm/.kaggle. Or use the environment method.\nThis means that you cannot authorize against the kaggle platform. To solve this, download your personal kaggle.json On the kaggle website, navigate to: ‚ÄúAccount‚Äù and click on ‚ÄúCreate New API Token‚Äù. As a result, the kaggle.json is downloaded.\nCopy the kaggle.json-file into the .kaggle-directory in your home directory.\nTyping the kaggle-command on the command line gives you the final clue as to what is missing: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/chrwittm/.kaggle/kaggle.json'\nTherefore, type:\nchmod 600 /home/<your user>/.kaggle/kaggle.json\nTyping the kaggle-command on the command line again confirms: We are in business :)"
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html#downloading-the-dataset",
    "href": "posts/2022-11-05-kaggle-titanic/index.html#downloading-the-dataset",
    "title": "My First Kaggle Competition: Titanic",
    "section": "Downloading the dataset",
    "text": "Downloading the dataset\nTo download the dataset, run the following command (which you can also find on the kaggle website):\nkaggle competitions download -c titanic\nAs a result, the file titanic.zip is downloaded.\nTo unzip type:\nunzip titanic.zip\nDoing this for the first time, this resulted in an error: /bin/bash: unzip: command not found\nTo install zip and unzip, type:\nsudo apt-get install zip\nsudo apt-get install unzip\nAs a result, unzipping works, and we have a dataset to work with :).\n\nimport pandas as pd\n\ntrain = pd.read_csv(\"train.csv\")\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S"
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html#implementing-a-fast.ai-tabular-learner",
    "href": "posts/2022-11-05-kaggle-titanic/index.html#implementing-a-fast.ai-tabular-learner",
    "title": "My First Kaggle Competition: Titanic",
    "section": "Implementing a Fast.ai Tabular Learner",
    "text": "Implementing a Fast.ai Tabular Learner\nThe goal was not to create a perfect submission, but to simply train a model as fast as possible to\n\nget a baseline\nto get to know how a kaggle competition works (remember, this is my first one)\n\nTherefore, I created a dataloaders as shown in lesson 1 or in the docs by sorting the variables into categorical or continuos one, excluding irrelevant ones).\n\nNote 1: In this blog post, I am presenting the steps in a fast-forward way, here is the original notebook.\n\n\nNote 2: When writing this up, I was not able to 100% re-produce the same results, but basically this is how the story went.\n\n\nfrom fastai.tabular.all import *\n\npath = \".\"\n\ndls = TabularDataLoaders.from_csv('train.csv', path=path, y_names=\"Survived\",\n    cat_names = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked'],\n    cont_names = ['Age', 'Fare'],\n    procs = [Categorify, FillMissing, Normalize])\n\nNow we can train a model:\n\nlearn = tabular_learner(dls, metrics=accuracy)\nlearn.fit_one_cycle(10) #change this variable for more/less training\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.548652\n      0.315984\n      0.640449\n      00:00\n    \n    \n      1\n      0.454461\n      0.325496\n      0.640449\n      00:00\n    \n    \n      2\n      0.373511\n      0.289948\n      0.640449\n      00:00\n    \n    \n      3\n      0.319270\n      0.251090\n      0.640449\n      00:00\n    \n    \n      4\n      0.280473\n      0.196879\n      0.640449\n      00:00\n    \n    \n      5\n      0.249269\n      0.173640\n      0.640449\n      00:00\n    \n    \n      6\n      0.225535\n      0.152192\n      0.640449\n      00:00\n    \n    \n      7\n      0.207350\n      0.141283\n      0.640449\n      00:00\n    \n    \n      8\n      0.192223\n      0.137462\n      0.640449\n      00:00\n    \n    \n      9\n      0.180697\n      0.137344\n      0.640449\n      00:00\n    \n  \n\n\n\nWith this learner, we can make the predictions on the test-dataset.\n\ntest = pd.read_csv(\"test.csv\")\n\n# replacing null values with 0\ntest['Fare'] = test['Fare'].fillna(0)\n\n# create Predictions as suggested here:\n# https://forums.fast.ai/t/tabular-learner-prediction-using-data-frame/90534/2\ntest_dl = learn.dls.test_dl(test)\npreds, _ = learn.get_preds(dl=test_dl)\n\ntest['Survived_pred'] = preds.squeeze()\ntest.head()\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n      Survived_pred\n    \n  \n  \n    \n      0\n      892\n      3\n      Kelly, Mr. James\n      male\n      34.5\n      0\n      0\n      330911\n      7.8292\n      NaN\n      Q\n      0.064765\n    \n    \n      1\n      893\n      3\n      Wilkes, Mrs. James (Ellen Needs)\n      female\n      47.0\n      1\n      0\n      363272\n      7.0000\n      NaN\n      S\n      0.454887\n    \n    \n      2\n      894\n      2\n      Myles, Mr. Thomas Francis\n      male\n      62.0\n      0\n      0\n      240276\n      9.6875\n      NaN\n      Q\n      -0.025921\n    \n    \n      3\n      895\n      3\n      Wirz, Mr. Albert\n      male\n      27.0\n      0\n      0\n      315154\n      8.6625\n      NaN\n      S\n      -0.015690\n    \n    \n      4\n      896\n      3\n      Hirvonen, Mrs. Alexander (Helga E Lindqvist)\n      female\n      22.0\n      1\n      1\n      3101298\n      12.2875\n      NaN\n      S\n      0.508172\n    \n  \n\n\n\n\nInterpreting the values in column Survived_pred is important, because we need to turn these values into 0 and 1 for the submission. The submission file should only have the columns PassengerId and Survived. For the first submission, I did not worry about it too much and simply picked a value 0.5. (Let‚Äôs come back to that a little later)\n\nthreshold = 0.5 #change this variable for more/less training\ntest['Survived'] = [ 1 if element > threshold else 0 for element in preds.squeeze()]\n\nsubmission1 = test[['PassengerId', 'Survived']]\nsubmission1.to_csv('submission1.csv', index=False)\n\nI uploaded the results, and they were better then random ;) - Score 0.73923\n\nThe score is not great, but the whole point was to get a baseline as quickly as possible, and to ‚Äúplay the whole kaggle game‚Äù. Actually, the fact that I produced this result in about 1-2 hours felt pretty good :).\n\nNote: Running this notebook, I got a score of 0.75119, I am not sure, what caused the difference‚Ä¶ but better is always good ;)\n\nSo how can we improve the score? More training, interpreting the results differently? As it turns out: Both.\nLet‚Äôs look at the distribution of Survived_pred:\n\ntest.Survived_pred.hist();\n\n\n\n\nAs it turned out, setting my threshold to 0.6 created a better result: Score: 0.74162. (this I could not reproduce with this notebook while writing up the blog post)\nAlso more training, produced better results, running for 50 cycles, resulted in a lower loss and a better result. Training with 50 cycles and threshold 0.7, this was the result: Score: 0.76794 (with this notebook 0.77033)\nSo there is some randomness when training, and it is important to properly interpret the results. Getting about 77% right with this simple approach is not to bad."
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html#re-implementing-the-excel-model",
    "href": "posts/2022-11-05-kaggle-titanic/index.html#re-implementing-the-excel-model",
    "title": "My First Kaggle Competition: Titanic",
    "section": "Re-Implementing the Excel Model",
    "text": "Re-Implementing the Excel Model\nAfter the quick win with Fast.AI, I decided to re-implement what Jeremy did in the Excel in video lecture 3 to predict the survivors. Let‚Äôs see how it performs against the Fast.AI tabular learner.\nSince that involved quite a bit of code, let me simply link to notebook and discuss the learnings / results.\nAs it turned out:\n\nI had to do a bit of data cleansing.\nThe feature engineering took some time which taught me some general python lessons.\nImplementing the optimizer was a nice exercise, revisiting gradient descent and matrix multiplication, and doing some hands-on work with tensors.\n\nThe first model with just one layer scored 0.75837, even better than the my Fast.AI baseline, but not quite as good as the optimized version.\nThe next iteration with 2 and 3 layers scored better:\n\nScore: 0.77033 (2-layers)\nScore: 0.77272 (3-layers)\n\n\nThis was quite surprising: The self-written algorithm is better than the Fast.AI one, any ideas why that would be?\nNonetheless, it seems to hit a ceiling at 77%, and it would make sense to dive deeper into tabular data, but that is for another time. My goal was not to optimize the competition result, but to participate in my first kaggle competition, and to re-visit the topic of gradient descent and matrix multiplication. I will most likely return to this dataset/challenge in the future."
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html",
    "href": "posts/2022-11-26-mnist/index.html",
    "title": "MNIST, the ‚ÄòHello World‚Äô of Computer Vision",
    "section": "",
    "text": "After Cat vs.¬†Dog, this is the next challenge for me in computer vision: Building on chapter 4 of the book, I challenged myself to implement a model based on the MNIST dataset, as recommended as further research.\nThis challenge is also available as a Kaggle competition, and I found this bit of competitive spirit to add some spice to the project. Additionally, it broadened the spectrum of implementation topics, because training a model is one thing, but using it for meaningful predictions in equally important and also required some effort. Last, but not least, submitting the results was a nice way to check if the results are actually correct. As predicted: ‚ÄúThis was a significant project and took you quite a bit of time to complete! I needed to do some of my own research to figure out how to overcome some obstacles on the way‚Äù.\nI took an iterative approach, following a similar path as for working on the Titanic-Challenge:\nIt was a challenging project, and I learned a lot on the way. Below are some key points and learnings."
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html#the-fast.ai-version-without-a-submission",
    "href": "posts/2022-11-26-mnist/index.html#the-fast.ai-version-without-a-submission",
    "title": "MNIST, the ‚ÄòHello World‚Äô of Computer Vision",
    "section": "The Fast.AI version without a submission",
    "text": "The Fast.AI version without a submission\nBy now this is pretty straight-forward for me. I just copy&pasted a few lines of code to do the training, and I was able to create a decent model very quickly in this notebook.\nThe catch with this version is, however, that it is not ready for the mass data load: 28.000 predictions need to be done in the competition - something which I addressed in my second iteration.\nAdditionally, I found it interesting that the MNIST dataset was already pushing the limits of my laptop: The training time of about 40 minutes was ok, but it is already quite a burden if it needs to be done multiple times. Moving the learning to Paperspace, training on a free GPU, was 10x faster (no surprise). Since I like to still have everything locally, it is quite convenient moving files back and forth via git, also for the .pkl-files. This way the training can be done with GPU, and the inference can be done locally. Interestingly, in all my other notebooks, local performance was not an issue. (But I expect that to change in future other projects)"
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html#resubmitting-working-with-the-csv-files",
    "href": "posts/2022-11-26-mnist/index.html#resubmitting-working-with-the-csv-files",
    "title": "MNIST, the ‚ÄòHello World‚Äô of Computer Vision",
    "section": "Resubmitting: Working with the csv-files",
    "text": "Resubmitting: Working with the csv-files\nI found not very elegant to just convert the csv-files to png-images. That seems convenient, but a bit wasteful. Therefore, I re-implemented the process this notebook.\nIt was surprisingly difficult to convert the data into the right formal in memory so that the learner would accept the image. But finally I was able to convert a PIL.Image.Image to fastai.vision.core.PILImage. As usual with these things, once it was done, it looks easy.\nNot surprisingly, but a nice way to verify the result, the submission score was the same:"
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html#my-first-submission",
    "href": "posts/2022-11-26-mnist/index.html#my-first-submission",
    "title": "MNIST, the ‚ÄòHello World‚Äô of Computer Vision",
    "section": "My first submission",
    "text": "My first submission\nWhen I first downloaded the Kaggle data, I was quite surprised to see that the download did not contain any image files, but just 2 large csv-files. Since I only knew how to handle images, I simply converted the data to png-images in this notebook.\nOnce that was done, I could take the model trained before in my first notebook to make my first submission. In this notebook, I took the converted images and collected the predictions. I found the result of 99.4% quite impressive."
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html#the-from-scratch-version",
    "href": "posts/2022-11-26-mnist/index.html#the-from-scratch-version",
    "title": "MNIST, the ‚ÄòHello World‚Äô of Computer Vision",
    "section": "The from-scratch version",
    "text": "The from-scratch version\nDoing it all from scratch was an interesting learning exercise because I think that I already had a good understanding of what needed to be done even before implementing it. But, as it turns out, there is this tremendous difference between thinking that you understood it, and actually implementing it. There is a lot of fine print, and you have to pay attention to the details: Formatting the data, getting it into the correctly shaped tensors, and implementing the gradient descent. Irrespective of what I had learned/understood before, this has greatly deepened and solidified by implementing the MNIST challenge.\nSome minor mysteries remain, which I also documented in the notebook, if you can guide me how to fix them, please let me know.\nThe finale result of my from scratch-version is not up to the first implementation with resnet18, but I am proud of it for other reasons ;)."
  },
  {
    "objectID": "posts/2022-11-27-rebuilding-quarto-blog/index.html",
    "href": "posts/2022-11-27-rebuilding-quarto-blog/index.html",
    "title": "When disaster strikes: Re-building a Quarto Blog",
    "section": "",
    "text": "For the last 2 months I have been a proud writer of this blog, until yesterday disaster struck: Upon publishing of my MNIST-blog post via the usual quarto publish gh-pages, I received the following error message üò®:\nThe publishing was not completed, and my blog only showed a naked header, but no posts any more, essentially everything was gone üò± (at least online)\nTrying to google a quick fix did not reveal any real result. Due to lack of time, I had to (officially) stop for the day, but back in my mind, this was really nagging me‚Ä¶"
  },
  {
    "objectID": "posts/2022-11-27-rebuilding-quarto-blog/index.html#what-is-needed-to-re-build-a-quarto-blog",
    "href": "posts/2022-11-27-rebuilding-quarto-blog/index.html#what-is-needed-to-re-build-a-quarto-blog",
    "title": "When disaster strikes: Re-building a Quarto Blog",
    "section": "What is needed to Re-build a Quarto Blog?",
    "text": "What is needed to Re-build a Quarto Blog?\nAs I kept thinking about this, the error message clearly pointed to something on my local machine. Additionally, I previously posted on how to avoid disaster, so what would be the best way to re-build everything?\nBut let‚Äôs think through the matter: What do you actually need to re-build a Quarto blog? You only need your posts-directory and a few other files (the ones mentioned here).\nThis is important, so let me re-phrase this: On your GitHub repo, there are (should be) 2 versions of your blog:\n\nIn the main-branch, you store your source-files:\n\nThe Jupyter notebooks, markdown files, some pictures used within your posts\nThe configuration files: Some .yml-, .qmd- and .css-files\n\nThe gh-pages branch gets generated to contain the rendered versions of your posts: When you check out your _site directory, it contains a file structure similar to the posts-directory in your main-branch, but the _site directory deals in html-, xml-, and json-files.\n\nTherefore, to re-build your site, as far as I understand it, you only need the content of your main branch, and the content of the gh-pages gets generated once you run quarto publish gh-pages."
  },
  {
    "objectID": "posts/2022-11-27-rebuilding-quarto-blog/index.html#re-building-my-quarto-blog",
    "href": "posts/2022-11-27-rebuilding-quarto-blog/index.html#re-building-my-quarto-blog",
    "title": "When disaster strikes: Re-building a Quarto Blog",
    "section": "Re-building my Quarto Blog",
    "text": "Re-building my Quarto Blog\nWith the above in mind, here is what I did:\n\nI moved my local copy of the blog‚Äôs repo‚Äôs main branch to my temp folder (the milder version of deleting it)\nI re-cloned the repo (the main branch): git clone git@github.com:chrwittm/chrwittm.github.io.git\nI re-published the blog: quarto publish gh-pages\n\nAnd viol√†: My blog was back online. üòÉ"
  },
  {
    "objectID": "posts/2022-11-27-rebuilding-quarto-blog/index.html#conclusion",
    "href": "posts/2022-11-27-rebuilding-quarto-blog/index.html#conclusion",
    "title": "When disaster strikes: Re-building a Quarto Blog",
    "section": "Conclusion",
    "text": "Conclusion\nFirst of all: With these kind of seeming disasters: Sit back and relax: Is this a big deal? Well my blog was down, who cared? Probably I cared the most about it - my ego üòâ.\nDon‚Äôt panic: Think through a problem: What could be the root cause, from what angle can you approach the problem?\nI think, the solution is actually really nice. The setup with the 2 branches has some nice redundancy built-in, and without having tried it before, the disaster recovery performed very well.\nThe solution also reminded me of the fast-setup approach, which Jeremy discussed in the Live-Coding session 1: You should spend time using your tools, not configuring them. Quarto, at least to me, nicely proved that point that even if something goes wrong, you can quickly recover.\nHappy blogging!"
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html",
    "title": "Wrapping-up Lesson 3",
    "section": "",
    "text": "Lesson 3 took me a while to rework, because it churned out quite a few interesting projects:\nAlongside these 3 main activities, I also started a blog on my machine learning journey, which is based on Quarto.\nLet me summarize what I have done and learned."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html#gradient-descent",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html#gradient-descent",
    "title": "Wrapping-up Lesson 3",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nThe main focus of lesson 3 for me was/is gradient descent, which I found pretty easy to understand on a high level:\n\nCalculate the predictions and the loss (forward-pass)\nCalculate the derivatives of the parameters (i.e.¬†how does changing the parameters change the loss) (backward-pass)\nUpdate the parameters (via the learning rate)\nDon‚Äôt forgot to initialize the gradients\nRestart\n\nHowever, in the actual implementation and its simplicity, there is a lot of magic, which I tried to unpack for myself. Working through Jeremy‚Äôs notebook ‚ÄúHow does a neural net really work?‚Äù, I tried to not only think through the concept, but also to visualize it. The result is available as\n\na blog post\na forum post\na Kaggle notebook in which you can easily also play with the visualizations interactively (by copying and running the notebook)\na GitHub notebook\n\nI was excited and honored to read that Ben, a fellow Fast.AI student, created even better visualizations building on my work. I highly recommend playing with it and also checking out his other projects.\nWhile I truly love the Fast.AI content, I also need to mention the great video ‚ÄúThe spelled-out intro to neural networks and backpropagation: building micrograd‚Äù from Andrej Karpathy, which dives at least one level deeper. If there is one key takeaway from this video, it is this one: ‚ÄúA single gradient tells us the effect changing a parameter has on the loss‚Äù. This insight is powerful, and somehow it tends to get lost from my point of view, because you either have a very complex model with many, many parameters so that this is difficult to grasp, or you have a seemingly simple model in which you mix up the slope of the quadratic with a gradient. Implementing the visualization of gradient descent was also about building the intuition of what is actually going on under to hood."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html#the-titanic-competition",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html#the-titanic-competition",
    "title": "Wrapping-up Lesson 3",
    "section": "The Titanic Competition",
    "text": "The Titanic Competition\nInspired by the Excel-based version of the Titanic Competition, I decided to enter the kaggle competition. As with many good project, this resulted in a few other mini-projects:\n\nThe logistics on how a kaggle competition actually works, which included installing kaggle on my local machine. The Live-Coding Sessions of the 2022 Fast.AI course are probably quite underrated (at least looking at the number of views they get on Youtube). I find them a great addition to the official lessons because the tackle side problems like installing kaggle (in Live-Coding Session 7 and the related official topic in the forums) which otherwise would set you back some hours (or more). A big shout-out for these sessions!\nRevisiting matrix multiplication. Apart from the math, this also was about some python basics for me. While the result of implementing matrix multiplication from scratch has probably been done a million times, it still taught me some valuable lessons.\n\nIn the actual Titanic Competition, I did not focus too much on submitting a perfect result, but I rather aimed at re-visiting/solidifying the topic of gradient descent by replicating the actual lesson content. I built for following 2 notebooks\n\nThe first one uses a Fast.AI tabular learner to create a baseline while getting the know the data.\nNext, I re-implemented the Excel-based version from the video in python in this notebook.\n\nWhile my final high score of 77.2% is far away from perfect, I decided to come back to this competition another time, focusing more on the content, not just on gradient descent (like this time)."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html#mnist-the-hello-world-of-computer-vision",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html#mnist-the-hello-world-of-computer-vision",
    "title": "Wrapping-up Lesson 3",
    "section": "MNIST, the ‚ÄòHello World‚Äô of Computer Vision",
    "text": "MNIST, the ‚ÄòHello World‚Äô of Computer Vision\nAs Jeremy points out at the end of lesson 3, this lesson corresponds to chapter 4 of the book. Indeed, it covers very similar topics, but the example used is the light version of the MNIST dataset (which only contains 3s and 7s). Following the recommendation for further research, I implemented a model for the complete MNIST dataset. As predicted: ‚ÄúThis was a significant project and took you quite a bit of time to complete! I needed to do some of my own research to figure out how to overcome some obstacles on the way‚Äù.\nAfter all the previous activities around gradient descent, the actual mechanics of what needed to be done were not too difficult. Nonetheless, I found the competition to be hard, because of the actual technicalities of the python implementation. Put differently, I think I could have easily written a good specification on how to solve the MNIST competition, but actually doing it yourself is a different thing.\nSeemingly simple tasks like converting the csv-files to images, converting a PIL image to a Fast.AI PIL image, or getting the tensors in the right shape took me some time to implement in python. I am still struggling with python as a language but I am seeing good progress, and the only way to improve is to keep coding."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html#wrapping-up-lesson-3",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html#wrapping-up-lesson-3",
    "title": "Wrapping-up Lesson 3",
    "section": "Wrapping-up Lesson 3",
    "text": "Wrapping-up Lesson 3\nWhile I could improve the results of my projects, both for Titanic and MNIST, it feels like it would be some way over-optimizing. I did not enter the competitions to win, but to learn about gradient descent. Having spent the last 8 weeks with my lesson 3-projects (and allowing myself to get somewhat side-tracked), I feel it is time to move on to the next lesson. I am looking forward to the next challenging projects!"
  }
]