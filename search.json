[
  {
    "objectID": "privacy.html",
    "href": "privacy.html",
    "title": "Datenschutz · Privacy Policy",
    "section": "",
    "text": "Hinweis / Note: Deutsch ist die rechtlich maßgebliche Fassung. English version below is for convenience."
  },
  {
    "objectID": "privacy.html#deutsch",
    "href": "privacy.html#deutsch",
    "title": "Datenschutz · Privacy Policy",
    "section": "Deutsch",
    "text": "Deutsch\n\nVerantwortlicher\nChristian Wittmann\nEssentalstraße 4, 89584 Ehingen (Donau), Deutschland\nE-Mail: impressum [dot] vibes099 [at] aleeas [dot] com\n\n\nZweck der Website\nPersönliches, nicht-kommerzielles Lern- und Notizblog („Machine Learning Journey“).\n\n\nHosting (GitHub Pages)\nDiese Website wird von GitHub Pages ausgeliefert. Dabei verarbeitet GitHub u. a. technische Protokolldaten (z. B. IP-Adresse, Zeitstempel), um die Website bereitzustellen und abzusichern. Weitere Informationen entnehmen Sie bitte den Datenschutzhinweisen von GitHub.\nRechtsgrundlage: Art. 6 Abs. 1 lit. f DSGVO (berechtigtes Interesse an sicherer, effizienter Bereitstellung).\n\n\nCookies, Einwilligung & Consent-Management\nStandardmäßig werden nur technisch notwendige Elemente geladen. Analyse-Dienste werden erst nach Ihrer Einwilligung über das Banner aktiviert. Sie können Ihre Entscheidung jederzeit über die Banner-Einstellungen ändern.\nRechtsgrundlage für Analyse: Art. 6 Abs. 1 lit. a DSGVO (Einwilligung).\nRechtsgrundlage für notwendige Verarbeitung (z. B. Auslieferung/Logs): Art. 6 Abs. 1 lit. f DSGVO.\n\n\nWebanalyse (Google Analytics 4)\nIch setze Google Analytics 4 (GA4) ein, um Seitenaufrufe und Interaktionen zu messen. GA4 speichert nach Google-Angaben keine IP-Adressen und bietet Datenschutz-Einstellungen für die EU. GA4 wird nur geladen, wenn Sie im Cookie-Banner zustimmen. Sie können die Einwilligung jederzeit widerrufen.\nVerarbeitete Daten (Beispiele): Seitenaufrufe, ungefähre Geräte-/Browser-Informationen, Interaktionsdaten.\nEmpfänger: Google Ireland Limited (und ggf. verbundene Unternehmen).\nDrittlandübermittlung: Eine Übermittlung in Drittländer kann erfolgen; Google stellt hierfür Sicherungsmechanismen bereit.\nSpeicherdauer: gemäß GA4-Konfiguration (aktuell i. d. R. 2–14 Monate, je nach Einstellung).\n\n\nKommentare (Utterances / GitHub Issues)\nFür Kommentare nutze ich Utterances. Kommentare werden als GitHub-Issues in einem öffentlichen Repository gespeichert. Zum Kommentieren ist eine Anmeldung mit einem GitHub-Konto erforderlich; GitHub verarbeitet dabei Ihre personenbezogenen Daten gemäß deren Richtlinien. Ihr Kommentar und Ihr GitHub-Nutzername sind öffentlich einsehbar.\nRechtsgrundlage: Art. 6 Abs. 1 lit. a DSGVO (Ihre freiwillige Nutzung/Einwilligung).\n\n\nEinbindung externer Inhalte\nDerzeit werden keine externen Inhalte eingebettet, die personenbezogene Daten ohne Interaktion übertragen. Links zu Drittseiten führen erst beim Anklicken zu einer Datenübertragung an diese Anbieter.\n\n\nEmpfänger / Kategorien von Empfängern\n\nHosting: GitHub (Bereitstellung/Logs)\n\nAnalyse (nach Einwilligung): Google (GA4)\n\nKommentare (freiwillig): GitHub (Utterances/Issues)\n\n\n\nSpeicherdauer\n\nServer-Logs: gemäß Vorgaben des Hosters.\n\nAnalytics: gemäß GA4-Retention-Einstellung.\n\nKommentare: dauerhaft im öffentlichen Repository, bis Sie sie löschen oder ich sie moderiere.\n\n\n\nPflicht zur Bereitstellung\nEs besteht keine gesetzliche Pflicht, personenbezogene Daten bereitzustellen. Ohne die Verarbeitung technischer Daten ist die Bereitstellung der Website jedoch nicht möglich; ohne Einwilligung erfolgt keine Webanalyse; ohne GitHub-Account ist kein Kommentieren möglich.\n\n\nIhre Rechte (DSGVO)\nSie haben das Recht auf Auskunft, Berichtigung, Löschung, Einschränkung, Datenübertragbarkeit sowie Widerruf erteilter Einwilligungen und Beschwerde bei einer Aufsichtsbehörde. Bitte kontaktieren Sie mich dazu unter der oben genannten Adresse.\nStand: 06.08.2025"
  },
  {
    "objectID": "privacy.html#english",
    "href": "privacy.html#english",
    "title": "Datenschutz · Privacy Policy",
    "section": "English",
    "text": "English\n\nController\nChristian Wittmann\nEssentalstraße 4, 89584 Ehingen (Donau), Germany\nEmail: impressum [dot] vibes099 [at] aleeas [dot] com\n\n\nPurpose of the site\nPersonal, non-commercial learning blog (“Machine Learning Journey”).\n\n\nHosting (GitHub Pages)\nThe site is delivered by GitHub Pages. GitHub processes technical logs (e.g., IP address, timestamps) to provide and secure the service. Please refer to GitHub’s privacy information for details.\nLegal basis: Art. 6(1)(f) GDPR (legitimate interest in secure and efficient delivery).\n\n\nCookies, Consent & Preference Management\nBy default, only essential elements are loaded. Analytics is activated only after your consent via the cookie banner. You can change your choice at any time via the banner settings.\nLegal basis for analytics: Art. 6(1)(a) GDPR (consent).\nLegal basis for necessary processing (delivery/logs): Art. 6(1)(f) GDPR.\n\n\nWeb Analytics (Google Analytics 4)\nI use Google Analytics 4 (GA4) to measure visits and interactions. According to Google, GA4 does not store IP addresses and provides EU-oriented privacy controls. GA4 loads only after you opt in via the banner. You can withdraw consent anytime.\nData processed (examples): page views, approximate device/browser info, interaction data.\nRecipient: Google Ireland Limited (and affiliates as applicable).\nInternational transfers: Data may be transferred to third countries under Google’s safeguards.\nRetention: according to GA4 retention settings (typically 2–14 months, depending on configuration).\n\n\nComments (Utterances / GitHub Issues)\nComments are powered by Utterances and stored as GitHub Issues in a public repository. To comment, you sign in with your GitHub account; GitHub processes your data under its policies. Your comment and GitHub username will be publicly visible.\nLegal basis: Art. 6(1)(a) GDPR (your voluntary use/consent).\n\n\nThird-party embeds\nNo third-party embeds are currently used that transfer personal data without interaction. Links to external sites transfer data only when you click them.\n\n\nRecipients / Categories of recipients\n\nHosting: GitHub (delivery/logs)\n\nAnalytics (upon consent): Google (GA4)\n\nComments (voluntary): GitHub (Utterances/Issues)\n\n\n\nStorage periods\n\nServer logs: according to the host’s policies.\n\nAnalytics: according to GA4 retention settings.\n\nComments: persist in the public repository until you delete or I moderate them.\n\n\n\nNecessity to provide data\nYou are not legally required to provide personal data. Without technical processing the site cannot be delivered; without consent no analytics will run; without a GitHub account you cannot comment.\n\n\nYour rights (GDPR)\nYou have the rights of access, rectification, erasure, restriction, portability, withdrawal of consent, and complaint to a supervisory authority. Please contact me using the details above.\nLast updated: 2025-08-06"
  },
  {
    "objectID": "impressum.html",
    "href": "impressum.html",
    "title": "Impressum · Legal Notice",
    "section": "",
    "text": "Hinweis / Note: Deutsch ist die rechtlich maßgebliche Fassung. English version below is for convenience."
  },
  {
    "objectID": "impressum.html#deutsch",
    "href": "impressum.html#deutsch",
    "title": "Impressum · Legal Notice",
    "section": "Deutsch",
    "text": "Deutsch\nAngaben gemäß § 18 MStV\nChristian Wittmann\nEssentalstraße 4\n89584 Ehingen (Donau)\nDeutschland\nE-Mail: impressum [dot] vibes099 [at] aleeas [dot] com\nZweck der Website\nDiese Website ist ein nicht-kommerzielles, persönliches Lern- und Notizblog. Ich dokumentiere meine Lernreise im Bereich Machine Learning und KI, um mir Konzepte durch Schreiben selbst zu erklären (Feynman-Technik).\nEs handelt sich nicht um ein journalistisch-redaktionelles Angebot im Sinne des MStV.\nVerantwortlich für den Inhalt\nChristian Wittmann (Anschrift wie oben)\nHosting\nDiese Website wird über GitHub Pages bereitgestellt.\nHaftung für Inhalte / Links\nTrotz sorgfältiger inhaltlicher Kontrolle kann ich keine Haftung für externe Inhalte übernehmen. Für den Inhalt verlinkter Seiten sind ausschließlich deren Betreiber verantwortlich.\nUrheberrecht\nSofern nicht anders angegeben, stehen Texte und Abbildungen unter der von mir gewählten Lizenz der jeweiligen Beiträge. Zitate und Code-Beispiele sind als solche gekennzeichnet.\nStand: 06.08.2025"
  },
  {
    "objectID": "impressum.html#english",
    "href": "impressum.html#english",
    "title": "Impressum · Legal Notice",
    "section": "English",
    "text": "English\nProvider Identification (Legal Notice)\nChristian Wittmann\nEssentalstraße 4\n89584 Ehingen (Donau)\nGermany\nEmail: impressum [dot] vibes099 [at] aleeas [dot] com\nPurpose of the website\nThis is a personal, non-commercial learning blog. I document my Machine Learning journey and explain concepts to myself using the Feynman technique.\nThis site is not a journalistic-editorial publication.\nContent responsibility\nChristian Wittmann (address as above)\nHosting\nThis website is hosted via GitHub Pages.\nLiability / Links\nAlthough I check external links carefully, I cannot accept liability for external content. Responsibility for linked pages lies with their respective operators.\nCopyright\nUnless stated otherwise, texts and images are provided under the license indicated in each post. Quotes and code samples are marked accordingly.\nLast updated: 2025-08-06"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christian’s Machine Learning Journey",
    "section": "",
    "text": "Building a Harmonized-API ‘Hello World’ with AI Core\n\n\n\nsap\n\norchestration\n\nllm\n\n\n\n\n\n\n\n\n\nJan 27, 2026\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s test RPT-1 on Titanic\n\n\n\nsap\n\nrpt-1\n\ntabular\n\ntitanic\n\n\n\n\n\n\n\n\n\nJan 9, 2026\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding an SAP-RPT-1 ‘Hello World’ with AI Core\n\n\n\nsap\n\nrpt-1\n\ntabular\n\n\n\n\n\n\n\n\n\nDec 17, 2025\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome SAP-RPT-1! What is it? How can you try it out?\n\n\n\nsap\n\nrpt-1\n\ntabular\n\n\n\n\n\n\n\n\n\nDec 12, 2025\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nManaging Python Environments\n\n\n\npython\n\ninstall\n\nfast.ai\n\n\n\n\n\n\n\n\n\nOct 24, 2025\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nRunning GPT-OSS Locally on macOS with Ollama + Open WebUI\n\n\n\nollama\n\ngpt-oss\n\nllm\n\n\n\n\n\n\n\n\n\nAug 8, 2025\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying a FastAPI App to SAP BTP\n\n\n\nfastapi\n\nsap\n\nbtp\n\n\n\n\n\n\n\n\n\nJul 10, 2025\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying a FastAPI App to the Cloud\n\n\n\nfastapi\n\nrender\n\n\n\n\n\n\n\n\n\nJul 9, 2025\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing a Greeting API with FastAPI\n\n\n\nfastapi\n\njupyter\n\n\n\n\n\n\n\n\n\nJul 8, 2025\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nThe Million-Token Question: What the Bible Teaches Us About LLM Pricing\n\n\n\ntokenization\n\nnlp\n\n\n\n\n\n\n\n\n\nJun 1, 2025\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nEmpirically estimating tokens per word across languages\n\n\n\ntokenization\n\nnlp\n\n\n\n\n\n\n\n\n\nMay 23, 2025\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nBeneath the Blue Dome\n\n\n\nvibe-writing\n\n\n\n\n\n\n\n\n\nApr 20, 2025\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Web Search for Large Language Models from Scratch\n\n\n\nllm\n\nReAct\n\nai\n\ntools\n\nfunction_calling\n\n\n\n\n\n\n\n\n\nMar 28, 2025\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Intelligence to Generative AI\n\n\n\nai\n\nml\n\ngenai\n\nllm\n\n\n\n\n\n\n\n\n\nMar 27, 2025\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Relative Risk Matters in AI Ethics: A Personal Journey into Gender Inference\n\n\n\nai-ethics\n\n\n\n\n\n\n\n\n\nMar 14, 2025\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nOmniChat - How to chat with any LLM\n\n\n\nchat\n\nllm\n\npython\n\ninjection\n\nopenai\n\nllama\n\nanthropic\n\nx.ai\n\ngrok\n\ngroq\n\n\n\n\n\n\n\n\n\nNov 30, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Run JavaScript in a Jupyter Notebook\n\n\n\njavascript\n\njupyter\n\n\n\n\n\n\n\n\n\nOct 22, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding the Apple Calculator in a Jupyter Notebook\n\n\n\nLLM\n\nfunction_calling\n\nReAct\n\ncalculator\n\n\n\n\n\n\n\n\n\nAug 2, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Turn GPT into a Calculator\n\n\n\nLLM\n\nfunction_calling\n\ntools\n\ncalculator\n\nReAct\n\n\n\n\n\n\n\n\n\nJul 30, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nUpdating Quarto and Helpful Features\n\n\n\nblogging\n\nquarto\n\nupdate\n\n\n\n\n\n\n\n\n\nJun 22, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nHow LLMs are Trained\n\n\n\nllm\n\nnlp\n\ntraining\n\n\n\n\n\n\n\n\n\nJun 21, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nRemembering the Wittmann Tours World Trip with RAG\n\n\n\nrag\n\nllm\n\nnlp\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Embeddings in 2D\n\n\n\nembeddings\n\nllm\n\nnlp\n\n\n\n\n\n\n\n\n\nMar 15, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Convert a Wordpress Blog into Markdown\n\n\n\nwordpress\n\nmarkdown\n\ndataset\n\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Chat for Jupyter Notebooks from Scratch\n\n\n\nchat\n\njupyter\n\nllama2\n\nllm\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Implement a C Binding in Python with ctypes\n\n\n\nc\n\nbinding\n\nllama.cpp\n\nllm\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nRunning LLama2 locally on a Mac\n\n\n\nllama2\n\napple silicon\n\nllama.cpp\n\nhugging face\n\nllm\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nHow to call the OpenAI API from a Jupyter Notebook\n\n\n\nopenai\n\njupyter\n\napi\n\nllm\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nRunning Fast.AI / Huggingface Transformers on Apple Silicon\n\n\n\nml\n\nfast.ai\n\napple silicon\n\nhugging face\n\nmnist\n\nnlp\n\n\n\n\n\n\n\n\n\nJan 5, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling Fast.AI on Apple Silicon\n\n\n\nml\n\nfast.ai\n\napple silicon\n\ninstall\n\n\n\n\n\n\n\n\n\nDec 13, 2023\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nTitanic with ChatGPT\n\n\n\nml\n\nkaggle\n\ntitanic\n\nchatgpt\n\neda\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nDiscovering Disaster Tweet Dataset Limitations\n\n\n\nml\n\ndata\n\nhugging face\n\nnlp\n\nkaggle\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nNatural Language Processing with Disaster Tweets\n\n\n\nkaggle\n\nnlp\n\nhugging face\n\nml\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nWrapping-up Lesson 3\n\n\n\nfast.ai\n\nkaggle\n\nml\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nWhen disaster strikes: Re-building a Quarto Blog\n\n\n\nblogging\n\nquarto\n\njupyter\n\n\n\n\n\n\n\n\n\nNov 27, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nMNIST, the ‘Hello World’ of Computer Vision\n\n\n\nkaggle\n\nmnist\n\nfast.ai\n\nvision\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nMy First Kaggle Competition: Titanic\n\n\n\nkaggle\n\ntitanic\n\nfast.ai\n\nml\n\ntabular\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Multiplication\n\n\n\nmath\n\npython\n\nnumpy\n\npytorch\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nHow I created this Blog\n\n\n\nblogging\n\nquarto\n\njupyter\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Blog Post using a Jupyter Notebook\n\n\n\nblogging\n\nquarto\n\njupyter\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Gradient Descent in 3D\n\n\n\nfast.ai\n\nml\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nFast.AI with Bears, Cats and Dogs\n\n\n\nfast.ai\n\nml\n\n\n\n\n\n\n\n\n\nOct 3, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n\n\n\n\n\n\nHello World!\n\n\n\nblogging\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\nChristian Wittmann\n\n\n\n\n\nNo matching items\n\n  \n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/2024-03-22-rag1-remembering-world-trip/index.html",
    "href": "posts/2024-03-22-rag1-remembering-world-trip/index.html",
    "title": "Remembering the Wittmann Tours World Trip with RAG",
    "section": "",
    "text": "Back in 2017/2018 my wife and I did a world trip, and we documented it on our blog Wittmann-Tours.de. These 14 month were among the most exciting times of my life, but nonetheless, I start forgetting details. Therefore, I thought it would be great to have a large language model (LLM) which could help me remember the details."
  },
  {
    "objectID": "posts/2024-03-22-rag1-remembering-world-trip/index.html#from-idea-to-data",
    "href": "posts/2024-03-22-rag1-remembering-world-trip/index.html#from-idea-to-data",
    "title": "Remembering the Wittmann Tours World Trip with RAG",
    "section": "From Idea to Data",
    "text": "From Idea to Data\nHow did I come up with this idea? Reworking the Retrieval Augmented Retrieval (RAG) section from the hackers guide, I decided, I wanted to build my own RAG application, but what would be a good data source? I needed a good data source which fulfilled 2 criteria:\n\nThe LLM I used for the exercise should not know the content of the data source.\nI should know the content of the data source very well to be able to judge if the LLM, once RAG is implemented, could answer questions related to the data source correctly.\n\nSince our blog is pretty low-profile, Wittmann-Tours.de fulfilled all the requirements perfectly, but how can I use it fot this project? As with many machine learning projects, data is the key to success. Therefore, I needed to pause a bit on the RAG implementation and started to work on converting my blog to markdown. This was more difficult than expected, and you can read the full story in this blog post. By now, the Wittmann-Tours blog is available my Wittmann-Tours GitHub repo under license CC-BY NC.\n\n\n\n\nDalle: Remembering the Wittmann Tours World Trip with RAG"
  },
  {
    "objectID": "posts/2024-03-22-rag1-remembering-world-trip/index.html#what-is-retrieval-augmented-generation-rag",
    "href": "posts/2024-03-22-rag1-remembering-world-trip/index.html#what-is-retrieval-augmented-generation-rag",
    "title": "Remembering the Wittmann Tours World Trip with RAG",
    "section": "What is Retrieval Augmented Generation (RAG)?",
    "text": "What is Retrieval Augmented Generation (RAG)?\nBefore explaining Retrieval Augmented Generation (RAG), let’s look at some of the challenges we can typically face with large language models (LLMs):\n\nLLMs can hallucinate, meaning that they can generate false or misleading information, nonetheless presented in a very plausible sounding way.\nLLMs do not have up-to-date information because they have been trained on a fixed corpus of information.\nLLMs cannot always give sources. Why? Here’s my take: As Andrej Karpathy pointed out, an LLM is a lossy compression of text containing only the gestalt of the text. This translates well to our own way of learning. After you have read a Wikipedia article, you cannot recite the whole article, but you have learned the essence of the article. Similarly, the detailed references are lost during LLM references.\n\n\nSpeaking of sources: IBM Research has published a very accessible Youtube Video / blog post describing key challenges with LLMs and how RAG addresses these issues.\n\nRAG addresses these issues by giving additional context to a question/prompt you send to an LLM, therefore addressing the issues above:\n\nWhen you ask an LLM about a topic, and pass along the related Wikipedia article, the answer is very likely to be factually correct.\nYou can pass any additional information to the LLM, crossing also into the time after the training cut-off.\nLet’s assume there is a mechanism to select the appropriate source (more on that later), the LLM can tell you the source of the information it used for the answer.\n\nGoing back to its origin, RAG was first introduced in a paper by Facebook AI Research (today’s research group of Meta) in 2020 as a mechanism to enhance the capabilities of language models by adding relevant contextual information to the user prompt. As we have seen in the hacker’s guide, here is a simple way how we can formulate a RAG-prompt:\nprompt_with_context = (\n    f\"Answer the question with the help of the provided context.\"\n    f\"## Question\"\n    f\"{question}\"\n    f\"## Context\"\n    f\"{context}\"\n)\nLet’s try that out and build mini-framework which enables a Llama2-LLM to answer questions related to the Wittmann-Tours.de blog. If you prefer to run all the code yourself, please check out this notebook on GitHub."
  },
  {
    "objectID": "posts/2024-03-22-rag1-remembering-world-trip/index.html#llm-setup",
    "href": "posts/2024-03-22-rag1-remembering-world-trip/index.html#llm-setup",
    "title": "Remembering the Wittmann Tours World Trip with RAG",
    "section": "LLM Setup",
    "text": "LLM Setup\nLet me reuse the pattern I developed in the chat consumer notebook based on my blog post Building Chat for Jupyter Notebooks from Scratch. This time, let’s use the maximum context window size possible for this model (n_ctx=4096) to fit as much context as possible into the prompt.\n\nimport sys\nsys.path.append('../../../lm-hackers/notebook_chat')\nfrom notebook_chat import ChatMessages, Llama2ChatVersion2\n\nfrom llama_cpp import Llama\nllm = Llama(model_path=\"../../../lm-hackers/models/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=4096, verbose=False)"
  },
  {
    "objectID": "posts/2024-03-22-rag1-remembering-world-trip/index.html#loading-the-wittmann-tours.de-dataset",
    "href": "posts/2024-03-22-rag1-remembering-world-trip/index.html#loading-the-wittmann-tours.de-dataset",
    "title": "Remembering the Wittmann Tours World Trip with RAG",
    "section": "Loading The Wittmann-Tours.de dataset",
    "text": "Loading The Wittmann-Tours.de dataset\nThe Wittmann-Tours.de blog is available for download as a dataset in the Wittmann-Tours repo.\nwget -P ./wt-blogposts https://github.com/chrwittm/wittmann-tours/raw/main/zip/blogposts-md.zip\nunzip -o ./wt-blogposts/blogposts-md.zip -d ./wt-blogposts/\nAs a result we have all the blog posts in a folder called wt-blogposts.\n\nNote: This is just an example, I write this blog in Jupyter notebooks, the the references will look a bit different below reflecting the file system on my local machine."
  },
  {
    "objectID": "posts/2024-03-22-rag1-remembering-world-trip/index.html#building-the-wittmann-tours-llm-with-rag",
    "href": "posts/2024-03-22-rag1-remembering-world-trip/index.html#building-the-wittmann-tours-llm-with-rag",
    "title": "Remembering the Wittmann Tours World Trip with RAG",
    "section": "Building the Wittmann-Tours LLM with RAG",
    "text": "Building the Wittmann-Tours LLM with RAG\nBefore we start, let’s quickly verify that the LLM does not know about the blog by asking an example question:\n\nquestion = \"What was the name of the guide who led us on our tour in the Masoala rain forest on Madagascar?\"\nchat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\nchat.prompt_llama2_stream(f\"{question}\")\n\n The name of the guide who led your tour in the Masoala rainforest on Madagascar is… (drumroll) …Rahel!\n\n\nWell, it tried to guess, but the answer is not correct. Let’s provide more context, here is the blog post about Masoala:\n\npath_to_blogpost = \"../../../lm-hackers/wt-blogposts/drei-tage-im-masoalaregenwald/index.md\"\n\nwith open(path_to_blogpost, 'r') as file:\n    content = file.read()\n\nprint(f\"The blogpost has {len(content)} characters\")\nprint(content[:905])\n\nThe blogpost has 18435 characters\n---\ntitle: 'Drei Tage im Masoala-Regenwald'\ndescription: \"\"\npublished: 2019-07-14\nredirect_from: \n            - https://wittmann-tours.de/drei-tage-im-masoala-regenwald/\ncategories: \"Brookesia, Chamäleon, Lemur, Madagaskar, Madagaskar, Maki, Masoala, Regenwald, roter Vari, Taggecko, Umweltschutz, Vari, Wald, Wanderung\"\nhero: ./img/wp-content-uploads-2019-06-CW-20180820-105656-0464-1024x683.jpg\n---\n# Drei Tage im Masoala-Regenwald\n\nNach einer knapp 2-stündigen Bootsfahrt von [Nosy Mangabe](http://wittmann-tours.de/nosy-mangabe) aus erreichten wir unser Ziel, die Masoala Forest Lodge. Wir landeten an einem Strand und gingen kaum 200 Meter landeinwärts, wo ein paar hübsche kleine Bungalows auf uns warteten. So viel Luxus hatten wir nach der vorherigen Campingnacht kaum erwartet. Um das gute Wetter - sprich kein Regen - auszunutzen, starteten wir umgehend auf die erste Wanderung durch den Urwald.\n\n\nBefore we can ask the question in context it is important to realize that the model we use has a maximum context window of 4096 tokens. Since the blog post is longer, I only pass the first section. Realizing this limitation, I will not solve this here, because the main goal it to understand how we can provide context at all.\n\ndef get_question_with_context(question, context):\n    return ( f\"Answer the question with the help of the provided context.\"\n             f\"## Question\"\n             f\"{question}\"\n             f\"## Context\"\n             f\"{context}\"\n)\n\n\nquestion = \"What was the name of the guide who led us on our tour in the Masoala rain forest on Madagascar?\"\nchat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\nchat.prompt_llama2_stream(f\"{get_question_with_context(question, content[:6000])}\")\n\n The guide who led us on our tour in the Masoala rainforest was named Armand.\n\n\nThe answer is correct! However, it feels a bit like a self-fulfilling prophecy since we manually added the right context.\nHow can we automate the search for the right context? The secret sauce contains a semantic search via embeddings."
  },
  {
    "objectID": "posts/2024-03-22-rag1-remembering-world-trip/index.html#using-embeddings-to-determine-context",
    "href": "posts/2024-03-22-rag1-remembering-world-trip/index.html#using-embeddings-to-determine-context",
    "title": "Remembering the Wittmann Tours World Trip with RAG",
    "section": "Using Embeddings to Determine Context",
    "text": "Using Embeddings to Determine Context\nI have discussed embeddings in detail in my blog post Visualizing Embeddings in 2D. Here is a quick recap:\nAn embedding is a high-dimensional numerical representations (a vector) of text which encodes semantic information. To put this in simple language: An embedding is a bunch of numbers which magically happens to describe the meaning of the text they represent. Embeddings which contain similar information are close to each other, and we can calculate the distance between 2 embeddings. Before we calculate the similarity between embeddings of questions and blog posts, let’s visualize them to get a more intuitive understanding what is happening under the hood.\n\nVisual Context Determination\nVisualizing the blog posts embeddings the same way as in Visualizing Embeddings in 2D, we get the following chart:\n\n\n\n2D Visualization of Question/Blog Post Embeddings with Vectors and Cosine Similarity Circle\n\n\nThis chart looks very confusing at first glance, so let’s unpack:\n\nThe arrows (the vectors) each represent a blog post, i.e. the embedding of the blog post which contains semantic information.\nThe arrows are color-coded by country. We can see that some clusters, for example, the arrow for Ethiopia in yellow mostly point in other directions than the red ones representing Japan. There are, however, also close ones. Without having checked, I assume that this could be blog posts talking about food.\nThere are 2 black arrows in the chart labeled “Nearest”. These are the two vectors representing the Masoala Blog Post and the question “What was the name of the guide who led us on our 3-day tour in the Masoala rain forest on Madagascar?”. (Please also read the Postscriptum for this blog post.)\nThe colorful circle in the middle represents the cosine similarity: Green represents closeness. Spanning via yellow to red, the similarity decreases.\n\nThis nice visualization, shows how the computer can do a semantic search and determine the most relevant context for a question we ask an LLM: One of the black arrows represents the question, and the other represents the blog post. Because the (consine) similarity is highest between the 2, the Masoala blog post is the best context for the question.\n\n\nComputational Context Determination\nThe visualization above hopefully provided a way to intuitively understand in 2D what we will now compute in higher dimensionality dimensions. Let’s start with defining some functions. If you want to follow along interactively, here is the related jupyter notebook.\n\nimport os\nimport glob\nfrom sentence_transformers import SentenceTransformer\n\n#emb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=\"mps\") #English\nemb_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"mps\") #Multi-lingual\n\ndef get_blog_post_files(path_to_blog):\n\n    pattern = os.path.join(path_to_blog, \"**/*.md\")\n    return glob.glob(pattern, recursive=True)\n\ndef get_blog_post(path):\n    with open(path, 'r') as file:\n        content = file.read()\n    return content\n\ndef get_text_embedding(text):\n    return emb_model.encode(text, convert_to_tensor=True)\n\ndef get_blog_post_embedding(path):\n    blog_post_text = get_blog_post(path)\n    return get_text_embedding(blog_post_text)\n\nTo see an example of an high-dimensional embedding, here is the question embedding and the first 5-dimensions:\n\nquestion_embedding = get_text_embedding(question)\nprint(question_embedding[:5])\nprint(question_embedding.shape)\n\ntensor([-0.0244, -0.0237, -0.0702, -0.0352, -0.0163], device='mps:0')\ntorch.Size([1024])\n\n\nNow let’s compute the cosine similarity in 1024-dimensional space for 3 examples:\n\nimport torch.nn.functional as F\n\ndef get_similarity(embedding1, embedding2):\n    return F.cosine_similarity(embedding1, embedding2, dim=0)\n\n\nblog_posts = [\"../../../lm-hackers/wt-blogposts/drei-tage-im-masoalaregenwald/index.md\",\n              \"../../../lm-hackers/wt-blogposts/der-bolivianische-dschungel-im-madidi-nationalpark/index.md\",\n              \"../../../lm-hackers/wt-blogposts/essen-mit-stern-hongkong-kulinarisch/index.md\"]\n\nsimilarities = [get_similarity(question_embedding, get_blog_post_embedding(blog_post)) for blog_post in blog_posts]\nsimilarities\n\n[tensor(0.6135, device='mps:0'),\n tensor(0.5241, device='mps:0'),\n tensor(0.3370, device='mps:0')]\n\n\nNot surprisingly, the blog post about the Masoala Rain Forst has the highest cosine similarity.\nLet’s put it all together and do a lookup over all blog posts related to a question.\n\ndef get_blog_post_as_context(question):\n\n    files = get_blog_post_files(\"../../../lm-hackers/wt-blogposts\")\n\n    best_match = \"\"\n    best_match_embedding = get_text_embedding(best_match)\n    question_embedding = get_text_embedding(question)\n    best_similarity = get_similarity(question_embedding, best_match_embedding)\n\n    for file in files:\n        blog_post_embedding = get_blog_post_embedding(file)\n        blog_post_similarity = get_similarity(question_embedding, blog_post_embedding)\n        if blog_post_similarity &gt; best_similarity:\n            best_similarity = blog_post_similarity\n            best_match = file\n    \n    return best_match\n\nThe mechanism in the cell above is quite inefficient, but the goal is to show the basic concept how to find the blog post best related to a question, or to put is in more general words, how to determine the best context for a question.\nLet’s combine the context determination with prompting the LLM and displaying the source of the answer:\n\nquestion = \"What was the name of the guide who led us on our tour in the Masoala rain forest on Madagascar?\"\nblog_post_path = get_blog_post_as_context(question)\nblog_post_context = get_blog_post(blog_post_path)\nchat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\nchat.prompt_llama2_stream(f\"{get_question_with_context(question, blog_post_context[:6000])}\")\nprint(f\"Context used: {blog_post_path}\")\n\n The guide who led us on our tour in the Masoala rainforest on Madagascar was named Armand.\n\n\nContext used: ../../../lm-hackers/wt-blogposts/nosy-mangabe/index.md"
  },
  {
    "objectID": "posts/2024-03-22-rag1-remembering-world-trip/index.html#conclusion",
    "href": "posts/2024-03-22-rag1-remembering-world-trip/index.html#conclusion",
    "title": "Remembering the Wittmann Tours World Trip with RAG",
    "section": "Conclusion",
    "text": "Conclusion\nWe have successfully built a RAG-application to answer questions related to the Wittmann-Tours.de blog - Juhu! Between the lines, you can already see more general concepts:\n\nWe have indexed all blog posts into embeddings to encode their semantics. This essentially translates to adding data to a vector database.\nWe have computed the embedding for a question and visualized/computed finding the closest blog post. This essentially translates to querying a vector database.\n\nThe implementation successfully helped to mitigate typical LLM-related problems described above in the following way:\n\nBy grounding the LLM in data related to the question, the LLM did not hallucinate.\nThe LLM can answer questions related to topics it was never trained on.\nAlongside the answer we get the source of the information and can cross-check if needed."
  },
  {
    "objectID": "posts/2024-03-22-rag1-remembering-world-trip/index.html#postscriptum",
    "href": "posts/2024-03-22-rag1-remembering-world-trip/index.html#postscriptum",
    "title": "Remembering the Wittmann Tours World Trip with RAG",
    "section": "Postscriptum",
    "text": "Postscriptum\nJust as I wrote the conclusion of this blog post, I realized what I would label a “copy&paste error”. This error, however, did not really materialize, and I think it is very interesting, so I though it is worth mentioning - But what happened?\nInitially, I used the embedding model (BAAI/bge-small-en-v1.5), copy&pasting from the hacker’s guide into my embedding notebook, copy&pasting into this notebook. This model, however, has been trained on English, but the Wittmann-Tours.de blog is written in German. Nonetheless, it performed very well in calculating the similarity of the vectors, and it even bridged the gap between the German texts and the English question.\nI only noticed one slight inconsistency: When visualizing the blog posts in 2D, the question “What was the name of the guide who led us on our 3-day tour in the Masoala rain forest on Madagascar?” did not show the Masoala blog post as the closest vector. Initially, I just thought the reason was rooted in the dimensionality reduction from 384 to 2 dimensions. After realizing the language issue, changing the embedding model to the multi-lingual BAAI/bge-m3 fixed that. For educational purposes, however, I used the slightly tweaked visualization from the english embedding model because the visualization from the multi-lingual model is too perfect: The arrows of the question and of the blog post are displayed as one. Embrace your errors, and usually they yield something useful!"
  },
  {
    "objectID": "posts/2024-03-22-rag1-remembering-world-trip/index.html#references",
    "href": "posts/2024-03-22-rag1-remembering-world-trip/index.html#references",
    "title": "Remembering the Wittmann Tours World Trip with RAG",
    "section": "References",
    "text": "References\n[1] The original RAG-paper: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n[2] The Hacker’s Guide by Jeremy Howard\n[3] Visualizing Embeddings in 2D\n[4] The Wittmann-Tours.de blog, also available on GitHub"
  },
  {
    "objectID": "posts/2025-08-08-running-gpt-oss-locally/index.html",
    "href": "posts/2025-08-08-running-gpt-oss-locally/index.html",
    "title": "Running GPT-OSS Locally on macOS with Ollama + Open WebUI",
    "section": "",
    "text": "Wouldn’t it be nice to have a ChatGPT-grade assistant running locally on your laptop? That’s now possible with OpenAI’s first open-weight models, GPT-OSS. The 20B version is small enough to run on a modern consumer laptop.\nMy MacBook Pro has an M2 Max with 32 GB of unified memory, by far not the latest and greatest, but the experience is… nice, running at around 33 output tokens per second. It’s not going to outrun GPT-5 in the cloud, and it’s not multi-modal, but when you just need text-based conversations, it’s a powerful model. And the offline factor is a real advantage when you’re disconnected or want to keep everything on-device.\nBack in February 2024, I wrote about running Llama 2 locally on my Mac. Fast-forward ~18 months and a lot has changed, not just model performance (it’s now far superior on the same hardware), but also the tooling. This time I chose Ollama for its active community and ecosystem of add-ons. In this post, I’ll walk you through all the steps to get GPT-OSS up and running on a Mac (as long as you have at least 16 GB of RAM), complete with a nice web UI via Open WebUI.\nI recorded the process as I went, so this guide is intentionally more verbose than OpenAI’s official Cookbook. (-&gt; to be explicit about every step you’ll need to take)"
  },
  {
    "objectID": "posts/2025-08-08-running-gpt-oss-locally/index.html#install-ollama",
    "href": "posts/2025-08-08-running-gpt-oss-locally/index.html#install-ollama",
    "title": "Running GPT-OSS Locally on macOS with Ollama + Open WebUI",
    "section": "Install Ollama",
    "text": "Install Ollama\nOllama is the runtime that will download, run, and manage your local models. You can install it in two ways:\nOption A — macOS installer\n\nDownload from https://ollama.com/download\nRun the installer — this gives you both the Ollama app and the ollama CLI.\nTo upgrade: simply download the latest installer and run it again.\n\nOption B — Homebrew (my recommendation)\nIf you’re comfortable with Terminal, Homebrew makes installation and upgrades much quicker:\nbrew install ollama  # installs Ollama\nollama --version     # check your installed version\nbrew upgrade ollama  # upgrade Ollama to the latest version"
  },
  {
    "objectID": "posts/2025-08-08-running-gpt-oss-locally/index.html#starting-and-stopping-ollama",
    "href": "posts/2025-08-08-running-gpt-oss-locally/index.html#starting-and-stopping-ollama",
    "title": "Running GPT-OSS Locally on macOS with Ollama + Open WebUI",
    "section": "Starting and stopping Ollama",
    "text": "Starting and stopping Ollama\nWith the Homebrew installation, I run Ollama as a background service so it’s always ready:\nbrew services start ollama   # starts in the background, auto-starts after reboot\nbrew services restart ollama # restarts after upgrading\nbrew services stop ollama    # stops completely\nIf you’d rather start it manually, run:\nollama serve\nThis will keep it running in that terminal until you press Ctrl+C."
  },
  {
    "objectID": "posts/2025-08-08-running-gpt-oss-locally/index.html#managing-models",
    "href": "posts/2025-08-08-running-gpt-oss-locally/index.html#managing-models",
    "title": "Running GPT-OSS Locally on macOS with Ollama + Open WebUI",
    "section": "Managing models",
    "text": "Managing models\nOllama has its own model library, and GPT-OSS lives here: https://ollama.com/library/gpt-oss.\nYou can manage the models on your Mac with the following commands (of course, you can replace gpt-oss with other model names from the library):\nollama list           # see which models are installed\nollama pull gpt-oss   # download GPT-OSS\nollama rm gpt-oss     # delete GPT-OSS"
  },
  {
    "objectID": "posts/2025-08-08-running-gpt-oss-locally/index.html#testing-gpt-oss-from-the-command-line",
    "href": "posts/2025-08-08-running-gpt-oss-locally/index.html#testing-gpt-oss-from-the-command-line",
    "title": "Running GPT-OSS Locally on macOS with Ollama + Open WebUI",
    "section": "Testing GPT-OSS from the command line",
    "text": "Testing GPT-OSS from the command line\nTesting as early as possible is important. Before adding the UI, let’s confirm everything is working correctly from the CLI:\nollama run gpt-oss\nThis will start a chat-like session with the model. Try out prompts like:\n\nList the planets of the solar system\nReverse the list\n\nOnce you’re done, you can exit the chat with the command /exit.\nYou can also do a one-off prompt without entering chat mode:\nollama run gpt-oss \"What is the meaning of life in one sentence?\""
  },
  {
    "objectID": "posts/2025-08-08-running-gpt-oss-locally/index.html#installing-open-webui",
    "href": "posts/2025-08-08-running-gpt-oss-locally/index.html#installing-open-webui",
    "title": "Running GPT-OSS Locally on macOS with Ollama + Open WebUI",
    "section": "Installing Open WebUI",
    "text": "Installing Open WebUI\nTo make the whole experience more user-friendly, let’s add a web UI. I use Open WebUI, which provides a clean interface similar to OpenAI, Anthropic, or Google, including chat history and multiple chat sessions.\nIf you read through the documentation, they mention a Docker setup first. However, the method below (listed as “recommended” later in their docs) is easier.\nFirst, install uv, a modern Python runner:\nbrew install uv\nThen start Open WebUI with:\nDATA_DIR=~/.open-webui uvx --python 3.11 open-webui@latest serve\nLet’s break it down:\n\nDATA_DIR=~/.open-webui: This sets an environment variable for the command that follows. It tells Open WebUI where to store its chat history and configuration. This path will persist your data between runs.\nuvx: Use UVX (a modern Python environment manager) to run Open WebUI in an isolated environment.\n--python 3.11: Specifies the Python version to use. The Open WebUI team recommends Python 3.11.\nopen-webui@latest serve: Installs (if necessary) and starts the latest version of Open WebUI."
  },
  {
    "objectID": "posts/2025-08-08-running-gpt-oss-locally/index.html#going-offline",
    "href": "posts/2025-08-08-running-gpt-oss-locally/index.html#going-offline",
    "title": "Running GPT-OSS Locally on macOS with Ollama + Open WebUI",
    "section": "Going Offline",
    "text": "Going Offline\nFor full offline capability, you need to pre-install a specific (pinned) version of Open WebUI once while online. This ensures you can run it later without any network connection. (Get the latest version number from Pypi.)\nuv tool install --python 3.11 open-webui==0.6.18   \nAfterwards, you can launch this version offline with:\nDATA_DIR=~/.open-webui ~/.local/bin/open-webui serve"
  },
  {
    "objectID": "posts/2025-08-08-running-gpt-oss-locally/index.html#running-open-webui",
    "href": "posts/2025-08-08-running-gpt-oss-locally/index.html#running-open-webui",
    "title": "Running GPT-OSS Locally on macOS with Ollama + Open WebUI",
    "section": "Running Open WebUI",
    "text": "Running Open WebUI\nLoading and setting up the virtual environment takes around 30 seconds (longer the very first time you run it). Once it’s ready, open:\nhttp://localhost:8080\n\nTip: Ollama must be running first (brew services start ollama). You can check if it’s active by visiting http://127.0.0.1:11434 — if the service responds, you’re good.\n\nFrom here, the UI should feel familiar if you’ve used ChatGPT. Start a new chat, select GPT-OSS, and you’re ready to go.\nWhen you’re finished, press Ctrl+C in the terminal where you started Open WebUI.\n\nNote: Leaving Ollama running as a background service is fine — it uses almost no CPU or battery when idle."
  },
  {
    "objectID": "posts/2025-08-08-running-gpt-oss-locally/index.html#conclusion",
    "href": "posts/2025-08-08-running-gpt-oss-locally/index.html#conclusion",
    "title": "Running GPT-OSS Locally on macOS with Ollama + Open WebUI",
    "section": "Conclusion",
    "text": "Conclusion\nYes, took a few steps, but the whole setup can be done in 30–60 minutes, even if you pause to test things along the way. In return, you get your own personal ChatGPT-style assistant running entirely on your Mac: Now you are ready for curiosity projects, privacy-sensitive work, or building your next idea without sending data to the cloud."
  },
  {
    "objectID": "posts/2025-03-27-intelligence-to-genai/index.html",
    "href": "posts/2025-03-27-intelligence-to-genai/index.html",
    "title": "From Intelligence to Generative AI",
    "section": "",
    "text": "How is Generative AI related to Intelligence? I’ve frequently seen the following diagram used as an opening slide in AI presentations to set the stage. Presenters, myself included, often skim through this slide too quickly, treating it as a superficial overview rather than unpacking its deeper meaning. You can view this diagram either as a historical journey through AI’s evolution or as a functional representation, where each layer embodies a distinct approach to problem-solving—each with its own capabilities, strengths, and limitations. Whichever perspective you choose, I feel this slide deserves more attention.\nIn this blog post, let’s slow down and thoroughly examine each level. Our goal is to develop a deeper understanding of the principles underlying each of these layers. After clearly defining each term, we’ll conceptually explore how these layers relate in a non-technical way. Then, we’ll implement each level from scratch, using lightweight Python examples. If you’re not into coding, feel free to skip directly to the visualizations and their interpretations. This practical exploration will help us intuitively understand when each approach is most suitable for a given use case.\nThroughout this detailed exploration, there are two important lessons to watch out for. First, we will see how we start from general human intelligence and, as we move deeper, transition from highly specialized to increasingly general computational approaches. Second, this journey serves as a reminder that even though Generative AI currently dominates the conversation, there may be other, better-suited approaches depending on your specific use case.\ngraph TD\n\n    subgraph Intelligence[\"Intelligence\"]\n        subgraph Artificial_Intelligence[\"Artificial Intelligence\"]\n            subgraph Machine_Learning[\"Machine Learning\"]\n                subgraph Deep_Learning[\"Deep Learning\"]\n                    Generative_AI[\"Generative AI\"]\n                end\n            end\n        end\n    end\n\n    %% Assign classes to subgraphs\n    class Intelligence intelligenceStyle;\n    class Artificial_Intelligence artificialStyle;\n    class Machine_Learning mlStyle;\n    class Deep_Learning deepStyle;\n    class Generative_AI innermostStyle;\n\n    %% Define the styling separately\n    classDef intelligenceStyle fill:#e0f7fa,stroke:#00796b,stroke-width:2px,color:#004d40;\n    classDef artificialStyle fill:#ffecb3,stroke:#ffb300,stroke-width:2px,color:#e65100;\n    classDef mlStyle fill:#d1c4e9,stroke:#7e57c2,stroke-width:2px,color:#311b92;\n    classDef deepStyle fill:#c8e6c9,stroke:#388e3c,stroke-width:2px,color:#1b5e20;\n    classDef innermostStyle fill:#ffccbc,stroke:#d84315,stroke-width:2px,color:#bf360c;"
  },
  {
    "objectID": "posts/2025-03-27-intelligence-to-genai/index.html#definitions",
    "href": "posts/2025-03-27-intelligence-to-genai/index.html#definitions",
    "title": "From Intelligence to Generative AI",
    "section": "Definitions",
    "text": "Definitions\nLet’s briefly define the terms in the image:\nIntelligence is the ability to solve problems, learn from experiences, reason, and adapt to new situations. It exists both in biological beings and, since the advent of computers, in artificial systems.\nArtificial Intelligence (AI) is about implementing intelligence in computers. Initially, AI was developed through “expert systems”, where humans manually encoded rules and logic (if a then b else c) to solve specific problems or make decisions. Broadly speaking, even a calculator could be considered an early form of AI due to its super-human calculation capabilities. However, these systems were effective only in tasks with clearly defined rules and struggled with complexity or unpredictability.\nMachine Learning (ML) advances AI by enabling computers to learn from data rather than relying exclusively on explicitly programmed logic. These models still depend on human-selected algorithms (such as Random Forest or K-Nearest Neighbors) but optimize themselves through exposure to data. Machine learning is widely used for tasks like predicting real estate prices, fraud detection, speech recognition, or personalized content recommendation. Despite their strengths, these models still require careful human input for algorithm selection and feature engineering, limiting adaptability in highly dynamic situations.\nDeep Learning (DL) is a subfield of machine learning that uses Deep Neural Networks (neural networks with multiple hidden layers) to automatically learn complex patterns directly from data. The process of training these networks has been famously termed “Software 2.0” [1] by Andrej Karpathy, highlighting how this approach eliminates the need for manual selection of algorithms or explicit feature engineering. Deep learning has achieved notable success in applications like image classification (distinguishing cats from dogs), recommendation systems (e.g., social media feeds), and natural language processing (translation). However, deep learning models typically lack interpretability and require substantial computational resources.\nGenerative AI leverages deep neural networks, specifically large language models (LLMs) like ChatGPT, to generate content such as text, images, and videos. These models have been trained on massive datasets, enabling them to approximate an understanding of both language structure and word meanings. As a result, LLMs can produce coherent, creative, and contextually relevant content in various styles. Furthermore, these models are often instruction-tuned, enabling them to perform diverse tasks based on natural language prompts.\nYou could also think of the path from intelligence to generative AI as a branching tree, as shown below - This is the way I usually walk through the previous illustration:\n\n\n\n\n\ngraph TD\n    A[Intelligence]\n    A --&gt; B[Biological Intelligence]\n    A --&gt; C[Artificial Intelligence]\n    C --&gt; D[\"Expert Systems&lt;br&gt;(Software 1.0)\"]\n    C --&gt; E[\"Machine Learning\"]\n    E --&gt; F[\"Machine Learning Algorithms&lt;br&gt;(e.g. Nearest Neighbors, Random Forests)\"]\n    E --&gt; G[\"Deep Learning&lt;br&gt;(Software 2.0)\"]\n    G --&gt; H[\"Task-Specific Models&lt;br&gt;(e.g. CNNs for Vision, RNNs for NLP)\"]\n    G --&gt; I[\"Generative AI&lt;br&gt;(Transformer-based)\"]\n\n    %% Apply the 'highlight' class to the nodes along the main path\n    class A,C,E,G,I highlight;\n    \n%% Define the styling for the highlighted path\nclassDef highlight fill:#d1e8ff,stroke:#1f78b4,stroke-width:2px,color:#0b3e68;"
  },
  {
    "objectID": "posts/2025-03-27-intelligence-to-genai/index.html#implementing-each-layer-from-scratch",
    "href": "posts/2025-03-27-intelligence-to-genai/index.html#implementing-each-layer-from-scratch",
    "title": "From Intelligence to Generative AI",
    "section": "Implementing Each Layer from Scratch",
    "text": "Implementing Each Layer from Scratch\nTo make these concepts more tangible, let’s implement each level from scratch using a simple example, finding an assumed linear pattern in some points plotted on a graph.\n\nIn the Intelligence section, I will hand the task over to you, leveraging your biological intelligence. You can fit a line to best match the points on the graph. This demonstrates how biological intelligence works, using intuition and experience to find a solution.\nIn the Artificial Intelligence (AI) sections, we will solve the problem by performing linear regression, finding the best-fitting line by minimizing the distances between the points and the line.\nIn the Machine Learning section, we will implement the gradient descent algorithm, which allows the computer to learn the best line to fit the data points by iteratively adjusting the parameters of the line based on the data it sees. You will see that the outcome closes in on the solution iteratively, the longer the model optimizes the parameters.\nIn the Deep Learning section, we let go of the assumed pattern of a line which needs to be fitted. Instead, we’ll use a neural network to learn the relationship between the points, allowing the AI to find the best solution purely from the data.\nIn the Generative AI section, we’ll simply present the problem directly to ChatGPT, leveraging a multimodal large language model. We’ll explore how Generative AI approaches the problem similarly to how a human would: interpreting the given task, selecting a suitable method, and generating the solution, all without explicit guidance on how to achieve it.\n\nThrough these implementations, we’ll clearly observe an evolution: Initially, we explicitly designed (or selected) a specific algorithm tailored to the problem at hand. At each subsequent level, we progressively let go of some assumptions and reduce explicit instructions, increasingly relying on the machine’s ability to discover generalizable solutions directly from data. This shift exemplifies the “bitter lesson” [2] famously articulated by Rich Sutton: general and scalable methods that depend more heavily on learning from data tend, over time, to outperform approaches relying predominantly on human-engineered rules and assumptions."
  },
  {
    "objectID": "posts/2025-03-27-intelligence-to-genai/index.html#preparation-data-setup",
    "href": "posts/2025-03-27-intelligence-to-genai/index.html#preparation-data-setup",
    "title": "From Intelligence to Generative AI",
    "section": "Preparation: Data Setup",
    "text": "Preparation: Data Setup\nBefore we implement each layer from scratch, let’s first create some example data. We’ll use a simple linear function as our baseline:\n\\(0.4x + 0.25\\)\nThis function serves as the “ground truth” we’ll aim to reconstruct. Next, we’ll generate randomized data points scattered around this line. Our goal will be to see how effectively each of our implementations can recover the underlying linear pattern.\nHere’s how the original linear function looks when plotted, and if you want to follow along by executing the code yourself, here you can find this blog post as a Jupyter notebook.\n\n\nCode\nfrom ipywidgets import interact\nfrom fastai.basics import *\n\nplt.rc('figure', dpi=90)\n\ndef plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n    x = torch.linspace(min,max, 100)[:,None]\n    if ylim: plt.ylim(ylim)\n\n    plt.grid(True)\n    # Draw solid zero lines\n    plt.axhline(0, color='black', linewidth=1)\n    plt.axvline(0, color='black', linewidth=1)\n    \n    plt.plot(x, f(x), color)\n    if title is not None: plt.title(title)\n\n\ndef f(x): return 0.4*x + 0.25\n\nplot_function(f, \"$0.4*x + 0.25$\")\n\n\n\n\n\n\n\n\n\n\nNext, let’s create the randomized points around the line.\n\n\nCode\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n\nnp.random.seed(3)\n\nx = torch.linspace(-2, 2, steps=20, dtype=torch.float64)[:, None]\ny = add_noise(f(x), 0.10, 0.15)\n\n# Create a figure and axes explicitly\nfig, ax = plt.subplots()\n\n# Plot on the axes\nax.scatter(x, y)\nax.grid(True)\nax.axhline(0, color='black', linewidth=1)\nax.axvline(0, color='black', linewidth=1)\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s also save the plotted points as a png-image, we will need it later in the chapter on Generative AI.\n\n\nCode\nimport io\nimport base64\n#from IPython.display import HTML\n\n# Save the figure (not the current plt state) to a BytesIO buffer\nbuffer = io.BytesIO()\nfig.savefig(buffer, format='png', bbox_inches='tight')\nbuffer.seek(0)  # Rewind the buffer to the beginning\n\n# Encode the buffer content to base64\nimage_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\nbuffer.close()\n\n# Display the image directly from the base64 string\n#HTML(f'&lt;img src=\"data:image/png;base64,{image_base64}\" alt=\"Generated Plot\" /&gt;')"
  },
  {
    "objectID": "posts/2025-03-27-intelligence-to-genai/index.html#implementation-1-biological-intelligence",
    "href": "posts/2025-03-27-intelligence-to-genai/index.html#implementation-1-biological-intelligence",
    "title": "From Intelligence to Generative AI",
    "section": "Implementation 1: Biological Intelligence",
    "text": "Implementation 1: Biological Intelligence\nLong before AI existed (and hopefully still today), humans used their own brains to solve problems. Now it’s your turn! Using your intuition, try manually fitting the best regression line to the points using the interactive plot below (or watch this video of me doing it):\n\n\nCode\ndef linear_function(a, b, x):\n    return a*x + b\n\ndef make_linear_function(a,b): return partial(linear_function, a,b)\n\n@interact(a=(-4.0, 4.0, 0.01), b=(-4.0, 4.0, 0.01))\ndef plot_manual_regression(a, b):  \n    plt.scatter(x,y) # plot points\n    manual_regression = make_linear_function(a, b)\n    plot_function(manual_regression, f\"${a:.4f}*x + {b:.4f}$\")\n\n\n\n\n\n  \nI expect the optimization was straightforward once you got a feel for what the two sliders do 😉. From my perspective, this simple experiment highlights several interesting points:\n\nIntuition over math: You solved this problem intuitively, without necessarily knowing the underlying mathematics—just as you can throw a ball without understanding physics. Essentially, you learned the solution directly from the data.\nIterative, trial-and-error approach: You arrived at the solution interactively through trial-and-error rather than directly or systematically, similar in spirit to how gradient descent algorithms find optimal solutions incrementally.\nLimited scalability: Manually solving this task took time, indicating that this approach isn’t scalable for larger, more complex problems.\nNon-deterministic outcomes: Your solution isn’t consistent or reproducible each time—there’s inherent variability (stochasticity) when humans solve problems intuitively.\n\nThese observations will reappear in various forms as we explore the next levels. Now let’s see how a machine tackles the same task."
  },
  {
    "objectID": "posts/2025-03-27-intelligence-to-genai/index.html#implementation-2-artificial-intelligence-ai",
    "href": "posts/2025-03-27-intelligence-to-genai/index.html#implementation-2-artificial-intelligence-ai",
    "title": "From Intelligence to Generative AI",
    "section": "Implementation 2: Artificial Intelligence (AI)",
    "text": "Implementation 2: Artificial Intelligence (AI)\nIn this chapter, we implement Simple Linear Regression to approximate a set of data points. Linear regression is a classical algorithm, and it represents an early approach to artificial intelligence. Think of it as a “software 1.0” or an expert system — not in the strict historical sense of the term, but as an example of how we initially designed algorithms by hand to mimic intelligent behavior.\nIn our implementation, we explicitly encode the rules for minimizing squared errors, handing over the problem to the computer to solve exactly as instructed. Although today it may seem a stretch to call this process “intelligent”, it still is an example of how we can achieve super-human performance leveraging a silicon-based implementation. Problem solving is algorithmic in this implementation and limited to exactly the problem at hand. In AI-terminology, it is very narrow.\n\n\nCode\nimport matplotlib.pyplot as plt\n\ndef plot_regression(x, y, y_pred):\n    \"\"\"\n    Plots the original data points and the fitted regression line.\n    \n    Parameters:\n    - x: array-like, the x-values of the data points\n    - y: array-like, the y-values of the data points\n    - y_pred: array-like, the predicted y-values from the regression model\n    \"\"\"\n    plt.rc('figure', dpi=90)\n    plt.scatter(x, y, label='Noisy Data', color='blue')\n    plt.plot(x, y_pred, label='Fitted Line', color='red')\n    plt.title('Linear Regression by Minimizing Sum of Squared Errors')\n    #plt.xlabel('x')\n    #plt.ylabel('y')\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\n# Convert x and y to NumPy arrays\nx_np = x.numpy()\ny_np = y.numpy()\n\n# Compute means of x_np and y_np\nx_mean = np.mean(x_np)\ny_mean = np.mean(y_np)\n\n# Compute the slope (m) using the closed-form solution:\nnumerator = np.sum((x_np - x_mean) * (y_np - y_mean))\ndenominator = np.sum((x_np - x_mean) ** 2)\nm = numerator / denominator\n\n# Compute the intercept (b)\nb = y_mean - m * x_mean\n\n# Compute the predicted values using our computed m and b\ny_pred = m * x_np + b\n\n# Call the plotting function with the computed values\nplot_regression(x_np, y_np, y_pred)\n\n# Output the computed coefficients\nprint(\"Slope (Coefficient):\", m)\nprint(\"Intercept:\", b)\n\n\n\n\n\n\n\n\n\nSlope (Coefficient): 0.37931796260451567\nIntercept: 0.20216576369094635\n\n\nAs expected, the computed result closely matches our original ground truth of \\(0.4x + 0.25\\), differing only slightly due to the introduced noise. Compared to the biological intelligence approach, this solution is significantly faster, perfectly deterministic (always producing the same result), and more precise. However, this accuracy comes with a trade-off: the solution is narrowly tailored to linear regression. Even slight variations in the problem statement would require a complete rewrite of the algorithm."
  },
  {
    "objectID": "posts/2025-03-27-intelligence-to-genai/index.html#implementation-3-machine-learning-ml",
    "href": "posts/2025-03-27-intelligence-to-genai/index.html#implementation-3-machine-learning-ml",
    "title": "From Intelligence to Generative AI",
    "section": "Implementation 3: Machine Learning (ML)",
    "text": "Implementation 3: Machine Learning (ML)\nThe essence of machine learning is learning directly from data. Let’s put this into practice by continuing to assume that the correct solution is a linear function, \\(ax + b\\) (no neural networks yet). However, rather than explicitly computing the parameters \\(a\\) and \\(b\\), we’ll approximate them using a general learning algorithm known as gradient descent.\nWe’ll begin by initializing parameters \\(a\\) and \\(b\\) to zero. The gradient descent algorithm will then iteratively nudge these parameters toward the best solution. In each iteration, we:\n\nPredict \\(y\\)-values from our current parameters (\\(a\\) and \\(b\\)) and the input \\(x\\)-values.\nCalculate the loss, measured as the mean squared error between our predicted \\(y\\)-values and the actual \\(y\\)-values.\nCompute the gradients of the loss with respect to \\(a\\) and \\(b\\), indicating how the parameters should be adjusted to minimize the loss.\n\nThis iterative process continues until the parameters converge close to the underlying linear function.\nAt this point, the specific details of implementation aren’t critical. The key takeaway is that gradient descent is a generic optimization algorithm (not limited to linear regression) that learns parameters directly from data. This makes our solution more adaptable compared to the explicit linear regression approach. We could easily replace our simple linear function with a more complex relationship, and gradient descent would still function effectively.\nHere, we already see the “bitter lesson” at play: The handcrafted, problem-specific algorithm is being replaced by a more general, data-driven learning method.\nLet’s see gradient descent in action (either run the code or watch this video).\n\n\nCode\nimport matplotlib.pyplot as plt\n\ndef plot_current_state(x, y, ab, loss, step):\n    \"\"\"\n    Plots the data, current fitted line, and predictions at the given step.\n    \n    Parameters:\n      x (Tensor): Input data x-values.\n      y (Tensor): Actual y-values.\n      ab (Tensor): Current parameters [a, b].\n      loss (float): Current loss value.\n      step (int): The current optimization step.\n    \"\"\"\n    #plt.figure(figsize=(6, 4))\n    plt.grid(True)\n    plt.axhline(0, color='black', linewidth=1)\n    plt.axvline(0, color='black', linewidth=1)\n\n    # Plot original data points\n    plt.scatter(x, y, label='Noisy Data', color='blue')\n\n    # Generate the current fitted line using the updated parameters\n    line_f = make_linear_function(ab[0].item(), ab[1].item())\n    x_vals = torch.linspace(-2.1, 2.1, 100)[:, None]\n    y_vals = line_f(x_vals)\n    plt.plot(x_vals, y_vals, label='Current Fit', color='red')\n\n    # Plot predictions for the current x values\n    y_pred_current = line_f(x)\n    plt.scatter(x, y_pred_current.detach(), label='Predictions', color='orange', s=30)\n\n    plt.title(f'Step {step}: Loss={loss:.4f}, a={ab[0].item():.3f}, b={ab[1].item():.3f}')\n    plt.legend()\n    plt.show()\n\n\n\n\nCode\nfrom IPython.display import clear_output\n\ndef mse(preds, acts): return ((preds - acts) ** 2).mean()\n\ndef linear_mse(params): \n    f = make_linear_function(*params)\n    return mse(f(x), y)\n\ndef run_optimizer_step(params, lr=0.01):\n    loss = linear_mse(params)\n    params.grad = None\n    loss.backward()\n    with torch.no_grad():\n        params -= params.grad * lr\n    return loss.item()\n\n# Reinitialize parameters\nab = torch.tensor([0., 0.], dtype=torch.float64, requires_grad=True)\n\n# Animation loop for visualization\nn_steps = 300\nfor i in range(n_steps):\n    loss = run_optimizer_step(ab, lr=0.01)\n    clear_output(wait=True)\n    plot_current_state(x, y, ab, loss, i+1)\n\n\n\n\n\n\n\n\n\nThe visualization of the gradient descent steps appears quite similar to our earlier (biological) approach, but there are some noteworthy differences:\n\nThe machine learning solution systematically moves toward the optimal result, whereas the biological approach is more random and intuitive.\nThe machine learning algorithm optimizes both parameters simultaneously. In contrast, humans typically adjust one parameter at a time, followed by iterative fine-tuning.\nThe machine learning solution is deterministic: it consistently arrives at the same solution each time it runs."
  },
  {
    "objectID": "posts/2025-03-27-intelligence-to-genai/index.html#implementation-4-deep-learning-dl",
    "href": "posts/2025-03-27-intelligence-to-genai/index.html#implementation-4-deep-learning-dl",
    "title": "From Intelligence to Generative AI",
    "section": "Implementation 4: Deep Learning (DL)",
    "text": "Implementation 4: Deep Learning (DL)\nWith deep learning approach, we use a neural network to learn the correlation between our random points. In the previous implementations, we learned the parameters of a the linear function from the points. Now we learn the whole function from the points, i.e. we do no longer assume that there is a linear correlation between the points. Consequently, we can learn more complex correlations between the points, and the final result will not be a linear function. This is a true software 2.0 approach, we just feed the data points into the neural network and let it learn the correlation between the points.\nFor this example, we will use a small neural network with just one hidden layer containing 20 neurons. The network’s weights are randomly initialized, and we will continue using mean squared error as our loss function and gradient descent to iteratively update the weights. As you can see, the training mechanism is the same as in the previous implementation, but we exchanged the linear function for a neural network which can learn more complex correlations between the points.\nTo see how the model learns, either run the code below or watch this video.\n\n\nCode\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\nimport torch\n\ndef create_layer(input_dim, output_dim):\n    \"\"\"Returns (weight, bias) for a fully connected layer.\"\"\"\n    # Weight: shape [input_dim, output_dim]\n    weight = torch.rand(input_dim, output_dim, dtype=torch.float64) - 0.5\n    weight.requires_grad_()\n\n    # Bias: shape [output_dim]\n    bias = torch.rand(output_dim, dtype=torch.float64) - 0.5\n    bias.requires_grad_()\n\n    return (weight, bias)\n\ndef plot_results(x_train, y_train, preds_train, x_dense, y_dense, step, loss):\n    \"\"\"Handles plotting of training data, predictions, and the neural net fit.\"\"\"\n    clear_output(wait=True)\n    plt.grid(True)\n    plt.axhline(0, color='black', linewidth=1)\n    plt.axvline(0, color='black', linewidth=1)\n    plt.scatter(x_train, y_train, label='Noisy Data', color='blue')\n    plt.scatter(x_train, preds_train, label='Predictions', color='orange', s=30)\n    plt.plot(x_dense, y_dense, label='Neural Net Fit', color='red')\n    plt.title(f'Step {step}: Loss={loss:.4f}')\n    plt.legend()\n    plt.show()\n\n\n\n\nCode\ndef forward_pass(xs, layer1, layer2):\n    \"\"\"\n    Performs a forward pass through the network.\n    Each layer is a tuple (weight, bias).\n    \"\"\"\n    w1, b1 = layer1\n    w2, b2 = layer2\n\n    hidden = xs @ w1 + b1\n    hidden = hidden.relu()\n    preds = hidden @ w2 + b2\n    return preds\n\ndef run_dl_optimizer_step(xs, ys, layer1, layer2, lr=0.005):\n    \"\"\"\n    Runs a single optimizer step with backpropagation.\n    Resets gradients, does forward pass, computes loss,\n    and updates both weights and biases.\n    \"\"\"\n    w1, b1 = layer1\n    w2, b2 = layer2\n\n    # Forward pass\n    preds = forward_pass(xs, layer1, layer2)\n    loss = ((preds - ys) ** 2).mean()\n\n    # Zero gradients for each parameter\n    w1.grad = None\n    b1.grad = None\n    w2.grad = None\n    b2.grad = None\n\n    # Backprop\n    loss.backward()\n\n    # Update weights & biases (SGD)\n    with torch.no_grad():\n        w1 -= w1.grad * lr\n        b1 -= b1.grad * lr\n        w2 -= w2.grad * lr\n        b2 -= b2.grad * lr\n\n    return loss.item()\n\ninput_dim = 1\nhidden_dim = 20\noutput_dim = 1\n\nlayer1 = create_layer(input_dim, hidden_dim)\nlayer2 = create_layer(hidden_dim, output_dim)\n\n# Clone x/y\nx_dl = x.clone().detach().type(torch.float64)\ny_dl = y.clone().detach().type(torch.float64)\n\n# Training + Animation\nn_steps = 200\nfor i in range(n_steps):\n    loss = run_dl_optimizer_step(x_dl, y_dl, layer1, layer2, lr=0.01)\n    \n    with torch.no_grad():\n        preds_train = forward_pass(x_dl, layer1, layer2)\n        x_dense = torch.linspace(-2.1, 2.1, 200)[:, None].type(torch.float64)\n        y_dense = forward_pass(x_dense, layer1, layer2)\n    \n    plot_results(x_dl, y_dl, preds_train, x_dense, y_dense, i+1, loss)\n\n\n\n\n\n\n\n\n\nAs you can see in the visualization, the neural network is able to learn the correlation between the points, and the final result is not a linear function. One might argue that using a neural net is an overkill for this simple task, but the point was to show how we can learn the correlation from the points simply from data without making any assumption about the underlying function. One downside is that the result is no longer fully interpretable, meaning that we cannot boil down the relation into a simple formula. Instead, we need to run a forward pass through the network to get the result.\nIn our example, the lack of interpretability feels like a disadvantage because we already know the underlying function. Yet, in real-world scenarios, the underlying relationships are rarely so simple. For example, could you precisely describe the difference between a cat and a dog using a set of rules? Probably not, but you still instinctively recognize one from the other. Similarly, neural networks learn such distinctions directly from data. The learned “logic” is implicitly encoded in the weights of the network. We can use these weights to make predictions, but the model typically cannot explicitly explain why it made a particular prediction."
  },
  {
    "objectID": "posts/2025-03-27-intelligence-to-genai/index.html#implementation-5-generative-ai-genai",
    "href": "posts/2025-03-27-intelligence-to-genai/index.html#implementation-5-generative-ai-genai",
    "title": "From Intelligence to Generative AI",
    "section": "Implementation 5: Generative AI (GenAI)",
    "text": "Implementation 5: Generative AI (GenAI)\nHow can we implement a generative AI (GenAI) to solve the problem of approximating a set of data points? We could give the LLM the data point and ask it to find the best approximation. Try to increase the difficulty a bit, to come back to the beginning. We will pass the png-file of the data point we previously created to the LLM and ask it to find the best approximation - very similar to the implementation via human intelligence.\nHow can we leverage Generative AI (GenAI) to approximate our set of data points? One straightforward approach is to provide the data points directly to a large language model (LLM) and ask it to determine the best-fitting solution. To increase the challenge and connect back to our initial biological intelligence implementation, we’ll provide an image (PNG) of the plotted data points to the LLM. The task for the model (same as to the human in implementation 1) is to interpret the visual input and approximate the best-fitting relationship purely from the image.\n\nprompt = \"\"\"Here is a screenshot of a scatter plot containing some data points.\nPlease analyze the image and provide the best approximation for these points.\nI need the actual solution, not only the way/code to solve it.\nFor any formulas that you return, it is crucial that you enclose\nLaTeX in `$`or `$$` so that the markdown is rendered correctly\"\"\"\n\n\n\nCode\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"o1\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": f\"{prompt}\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"},\n                },\n            ],\n        }\n    ],\n)\n\n\nThe following output of the model does not render correctly in the blog. Please refer to the Jupyter notebook version for a better visualization.\n\n\nCode\nfrom IPython.display import Markdown, display\n\n# Extract markdown content from the response\nmd_content = response.choices[0].message.content\n\n# Display the extracted markdown\ndisplay(Markdown(md_content))\n\n\nTo approximate the data from the scatter plot, we will assume a linear relationship of the form:\n[ y = mx + b ]\n\nStep 1: Extract Approximate Data Points\nFrom visual observation, the points in the scatter plot seem to be following a roughly linear trend. Approximate data points extracted:\n\n\n\n( x )\n( y )\n\n\n\n\n-2.0\n-0.8\n\n\n-1.8\n-0.5\n\n\n-1.5\n-0.3\n\n\n-1.2\n-0.2\n\n\n-1.0\n-0.1\n\n\n-0.7\n0.0\n\n\n-0.5\n0.0\n\n\n-0.2\n0.1\n\n\n0.0\n0.2\n\n\n0.2\n0.4\n\n\n0.5\n0.5\n\n\n0.8\n0.6\n\n\n1.0\n0.6\n\n\n1.2\n0.7\n\n\n1.5\n0.9\n\n\n1.8\n1.1\n\n\n2.0\n1.2\n\n\n\n\n\nStep 2: Compute the Best Fit Line\nUsing the least squares method, the equations for the slope ( m ) and intercept ( b ) are:\n[ m = ]\n[ b = ]\nSubstituting the summed values from the dataset:\n[ x = 0.1, y = 3.2, xy = 4.8, x^2 = 10.7, N = 17 ]\n[ m = = = ]\n[ b = = = ]\n\n\nStep 3: Approximate Equation\nThus, the best fit equation for the given data is:\n[ y 0.447x + 0.186 ]\nThis equation can be used for predictions or further analysis.\n\n\n\nLet’s visualize the result:\n\n\nCode\nfrom fastai.basics import *\n\nplt.rc('figure', dpi=90)\n\ndef plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n    x = torch.linspace(min,max, 100)[:,None]\n    if ylim: plt.ylim(ylim)\n\n    plt.grid(True)\n    # Draw solid zero lines\n    plt.axhline(0, color='black', linewidth=1)\n    plt.axvline(0, color='black', linewidth=1)\n    \n    plt.plot(x, f(x), color)\n    if title is not None: plt.title(title)\n\n\ndef f(x): return 0.447*x + 0.186\n\nplot_function(f, \"$0.447*x + 0.186$\")\nplt.scatter(x, y, label='Noisy Data', color='blue');\n\n\n\n\n\n\n\n\n\nThe result looks good, but this was clearly a challenging task for the LLM. gpt-4o failed miserably, and o1 also struggled. I have run the prompt (which I have kept simple for good reasons, without giving any hints) a few times, and the results were mixed. Please treat the response above as one of the better responses, specifically selected to show that the frontier models can solve this task even in a zero-shot approach.\nWhile the accuracy and determinism of the generative AI approach are lower compared to our previous methods, this experiment offers a glimpse of the future. The LLM demonstrated agency: It didn’t just solve the task, it independently determined how to approach it without explicit guidance. Moreover, no special formatting was needed. The model processed the input almost exactly as a human would, simply from an image of the plotted points. This highlights a major advantage of multimodal models: They accept more natural, human-like inputs compared to purely text-based models.\nOnce again, we see the “bitter lesson” in action: Specialized, task-specific implementations are gradually replaced by general-purpose models, trained solely from data, capable of flexibly solving a broad range of problems."
  },
  {
    "objectID": "posts/2025-03-27-intelligence-to-genai/index.html#conclusion",
    "href": "posts/2025-03-27-intelligence-to-genai/index.html#conclusion",
    "title": "From Intelligence to Generative AI",
    "section": "Conclusion",
    "text": "Conclusion\nOur journey started with a slide showing a drill-down from intelligence to artificial intelligence, machine learning, deep learning, and finally to generative AI. We inspected every level in detail and even implemented it from scratch.\nInitially, we started with explicitly designed, human-crafted algorithms tailored precisely to specific tasks. Gradually, we moved towards increasingly general and scalable approaches, relying less on explicit human guidance and more on data-driven learning methods.\nThe following table compares the characteristics of each implementation:\n\n\n\n\n\n\n\n\n\n\n\n\nApproach\nAccuracy\nDeterminism\nScalability\nInterpretability\nComputational Cost\nHuman Effort\n\n\n\n\nBiological Intelligence\nMedium\nLow\nLow\nMedium\nLow\nHigh\n\n\nArtificial Intelligence (Linear Regression)\nHigh\nHigh\nLow\nHigh\nLow\nMedium\n\n\nMachine Learning (Gradient Descent)\nHigh\nHigh\nMedium\nHigh\nMedium\nMedium\n\n\nDeep Learning (Neural Network)\nHigh\nHigh\nHigh\nLow\nHigh\nLow\n\n\nGenerative AI (Multimodal LLM)\nMedium/Variable\nLow\nHigh\nMedium\nHigh\nLow\n\n\n\nThis comparison highlights important trade-offs: While simpler methods (like linear regression) excel at interpretability and computational efficiency, they lack flexibility and scalability. In contrast, sophisticated techniques (deep learning and generative AI) achieve greater flexibility, generalization, and adaptability at the expense of interpretability, determinism, and increased computational requirements.\nOur exploration also illustrated the “bitter lesson” by Rich Sutton: General-purpose, scalable methods trained directly from data consistently outperform highly specialized, manually engineered solutions in the long term. Yet, the most advanced model isn’t always the best solution for every task. Selecting the appropriate AI method depends on clearly understanding your specific requirements, resources, and desired outcomes.\nIn summary, as you approach AI implementation decisions, remember that although generative AI and deep learning dominate today’s discussions, simpler or intermediate methods can often be more effective, explainable, and resource-efficient, depending on your particular problem and context."
  },
  {
    "objectID": "posts/2025-03-27-intelligence-to-genai/index.html#references",
    "href": "posts/2025-03-27-intelligence-to-genai/index.html#references",
    "title": "From Intelligence to Generative AI",
    "section": "References",
    "text": "References\n[1] Karpathy, A. (2017). Software 2.0\n[2] Sutton, R. (2019). The Bitter Lesson"
  },
  {
    "objectID": "posts/2025-10-24-managing-python-environments/index.html",
    "href": "posts/2025-10-24-managing-python-environments/index.html",
    "title": "Managing Python Environments",
    "section": "",
    "text": "I have a confession to make… Ever since I set up my Mac, I’ve been happily pip-installing all the Python packages I ever needed into the base environment. Foreseeably, this resulted in a dependency nightmare, followed by a dependency deadlock. The honest analysis of my setup was that I didn’t understand what I was doing, I followed the recommendations and I had focused on learning how to build my projects. I have no regrets, but now it was time to take a step back and learn more about managing what I had installed.\nIn the spirit of what Jeremy was teaching regarding the initial Fast.AI setup, I wanted to build a system that allows me to continue working in an iterative and explorative way, but which is more stable and manageable in the long term. I ended up doing a fresh Miniforge installation and I set up a few environments in an automated way. This blog post is a summary of my activities and background information I learned along the way."
  },
  {
    "objectID": "posts/2025-10-24-managing-python-environments/index.html#installations-vs.-environments",
    "href": "posts/2025-10-24-managing-python-environments/index.html#installations-vs.-environments",
    "title": "Managing Python Environments",
    "section": "Installations vs. Environments",
    "text": "Installations vs. Environments\nLet’s start with some basic terminology and an as-is analysis. If you’re familiar with the basics, feel free to skip ahead to the next section.\nOn any system running Python, there is at least one installation and one environment, but what is the difference between the two? In a nutshell, the installation is the base layer that includes the Python interpreter, on which you can have one or more environments. The installation contains all the tooling for creating and running environments, and this installation is specific to the operating system or underlying hardware it is running on: For example, Windows or macOS (on Intel or Apple Silicon).\nTo understand how these pieces fit together, let’s first examine the installation layer. Then we will see how environments build upon it.\n\nWhat is a Python installation?\nMore specifically, a Python installation is a self-contained directory tree that provides:\n\na Python executable (the interpreter)\nits standard library (e.g., os, sys, json, etc.)\ntools to manage packages or environments (e.g., pip, conda, venv)\na way to compile or locate C-extensions for that platform: This means it can either compile C code using the system’s compiler, or download pre-built binaries (wheels) that match your specific architecture (e.g., Apple Silicon vs Intel).\n\nAn installation is tied to the operating system:\n\nIt’s built for a specific CPU architecture and OS layout.\nThe OS can “see” it via a path like /opt/homebrew/bin/python3 or /Users/chrwittm/miniforge3/bin/python.\n\nSo, an installation is a real physical presence on disk, known to the OS, with binaries that can be executed directly.\n\n\nWhat is a Python Environment (in conda/mamba)?\nBuilding on an installation, a Python environment is a sandboxed workspace with its own Python interpreter and packages. Creating environments for the Miniforge installation is done using conda or mamba (a faster conda for package resolution). Each environment is represented by a directory that contains:\n\nits own Python interpreter binary: With conda/mamba Python itself is treated just like another package, therefore any environment can have its own version (unlike venv, which always uses the parent installation’s Python version)\nits own site-packages folder for installed libraries (e.g. numpy or pandas)\nits own activation script: When you run conda activate myenv it changes PATH and sets some environment variables to tell the shell: “Use this Python and these packages.”\n\nIn short: environments share the underlying toolchain while maintaining complete independence for Python versions and packages."
  },
  {
    "objectID": "posts/2025-10-24-managing-python-environments/index.html#as-is-analysis-how-many-installations-are-there",
    "href": "posts/2025-10-24-managing-python-environments/index.html#as-is-analysis-how-many-installations-are-there",
    "title": "Managing Python Environments",
    "section": "As-is analysis: How many installations are there?",
    "text": "As-is analysis: How many installations are there?\nThere is not just one Python installation on any given Mac, there are multiple.\nIn my case, there were (and still are) 3 installations. Here is where to find them and what they are good for.\n\nApple’s Python stub at /usr/bin/python3\n\nIt’s tied to Xcode/Command Line Tools and can change with macOS updates.\nDo not change this installation\n\nHomebrew Python at /opt/homebrew/bin/python3\n\nIt’s tied to Homebrew, and it’s installed via Homebrew (either explicitly or as a dependency)\nDo not change this installation\n\nYour own base environment e.g. at ~/miniforge3\n\nYour own Python installation\nIt is the default environment used in the terminal.\nThis environment should be clean - a rule I broke, which we’re going to fix.\n\n\nBefore starting with the installation task, let’s dive a bit deeper and understand what the base environment actually is and does."
  },
  {
    "objectID": "posts/2025-10-24-managing-python-environments/index.html#what-is-the-base-environment",
    "href": "posts/2025-10-24-managing-python-environments/index.html#what-is-the-base-environment",
    "title": "Managing Python Environments",
    "section": "What is the base environment?",
    "text": "What is the base environment?\nWhen you launch the Terminal on macOS (after installing Miniforge), the base environment is typically activated as the default environment.\nThe base environment lives directly in ~/miniforge3/ while all other environments are created under ~/miniforge3/envs/&lt;name&gt;/.\nWhen you activate the base environment, conda prepends its bin directory to the shell’s PATH variable, like this:\nexport PATH=\"/Users/chrwittm/miniforge3/bin:$PATH\"\nThis ensures that when you type python, pip, or conda, the shell finds and executes the versions from the base environment, not from macOS or Homebrew.\nTo reiterate a subtle point: Activation doesn’t start Python (no binaries are executed), it just changes your shell’s environment variables so that all Python-related commands now point to the environment’s specific directory.\nNow that we understand what base is and how it works, the question becomes: what should we actually use it for?"
  },
  {
    "objectID": "posts/2025-10-24-managing-python-environments/index.html#the-goal-a-clean-base-environment",
    "href": "posts/2025-10-24-managing-python-environments/index.html#the-goal-a-clean-base-environment",
    "title": "Managing Python Environments",
    "section": "The goal: A clean base environment",
    "text": "The goal: A clean base environment\nThe only installation you should modify is your own, in my case, Miniforge. The other installations are managed by macOS or Homebrew.\nIn the Miniforge installation, the base environment should remain clean. Its only purpose is to create and manage other environments. In other words, it should be a clean, ARM-native Python ‘factory’ on macOS to manage all your environments.\nMy initial setup contains 3 environments:\n\nA playground environment which takes the role of what base was for me previously, an environment where I can freely experiment with packages, but which I can re-create whenever necessary\nA quarto-blogging environment for writing this blog\nA fast.ai environment (fastai-latest), which also contains pytorch, so that I can run all the fast.ai projects I have built in the past and want to continue building in the future.\n\nComment: Ok"
  },
  {
    "objectID": "posts/2025-10-24-managing-python-environments/index.html#re-installing-miniforge",
    "href": "posts/2025-10-24-managing-python-environments/index.html#re-installing-miniforge",
    "title": "Managing Python Environments",
    "section": "Re-installing Miniforge",
    "text": "Re-installing Miniforge\nAfter diving into the theory, let’s (re-)install Miniforge which is a quick and easy process. First, I’ll describe how to remove an existing Miniforge installation (my case), followed by doing the installation. If you start fresh, just skip the optional steps.\n\nBackup existing environments that you want to keep (if you have any)\nIf you want to keep specific environments, export them first (replace myenv_backup with a descriptive name):\nconda env export --no-builds &gt; myenv_backup.yml\n\n\nRemove Old Installation (if you have one)\nDelete the old Miniforge directory, including all environments\nconda deactivate 2&gt;/dev/null || true    # deactivate, suppressing error messages\nrm -rf ~/miniforge3                     # deleting Miniforge\n\n\nDownload Latest Miniforge Installer (for me Apple Silicon)\nThe Miniforge repo has installers for different platforms. For Apple Silicon Macs, we need the ARM64 version (Miniforge3-MacOSX-arm64.sh).\nIn the terminal, run the following commands:\ncd ~/Downloads\ncurl -LO https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh\n\n\nRun the Installer\nTo start the installation, run:\nbash Miniforge3-MacOSX-arm64.sh\nAccept the terms and conditions, the default path, and also answer “yes” to the question “Do you wish to update your shell profile to automatically initialize conda?”. This will update your ~/.zshrc file to initialize conda any time you launch a terminal.\nEither restart the terminal (recommended) or run source ~/.zshrc. As a result, you should see that the base environment is active, indicated by (base) in front of your terminal prompt.\nAfterwards you can verify the installation by running:\nwhich python        # should return the miniforge3 path\npython --version    # should return the Python version, e.g. Python 3.12.11"
  },
  {
    "objectID": "posts/2025-10-24-managing-python-environments/index.html#creating-the-playground",
    "href": "posts/2025-10-24-managing-python-environments/index.html#creating-the-playground",
    "title": "Managing Python Environments",
    "section": "Creating the playground",
    "text": "Creating the playground\nWe want the base environment to stay clean. As a substitute, let’s create a playground environment that we can use as the default environment instead.\nmamba create -n playground python=3.12 -c conda-forge\nThis command creates a new isolated environment named playground inside the Miniforge installation, installs Python 3.12 and its dependencies from the conda-forge community channel (a trusted source of packages).\nTo activate the playground use this command:\nconda activate playground\n\nAutomatic activation of playground environment\nSince the goal is to keep the base environment clean, let’s activate the playground by default when you start a new terminal.\nTo achieve this, we first need to deactivate the automatic activation of the base environment:\nconda config --set auto_activate_base false\nTo automatically activate the playground, add the following line to your ~/.zshrc:\nconda activate playground\nTo see the effect, either restart the terminal (recommended) or run source ~/.zshrc. As a result, you should see that the playground environment is active, indicated by (playground) in front of your terminal prompt."
  },
  {
    "objectID": "posts/2025-10-24-managing-python-environments/index.html#creating-environments-with-configuration-files",
    "href": "posts/2025-10-24-managing-python-environments/index.html#creating-environments-with-configuration-files",
    "title": "Managing Python Environments",
    "section": "Creating environments with configuration files",
    "text": "Creating environments with configuration files\nOur playground isn’t completed yet. It would be helpful to have basic packages like numpy or pandas preinstalled, so that when exploring new ideas, I don’t have to install them each time. Additionally, to help prevent a future dependency nightmare, it would be great to be able to reproduce a new playground with minimal effort: If at any point in the future, the playground would be messed up, I could just refresh it without any second thoughts.\nThe way to do this is to create an environment from YAML-configuration file. I started with the basic collection of packages, which will grow over time. (More on how to manage the evolution in the next section.)\nname: playground\n\nchannels:\n  - conda-forge     # primary open-source channel with Apple Silicon builds\n\ndependencies:\n  - python=3.12\n  - ipython         # enhanced REPL\n  - jupyterlab      # notebook / lab environment\n  - ipywidgets      # interactive widgets for Jupyter\n  - numpy           # numerical computing\n  - pandas          # data analysis\n  - matplotlib      # plotting library\nTo (re-)create the playground environment, you can simply run:\nmamba env create -f playground.yml"
  },
  {
    "objectID": "posts/2025-10-24-managing-python-environments/index.html#managing-environments-via-a-github-repo",
    "href": "posts/2025-10-24-managing-python-environments/index.html#managing-environments-via-a-github-repo",
    "title": "Managing Python Environments",
    "section": "Managing environments via a GitHub repo",
    "text": "Managing environments via a GitHub repo\nTo keep the YAML files for creating the playground and other environments consistent across multiple machines, I created a GitHub repo called python-environments.\nAdditionally, I implemented a creation script for each environment to fully automate the process. The script also takes care of tasks like making sure the respective environment is not active during the refresh.\nHere is the playground creation script as an example.\n#!/usr/bin/env bash\nset -e  # exit on first error\n\necho \"🔄 Rebuilding Playground environment...\"\n\n# Source shared safety check\nsource ../helpers/ensure_no_env.sh  # Check that no environment is currently active\n\n# Remove old environment if it exists\nconda remove -n playground --all -y || true\n\n# Recreate from YAML\nmamba env create -f playground.yml\n\necho \"✅ Playground environment rebuilt successfully.\"\necho \"\"\necho \"To activate it, run:\"\necho \"conda activate playground\"\nAssuming you are in the root directory of the repo, you can run the script like this:\ncd playground\nbash rebuild_playground.sh"
  },
  {
    "objectID": "posts/2025-10-24-managing-python-environments/index.html#final-thoughts",
    "href": "posts/2025-10-24-managing-python-environments/index.html#final-thoughts",
    "title": "Managing Python Environments",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nJust like any good project, resolving the dependency nightmare took a bit longer than initially expected. But I’m very happy with the result because I have transitioned from following setup instructions to having created my own end-to-end Python environment setup workflow which supports my experimental working style.\nInfrastructure projects like this one can feel like a distraction from building, but they’re actually investments for the future. This new setup not only solved my initial problem (the dependency nightmare), but it will make me a lot more productive in the future.\nAs a side note, I first encountered Python environments when deploying services to the cloud as I have described in a previous blog post. With my new setup, my local workflow now mirrors the cloud workflow, which should make future cloud projects smoother.\nOne last reflection: This experience reinforced my view that you should learn skills when you need them. Learning about creating different Python environments at this point in time was the right time for me. Had I spent time on it earlier, my overall enthusiasm would most likely have been a lot less. I find learning to solve specific problems much more effective than learning something just in case I might need it in the future."
  },
  {
    "objectID": "posts/2025-12-12-sap-rpt-1-hello-world/index.html",
    "href": "posts/2025-12-12-sap-rpt-1-hello-world/index.html",
    "title": "Welcome SAP-RPT-1! What is it? How can you try it out?",
    "section": "",
    "text": "SAP announced SAP-RPT-1 at TechEd 2025 in Berlin. What is it good for, and how can you try it out? In this blog post, I’ll walk you through the announcement and show you how to get started, both in a no-code approach and in Python."
  },
  {
    "objectID": "posts/2025-12-12-sap-rpt-1-hello-world/index.html#what-is-sap-rpt-1",
    "href": "posts/2025-12-12-sap-rpt-1-hello-world/index.html#what-is-sap-rpt-1",
    "title": "Welcome SAP-RPT-1! What is it? How can you try it out?",
    "section": "What is SAP-RPT-1?",
    "text": "What is SAP-RPT-1?\nRPT stands for Relational Pre-Trained Transformer which SAP wants us to pronounce as ‘rapid one’. To make sure this is pronounced correctly, they even write out phonetic transcription [ˈræpɪd] [wʌn] - so no excuses here 😉.\nIf you are looking for a quick executive summary, check out Philipp Herzig’s Keynote presentation which explains that RPT-1 is a new type of AI model that is optimized for making predictions on tabular data. Unlike LLMs which generate the next word (token) for a text sequence, RPT-1 predicts the next field in a table row. Putting this in a practical example, Philipp mentions that RPT-1 can predict, for example, delivery times or customer churn, essentially replacing traditional ML models like XGBoost or Random Forest with a single RPT-1 model.\nFor now, SAP-RPT-1 comes in 3 flavors:\n\nSAP-RPT-1-Small: A small model optimized for super-fast predictions\nSAP-RPT-1-Large: A larger model optimized for highest accuracy\nSAP-RPT-1-OSS: An “open source” version for everybody to learn (Github / Huggingface)\n\nThe way Philipp talked about the OSS version, I assume that this is also the version which is available in the RPT Playground, your starting point to try out RPT-1 in a no-code environment."
  },
  {
    "objectID": "posts/2025-12-12-sap-rpt-1-hello-world/index.html#trying-out-sap-rpt-1-in-the-rpt-playground",
    "href": "posts/2025-12-12-sap-rpt-1-hello-world/index.html#trying-out-sap-rpt-1-in-the-rpt-playground",
    "title": "Welcome SAP-RPT-1! What is it? How can you try it out?",
    "section": "Trying out SAP-RPT-1 in the RPT Playground",
    "text": "Trying out SAP-RPT-1 in the RPT Playground\nThe easiest way to get started with RPT-1 is the RPT Playground. SAP has prepared some data, but you can also upload your own tabular data (CSV files) and use RPT-1 to make predictions on it. The trick for making predictions with RPT-1 is to mask the field you want to predict with [PREDICT]. Once you click on the “Predict” button, RPT-1 will fill in the masked field with its prediction.\nWhile SAP’s prepared use cases work well, you’ll get a better sense of RPT-1’s capabilities by trying it with your own data. To help you get started, I’ve prepared the example data from the coding tutorial (allowing you to predict sales groups from Product, Price, Customer, Country) in a CSV file for you to download. You can easily modify this file in Excel (or any other editor), save as CSV and try your own data in the RPT Playground. If you’d prefer to start simple, try out the example CSV file I have prepared for you.\nIf you’re not a coder, simply skip the coding section and jump right to the conclusion."
  },
  {
    "objectID": "posts/2025-12-12-sap-rpt-1-hello-world/index.html#trying-out-sap-rpt-1-in-python",
    "href": "posts/2025-12-12-sap-rpt-1-hello-world/index.html#trying-out-sap-rpt-1-in-python",
    "title": "Welcome SAP-RPT-1! What is it? How can you try it out?",
    "section": "Trying out SAP-RPT-1 in Python",
    "text": "Trying out SAP-RPT-1 in Python\nIf you want to get your hands dirty with code, I have prepared a Jupyter Notebook that shows in detail how to use RPT-1.\n\nPrerequisites\nAssuming you have access to the RPT Playground and that you have downloaded the Jupyter notebook, you need to fetch your access token from the bottom of the playground page.\nTo avoid storing your credentials in the notebook, copy & paste the token (RPT_TOKEN) into a .env file in the same folder where you have stored the Jupyter notebook. It should look like this:\nRPT_TOKEN=\"eyJhbGciOiJIUzI1\" # truncated, your token is a lot longer\nOnce you’ve done that, you’re ready to run the notebook.\n\n\nPreparing Test Data\nHere’s the test data in the format RPT-1 expects:\n\n\nCode\npayload = {\n    \"prediction_config\": {\n        \"target_columns\": [\n            {\n                \"name\": \"SALESGROUP\",\n                \"prediction_placeholder\": \"[PREDICT]\"\n                # \"task_type\": \"classification\" or \"regression\" can be specified here if needed\n            }\n        ]\n    },\n    \"index_column\": \"ID\",\n    \"rows\": [\n        {\n            \"ID\": \"1001\",\n            \"PRODUCT\": \"Tablet\",\n            \"PRICE\": 599.00,\n            \"CUSTOMER\": \"TechStart Inc\",\n            \"COUNTRY\": \"USA\",\n            \"SALESGROUP\": \"[PREDICT]\"\n        },\n        {\n            \"ID\": \"1002\",\n            \"PRODUCT\": \"Standing Desk\",\n            \"PRICE\": 325.50,\n            \"CUSTOMER\": \"Workspace Solutions\",\n            \"COUNTRY\": \"Germany\",\n            \"SALESGROUP\": \"[PREDICT]\"\n        },\n        {\n            \"ID\": \"1003\",\n            \"PRODUCT\": \"Workstation\",\n            \"PRICE\": 1450.00,\n            \"CUSTOMER\": \"Enterprise Systems Ltd\",\n            \"COUNTRY\": \"Canada\",\n            \"SALESGROUP\": \"Enterprise Solutions\"\n        },\n        {\n            \"ID\": \"1004\",\n            \"PRODUCT\": \"Laptop Pro\",\n            \"PRICE\": 1899.99,\n            \"CUSTOMER\": \"Business Corp\",\n            \"COUNTRY\": \"UK\",\n            \"SALESGROUP\": \"Enterprise Solutions\"\n        },\n        {\n            \"ID\": \"1005\",\n            \"PRODUCT\": \"Gaming Laptop\",\n            \"PRICE\": 1250.00,\n            \"CUSTOMER\": \"Digital Ventures\",\n            \"COUNTRY\": \"USA\",\n            \"SALESGROUP\": \"Enterprise Solutions\"\n        },\n        {\n            \"ID\": \"1006\",\n            \"PRODUCT\": \"Smart Watch\",\n            \"PRICE\": 299.99,\n            \"CUSTOMER\": \"Gadget Store\",\n            \"COUNTRY\": \"Australia\",\n            \"SALESGROUP\": \"Consumer Electronics\"\n        },\n        {\n            \"ID\": \"1007\",\n            \"PRODUCT\": \"Ergonomic Chair\",\n            \"PRICE\": 445.00,\n            \"CUSTOMER\": \"Office Outfitters\",\n            \"COUNTRY\": \"France\",\n            \"SALESGROUP\": \"Office Furniture\"\n        },\n        {\n            \"ID\": \"1008\",\n            \"PRODUCT\": \"Storage Array\",\n            \"PRICE\": 3500.00,\n            \"CUSTOMER\": \"CloudTech Systems\",\n            \"COUNTRY\": \"Singapore\",\n            \"SALESGROUP\": \"Data Infrastructure\"\n        },\n        {\n            \"ID\": \"1009\",\n            \"PRODUCT\": \"Network Switch\",\n            \"PRICE\": 175.50,\n            \"CUSTOMER\": \"ConnectIT\",\n            \"COUNTRY\": \"Japan\",\n            \"SALESGROUP\": \"Networking Devices\"\n        }\n    ]\n}\n\n\nFor better readability, here is the data in table format:\n\n\nCode\nimport pandas as pd\n\nsample_data = pd.DataFrame(payload[\"rows\"])\nsample_data\n\n\n\n\n\n\n\n\n\nID\nPRODUCT\nPRICE\nCUSTOMER\nCOUNTRY\nSALESGROUP\n\n\n\n\n0\n1001\nTablet\n599.00\nTechStart Inc\nUSA\n[PREDICT]\n\n\n1\n1002\nStanding Desk\n325.50\nWorkspace Solutions\nGermany\n[PREDICT]\n\n\n2\n1003\nWorkstation\n1450.00\nEnterprise Systems Ltd\nCanada\nEnterprise Solutions\n\n\n3\n1004\nLaptop Pro\n1899.99\nBusiness Corp\nUK\nEnterprise Solutions\n\n\n4\n1005\nGaming Laptop\n1250.00\nDigital Ventures\nUSA\nEnterprise Solutions\n\n\n5\n1006\nSmart Watch\n299.99\nGadget Store\nAustralia\nConsumer Electronics\n\n\n6\n1007\nErgonomic Chair\n445.00\nOffice Outfitters\nFrance\nOffice Furniture\n\n\n7\n1008\nStorage Array\n3500.00\nCloudTech Systems\nSingapore\nData Infrastructure\n\n\n8\n1009\nNetwork Switch\n175.50\nConnectIT\nJapan\nNetworking Devices\n\n\n\n\n\n\n\nAs you can see, we have 4 attribute columns: Product, Price, Customer, Country. Our target column is the Sales Group. Therefore, the sales group is masked in 2 lines with the value [PREDICT], so that RPT-1 can demonstrate that it can fill in the missing values.\n\n\nRunning the predictions\nTo run the predictions, we will use the requests library to send an HTTP POST request to the RPT-1 API endpoint. The request will include our test data and the authorization token.\n\n\nCode\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nauth_token = os.getenv(\"RPT_TOKEN\")\n\n\n\n\nCode\nimport requests\n\nurl = \"https://rpt.cloud.sap/api/predict\"\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {auth_token}\"\n}\n\nresponse = requests.post(url, json=payload, headers=headers)\n\n\n\n\nAnalyzing the results\nLet’s take a look at the results returned by RPT-1. Here are the predictions in the raw JSON format:\n\n\nCode\nimport json\n\ndata = response.json()\npreds = data[\"prediction\"][\"predictions\"]\nprint(json.dumps(preds, indent=2, ensure_ascii=False))\n\n\n[\n  {\n    \"ID\": 1001,\n    \"SALESGROUP\": [\n      {\n        \"confidence\": null,\n        \"prediction\": \"Enterprise Solutions\"\n      }\n    ]\n  },\n  {\n    \"ID\": 1002,\n    \"SALESGROUP\": [\n      {\n        \"confidence\": null,\n        \"prediction\": \"Office Furniture\"\n      }\n    ]\n  }\n]\n\n\nLet’s merge these predictions back into the tabular format to see the results more clearly.\n\n\nCode\npreds_df = sample_data.copy(deep=True)\n\npreds_df[\"ID\"] = preds_df[\"ID\"].astype(int)\n\nfor pred in preds:\n    row_idx = int(pred[\"ID\"])\n    predicted_value = pred[\"SALESGROUP\"][0][\"prediction\"]\n    preds_df.loc[preds_df[\"ID\"] == row_idx, \"SALESGROUP\"] = predicted_value\n\npreds_df\n\n\n\n\n\n\n\n\n\nID\nPRODUCT\nPRICE\nCUSTOMER\nCOUNTRY\nSALESGROUP\n\n\n\n\n0\n1001\nTablet\n599.00\nTechStart Inc\nUSA\nEnterprise Solutions\n\n\n1\n1002\nStanding Desk\n325.50\nWorkspace Solutions\nGermany\nOffice Furniture\n\n\n2\n1003\nWorkstation\n1450.00\nEnterprise Systems Ltd\nCanada\nEnterprise Solutions\n\n\n3\n1004\nLaptop Pro\n1899.99\nBusiness Corp\nUK\nEnterprise Solutions\n\n\n4\n1005\nGaming Laptop\n1250.00\nDigital Ventures\nUSA\nEnterprise Solutions\n\n\n5\n1006\nSmart Watch\n299.99\nGadget Store\nAustralia\nConsumer Electronics\n\n\n6\n1007\nErgonomic Chair\n445.00\nOffice Outfitters\nFrance\nOffice Furniture\n\n\n7\n1008\nStorage Array\n3500.00\nCloudTech Systems\nSingapore\nData Infrastructure\n\n\n8\n1009\nNetwork Switch\n175.50\nConnectIT\nJapan\nNetworking Devices\n\n\n\n\n\n\n\nFor comparison, here is the original sample data before predictions:\n\n\nCode\nsample_data\n\n\n\n\n\n\n\n\n\nID\nPRODUCT\nPRICE\nCUSTOMER\nCOUNTRY\nSALESGROUP\n\n\n\n\n0\n1001\nTablet\n599.00\nTechStart Inc\nUSA\n[PREDICT]\n\n\n1\n1002\nStanding Desk\n325.50\nWorkspace Solutions\nGermany\n[PREDICT]\n\n\n2\n1003\nWorkstation\n1450.00\nEnterprise Systems Ltd\nCanada\nEnterprise Solutions\n\n\n3\n1004\nLaptop Pro\n1899.99\nBusiness Corp\nUK\nEnterprise Solutions\n\n\n4\n1005\nGaming Laptop\n1250.00\nDigital Ventures\nUSA\nEnterprise Solutions\n\n\n5\n1006\nSmart Watch\n299.99\nGadget Store\nAustralia\nConsumer Electronics\n\n\n6\n1007\nErgonomic Chair\n445.00\nOffice Outfitters\nFrance\nOffice Furniture\n\n\n7\n1008\nStorage Array\n3500.00\nCloudTech Systems\nSingapore\nData Infrastructure\n\n\n8\n1009\nNetwork Switch\n175.50\nConnectIT\nJapan\nNetworking Devices"
  },
  {
    "objectID": "posts/2025-12-12-sap-rpt-1-hello-world/index.html#conclusion",
    "href": "posts/2025-12-12-sap-rpt-1-hello-world/index.html#conclusion",
    "title": "Welcome SAP-RPT-1! What is it? How can you try it out?",
    "section": "Conclusion",
    "text": "Conclusion\nAs we can see, RPT-1 has filled in the missing sales groups based on the patterns contained in the context, i.e., the other entries. Essentially, the non-masked rows serve as few-shot examples that RPT-1 uses for in-context learning. This behavior is comparable to an LLM imitating examples you provide in a prompt. Here’s a simple example of such a prompt:\nWrite an executive summary for the Lord of the Rings.\n\nHere is an example for Romeo and Juliet:\nBoy and girl fall in love, in the end they both die.\nIf you try this prompt, you might get a result like this:\nGuy inherits ring. Walks a lot. Destroys it.\nSimilarly, RPT-1 uses the non-masked rows as examples to predict the masked values. The key difference is that RPT-1 does not semantically generate text, but it predicts the most likely value for the masked fields based on the patterns it has learned during training.\nOf course, the example we used is very simple, and it will be interesting to see how RPT-1 performs on more complex datasets. For now, I hope you are all set to try SAP-RPT-1 with your own data. Happy experimenting!"
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html",
    "href": "posts/2022-11-26-mnist/index.html",
    "title": "MNIST, the ‘Hello World’ of Computer Vision",
    "section": "",
    "text": "After Cat vs. Dog, this is the next challenge for me in computer vision: Building on chapter 4 of the book, I challenged myself to implement a model based on the MNIST dataset, as recommended as further research.\nThis challenge is also available as a Kaggle competition, and I found this bit of competitive spirit to add some spice to the project. Additionally, it broadened the spectrum of implementation topics, because training a model is one thing, but using it for meaningful predictions in equally important and also required some effort. Last, but not least, submitting the results was a nice way to check if the results are actually correct. As predicted: “This was a significant project and took you quite a bit of time to complete! I needed to do some of my own research to figure out how to overcome some obstacles on the way”.\nI took an iterative approach, following a similar path as for working on the Titanic-Challenge:\nIt was a challenging project, and I learned a lot on the way. Below are some key points and learnings."
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html#the-fast.ai-version-without-a-submission",
    "href": "posts/2022-11-26-mnist/index.html#the-fast.ai-version-without-a-submission",
    "title": "MNIST, the ‘Hello World’ of Computer Vision",
    "section": "The Fast.AI version without a submission",
    "text": "The Fast.AI version without a submission\nBy now this is pretty straight-forward for me. I just copy&pasted a few lines of code to do the training, and I was able to create a decent model very quickly in this notebook.\nThe catch with this version is, however, that it is not ready for the mass data load: 28.000 predictions need to be done in the competition - something which I addressed in my second iteration.\nAdditionally, I found it interesting that the MNIST dataset was already pushing the limits of my laptop: The training time of about 40 minutes was ok, but it is already quite a burden if it needs to be done multiple times. Moving the learning to Paperspace, training on a free GPU, was 10x faster (no surprise). Since I like to still have everything locally, it is quite convenient moving files back and forth via git, also for the .pkl-files. This way the training can be done with GPU, and the inference can be done locally. Interestingly, in all my other notebooks, local performance was not an issue. (But I expect that to change in future other projects)"
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html#resubmitting-working-with-the-csv-files",
    "href": "posts/2022-11-26-mnist/index.html#resubmitting-working-with-the-csv-files",
    "title": "MNIST, the ‘Hello World’ of Computer Vision",
    "section": "Resubmitting: Working with the csv-files",
    "text": "Resubmitting: Working with the csv-files\nI found not very elegant to just convert the csv-files to png-images. That seems convenient, but a bit wasteful. Therefore, I re-implemented the process this notebook.\nIt was surprisingly difficult to convert the data into the right formal in memory so that the learner would accept the image. But finally I was able to convert a PIL.Image.Image to fastai.vision.core.PILImage. As usual with these things, once it was done, it looks easy.\nNot surprisingly, but a nice way to verify the result, the submission score was the same:"
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html#my-first-submission",
    "href": "posts/2022-11-26-mnist/index.html#my-first-submission",
    "title": "MNIST, the ‘Hello World’ of Computer Vision",
    "section": "My first submission",
    "text": "My first submission\nWhen I first downloaded the Kaggle data, I was quite surprised to see that the download did not contain any image files, but just 2 large csv-files. Since I only knew how to handle images, I simply converted the data to png-images in this notebook.\nOnce that was done, I could take the model trained before in my first notebook to make my first submission. In this notebook, I took the converted images and collected the predictions. I found the result of 99.4% quite impressive."
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html#the-from-scratch-version",
    "href": "posts/2022-11-26-mnist/index.html#the-from-scratch-version",
    "title": "MNIST, the ‘Hello World’ of Computer Vision",
    "section": "The from-scratch version",
    "text": "The from-scratch version\nDoing it all from scratch was an interesting learning exercise because I think that I already had a good understanding of what needed to be done even before implementing it. But, as it turns out, there is this tremendous difference between thinking that you understood it, and actually implementing it. There is a lot of fine print, and you have to pay attention to the details: Formatting the data, getting it into the correctly shaped tensors, and implementing the gradient descent. Irrespective of what I had learned/understood before, this has greatly deepened and solidified by implementing the MNIST challenge.\nSome minor mysteries remain, which I also documented in the notebook, if you can guide me how to fix them, please let me know.\nThe finale result of my from scratch-version is not up to the first implementation with resnet18, but I am proud of it for other reasons ;)."
  },
  {
    "objectID": "posts/2024-06-21-how-llms-are-trained/index.html",
    "href": "posts/2024-06-21-how-llms-are-trained/index.html",
    "title": "How LLMs are Trained",
    "section": "",
    "text": "Large Language Models (LLMs) like ChatGPT have become invaluable tools for many of us. But what lies beneath the surface of these sophisticated models? How have they been trained to deliver such a seemingly magical experience? In this blog post, we will conceptually explore the training methods used for LLMs and dive into some details. In line with Jeremy Howard’s Hackers’ Guide to Language Models [1], our journey begins with the foundational ULMFit Paper [2], which introduced a three-step approach to language model training. After a brief detour into tokenization, we will examine how this three-step approach is implemented today by analyzing the LLaMA papers and other sources. By exploring the different phases, you will gain a deeper understanding of the inner mechanics of large language model training and build intuition on how these models learn a broad range of skills by simply predicting the next token in a sequence of tokens. In conclusion, we will see that today’s large language model training involves at least four steps, with an optional fifth step."
  },
  {
    "objectID": "posts/2024-06-21-how-llms-are-trained/index.html#the-ulmfit-paper",
    "href": "posts/2024-06-21-how-llms-are-trained/index.html#the-ulmfit-paper",
    "title": "How LLMs are Trained",
    "section": "The ULMFiT Paper",
    "text": "The ULMFiT Paper\nToday’s approach to training large language models dates back to the 2018 ULMFit Paper [2] by Jeremy Howard and Sebastian Ruder. They describe a transfer learning approach to natural language processing (NLP). Although the paper’s intention was to train a classifier, such as generating ratings from an IMDb movie review, the approach is remarkably similar to training a large language model today.\nThe ULMFiT method consists of three main steps:\n\nPre-Training: The language model is trained on a large corpus of text to learn the essence of language, its structure, the meaning of words, and real-world concepts. In the ULMFiT paper, they used the Wikitext-103 dataset containing about 100 million tokens of Wikipedia text. Back in the days, this was considered to be large 😉. This phase is not task-specific, instead, it teaches the model general language understanding. Today, this step is still the foundation for training large language models, but the models are trained on significantly larger and more diverse datasets.\nFine-Tuning: In this phase, the model is fine-tuned on domain-specific data, such as movie reviews, to learn the specifics of the domain. Originally, this was done using discriminative fine-tuning, where different layers of the model are trained with varying learning rates. This fine-tuning phase conceptually persists in modern language model training, although the training methods have changed to become instruction tuning.\nClassifier Fine-Tuning: Finally, the model is trained for the specific task, such as classifying movie reviews, using labeled examples. Originally, this was done by adding some final layers to the model and carefully freezing/unfreezing the pre-trained model to prevent it from forgetting its pre-training. Again, this additional fine-tuning step still exists in today’s language model training, but this has become Reinforcement Learning by Human Feedback (RLHF) which fine-tunes a model according to human preference.\n\n\n\n\n\nThe 3-step ULMFiT Training Approach"
  },
  {
    "objectID": "posts/2024-06-21-how-llms-are-trained/index.html#the-3-steps-of-training-large-language-models",
    "href": "posts/2024-06-21-how-llms-are-trained/index.html#the-3-steps-of-training-large-language-models",
    "title": "How LLMs are Trained",
    "section": "The 3 Steps of Training Large Language Models",
    "text": "The 3 Steps of Training Large Language Models\nWhile the techniques have evolved, training a large language models today still fits a 3-step approach of pre-training, followed by two fine-tuning steps: Instruction tuning and reinforcement learning by human feedback (RLHF). When you read through the papers or announcements of recent LLMs (ChatGPT or Llama3), you will often find references to pre-training and fine-tuning, but many details are hidden between the lines about the process. The Llama2 paper [3], not surprisingly due to Meta’s open-source approach, is one of the best resources to read about the process. Andrej Karpathy’s video Intro to Large Language Models [4] also excellently explains the different stages of the training process. Let me break it down for you.\n\n\n\n\n\nsequenceDiagram\n    participant DP as Data Provider\n    participant TP as Training Pipeline\n    participant Human as Human Annotator\n\n    %% Step 1: Language Model Pre-Training\n    Note over DP,TP: Step 1: Language Model Pre-Training\n    DP -&gt;&gt; TP: Provide Large General-Domain Corpus for Pre-Training\n    TP --&gt;&gt; DP: Return Pre-Trained Language Model (Foundation Model)\n\n    %% Step 2: Instruction Tuning\n    Note over DP,TP: Step 2: Instruction Tuning\n    DP -&gt;&gt; TP: Provide Instruction-Focused Corpus for Fine-Tuning\n    TP --&gt;&gt; DP: Return Instruction-Tuned Language Model (Assistant Model)\n\n    %% Step 3: Reinforcement Learning with Human Feedback (RLHF)\n    Note over DP,Human: Step 3: Reinforcement Learning with Human Feedback (RLHF)\n    DP -&gt;&gt; TP: Prompting Instruction-Tuned Language Model\n    TP -&gt;&gt; Human: Provide Generated Outputs for Feedback\n    Human -&gt;&gt; TP: Provide Feedback on Model Outputs\n    TP --&gt;&gt; DP: Return RLHF-Tuned Language Model (Chat Model)\n\n\n\n\n\n\nWhile the 3-step approach is exactly what happens when training an LLM, we need to step back and talk about tokenization first. Since computers / computer scientists start counting at 0 and since the tokenization happens before pre-training, let’s call it “step 0”."
  },
  {
    "objectID": "posts/2024-06-21-how-llms-are-trained/index.html#step-0-tokenization",
    "href": "posts/2024-06-21-how-llms-are-trained/index.html#step-0-tokenization",
    "title": "How LLMs are Trained",
    "section": "Step 0: Tokenization",
    "text": "Step 0: Tokenization\nYou often hear the phrase that “LLMs are trained to predict the next word of a sentence”. This is a convenient simplification we can easily relate to, and we will use this simplification for the rest of this blog post, but let’s face reality for a few sentences: Today’s LLM actually thinks in terms of tokens which can either be letters, a combination of letters or whole words, maybe something like a syllable. Tokens are the building blocks, the atoms of the LLM-generated language, but they also determine the network architecture and they are the currency of LLMs. After all, LLMs API are usually billed based on tokens.\nFor the purpose of this blog post, we need to understand that the vocabulary of a tokenizer, i.e. the individual tokens, is learned from data. Many tokenizers use Byte Pair Encoding (BPE) to determine which combination of letters is a token. Simplistically said, BPE starts with a vocab consisting of just letters. Then it finds the most common combination of letters and assigns it to a new token. This process is repeated until the size of the vocab (a pre-determined number) is reached.\nThe vocabulary of a tokenizer determines the capabilities of a model, for example, by including or excluding characters other than the Latin alphabet, an implicit decision about language support is already made. Additionally the vocabulary of the tokenizer is an indicator for how efficient an LLM can be in a language because there is a tradeoff between vocab size and computational efficiency. Very small vocabularies (only letters, for example) are similarly inefficient as having millions of tokens (like all entries of all encyclopedia of all supported languages).\nThe composition of the vocab already contains information on how language is structured, because the most frequent words get their own tokens. Infrequent or combined words are split up in more than one token or even only letters. The following illustration generated via Tiktokenizer show, for example, how “token” is a token, but “tokenization” consists of 2 tokens.\n\n\n\n\nTokenization illustrated via Tiktokenizer\n\n\nTokenization, however, is a process which needs to be finished before the LLM is trained, because the LLM only learns on tokenized data. Additionally, the size of the vocabulary also influences the network architecture, because for every token of the vocab an embedding is learned during LLM training. These embeddings are numerical representations of the tokens which contain semantic meaning. This is easier imagined for words, and I explained embeddings and their meanings in detail in this blog post.\nFor the remainder of this blog post let’s stick to the convenient lie [5] that LLMs are next word predictors (not next token predictors) because it makes thinking about LLM training more intuitive. Before we move on to pre-training, one final note on tokenization: The video Let’s build the GPT Tokenizer by Andrej Karpathy [6] is very accessible for learning more about tokenization."
  },
  {
    "objectID": "posts/2024-06-21-how-llms-are-trained/index.html#step-1-pre-training",
    "href": "posts/2024-06-21-how-llms-are-trained/index.html#step-1-pre-training",
    "title": "How LLMs are Trained",
    "section": "Step 1: Pre-Training",
    "text": "Step 1: Pre-Training\nThe first step in training large language models is pre-training. During pre-training, the model “reads” vast amounts of text, in a self-supervised way to learn the following:\n\nGrammar and Syntax: Understanding the rules and structure of language.\nSemantics: Learning the meanings of words and phrases in various contexts.\nWorld Knowledge: Accumulating general information about the world, which is embedded in the text.\n\nThis is easily said, but what happens under the hood, i.e., in the net? While the learning mechanisms of babies and LLMs function in completely different ways, you could nonetheless say that LLMs learn about language like children. Children have highly adaptive brains, but they initially know nothing about language. By paying attention to the world around them (and receiving instruction and correction), they learn the meanings of words, the structure of sentences, and grammar — no textbook required. Similarly, LLMs begin with randomly initialized neural networks and are exposed to vast amounts of text. They use mechanisms like the transformer architecture with its attention mechanism to learn grammar and syntax, semantics, and gain world knowledge. Let’s unpack the various aspects of LLM pre-training in the following sections.\n\nHow large is “vast”?\nWhen we say that during pre-training, the model “reads” vast amounts of text, how large is “vast” actually?\nLet’s try a human-centric comparison: How much can a human read in a lifetime compared to the training set of an LLM? For a first reference point, let’s calculate the amount of text a human could read in a lifetime:\n\n\nCode\n# Constants\nhours_per_day = 2\ndays_per_year = 365\nyears = 50\nreading_speed_wpm = 250\naverage_book_length_words = 85000\n\n# Calculate total number of hours spent reading\ntotal_hours_per_year = hours_per_day * days_per_year\ntotal_hours = total_hours_per_year * years\n\n# Calculate total number of words read\ntotal_minutes = total_hours * 60\ntotal_words = total_minutes * reading_speed_wpm\n\n# Calculate total number of books read\ntotal_books = total_words / average_book_length_words\n\n# Calculate total number of tokens read (1 token per 0.75 words)\ntokens_per_word = 1 / 0.75\ntotal_tokens = total_words * tokens_per_word\n\n# Print the results with formatted numbers\nprint(f\"Using the parameters defined in the code, a human might read for {total_hours:,} hours in a lifetime.\")\nprint(f\"To put this into numbers, a human would have read:\")\nprint(f\" - {total_books:,.2f} books\")\nprint(f\" - {total_words:,} words\")\nprint(f\" - {int(total_tokens):,} tokens\")\n\n\nUsing the parameters defined in the code, a human might read for 36,500 hours in a lifetime.\nTo put this into numbers, a human would have read:\n - 6,441.18 books\n - 547,500,000 words\n - 730,000,000 tokens\n\n\nLet’s compare this to the training data of a large language models. The following table contains data from Wikipedia with some additions. It turns out that even the “ancient” GPT-2 was trained on almost an order of magnitude more tokens than a human can read in a lifetime:\n\n\n\n\n\n\n\n\n\n\nModel\nRelease Date\nParameters\nTraining Data (tokens)\nMultiplier (compared to human)\n\n\n\n\nHuman\nOngoing\nIncomparable\n730 million\n1x\n\n\nGPT-1\nJune 2018\n117 million\n5 billion\n6.85x\n\n\nGPT-2\nFebruary 2019\n1.5 billion\n10 billion\n13.70x\n\n\nGPT-3\nJune 2020\n175 billion\n300 billion\n410.96x\n\n\nGPT-4\nMarch 2023\nEstimated 1.5 trillion\nunknown\nunknown\n\n\n\nThe Llama Paper [7], gives even more insights as llama was trained on publicly available data. The following table shows the model training data, and converting this into tokens, the paper states that “LLaMA-33B and LLaMA-65B were trained on 1.4T tokens. The smaller models were trained on 1.0T tokens.”\n\n\n\n\nPre-training data used to train Meta’s llama models (Source)\n\n\nState-of-the-art LLMs have therefore seen several orders of magnitude more text than the average human, but what does this actually mean? How can LLMs learn from just reading text?\n\n\nWhat does “reading” mean?\nWhen we say that a large language model reads text, that is oversimplifying the process. Instead, the training pipeline samples batches from the training set text, and the size of this batch is the context window of the large language model. The text is therefore split up into these batches, each of which is used to train the model on next word prediction. Conveniently, one batch of text can be turned into many training examples by masking different parts of the text [8]. Typically, it is performed like this:\n\nBatch: “The cat sat on the mat”\nTraining example 1: “The cat sat on the …”\nTraining example 2: “The cat sat on …”\nTraining example 3: “The cat sat …”\nTraining example 4: “The cat …”\nTraining example 5: “The …”\n\nFor each of these training examples, the model attempts to predict the next word. Although it may not be obvious at first sight, a model needs extensive knowledge to make an accurate prediction. First of all, it needs some understanding of grammar. For example, words like pronouns (“he”, “she”, “it”) would be poor predictions, and while adjectives (“beautiful”, “hard-working”) might make sense in some contexts, they are not ideal here. We are looking for a noun, but not just any noun, “sky” or “idea” would be poor choices. Instead, the model needs to have knowledge about the world to determine that we need a physical object associated with cats (implying that the model can understand what a cat is) and the surrounding context. Therefore, words like “couch”, “bed”, or “mat” are likely candidates. As you can see, the model needs to know quite a bit about the world to make a good prediction.\nConveniently, the actual word the model should predict is known, allowing the model to learn in a self-supervised way by evaluating its predicted word against the actual masked word. The prediction is not just a single word but a probability distribution of all the words in the model’s (i.e. the tokenizer’s) vocabulary. The most likely word in this distribution is chosen as the prediction, check out this video from 3Blue1Brown [5] to see it in action.\nWhen calculating the loss (via cross-entropy loss), the model therefore receives nuanced feedback on its performance rather than just a simple right or wrong answer. This feedback helps the model understand how close its prediction was to the actual word, and it can adjust its internal parameters accordingly. This adjustment process, known as gradient descent, updates the model weights via back-propagation to improve future predictions. This means that all the neurons in the many layers of the neural network receive feedback on how well they contributed to the final prediction. This way the model not only learns to predict the exact next word but learns to understand the context and semantics of the language better.\nFor example, if the model predicted “number” instead of “mat,” the loss would be high, and parameters in the network would receive more significant updates compared to a semantically better prediction like “couch.” In both cases, however, the model learns from its predictions. As the gradients flow back, more semantic meaning is baked into the token embeddings. Additionally, the key and query matrices of the transformer architecture learn about how words are related to each other, enhancing the model’s understanding of language and its semantics.\nSumming up this section, “reading” is a lot more than just acknowledging the text. It essentially dissects the text into training examples, and the model learns by trying to predict masked words in the texts.\n\n\nLearning Semantics, Grammar and Syntax, and World Knowledge\nWhat exactly happens within the different matrices during gradient descent is basically impossible to grasp, but can we build some intuition on how an LLM can acquire knowledge even though it primarily functions as a next word predictor? Let’s start with the raw embeddings of each token. Even when carefully trained, the meaning of each token remains ambiguous, even when we think in terms of words. A “model,” for instance, can refer to a machine learning model, a fashion model, or a toy.\nThe transformer architecture and the attention mechanism allow the tokens to “communicate” with each other. Through the key and query matrices, the model learns which words/tokens are more related to each other. However, we should not think about this in the way of a look-up table, but rather that the patterns like adjective-noun relations are learned by the model so that during inference the respective embeddings can be updated. Let’s take the context of “machine learning model”: During training time, the model has learned that some words in certain locations are likely to imprint additional meaning on other words. This knowledge is baked into the LLM so that at inference it can pass on the meaning of “machine learning” to “model”.\nThe model does not only learn how to update the semantics, but also an understanding of the structure of language starts to emerge. For example, a large language model learns to use the third person “s” when it sees many sentences like “Tom likes chocolate.” However, the LLM does not explicitly learn the rule itself (“In the present tense, add an”s” to verbs when the subject is a singular third person (he, she, it)“). Instead, the model learns to mimic the pattern of the training data by selecting the most likely form of the verb”to like” during inference.\nThe more training data the model is exposed to and the larger the neural network, the more implicit knowledge the model can store. By reading about chocolate, the model learns that it is usually brownish, contains cocoa and sugar, tastes good, and perhaps some chocolate brands. This is how a model can store knowledge. Again, this is not a look-up table, but the model learns the gestalt of chocolate. Think of it like reading the Wikipedia article about chocolate: Neither you nor the LLM can recite the article word-for-word afterwards, but both you and the LLM will have learned something about chocolate. Andrej Karpathy calls this a “lossy compression”, and this is one factor why models hallucinate: They simply pick the most likely next word/token based on what they have read and understood about chocolate. We should not think about the learning process for LLMs as a deterministic process like filling a bunch of database tables with facts, but it is a much softer and subtle process, just like our human learning experience.\nTherefore, an LLM can easily continue a prompt like “Chocolate is made of …” based on everything it has learned, and it will likely produce a good answer. However, there is a risk that some information may be inaccurate. As we move into the next training phase, we will get to know strategies on how these risks can be minimized.\nBefore we move on to the next training stage, we need to talk about the term “Foundation Model”, which is used to describe a model delivered by pre-training. Foundation models are not the models which are used by ChatGPT. Rather, a foundation model can become a chat model, but we still need 2 more training steps (Remember the 3-step training approach!). Nonetheless, the terminology can be confusing because you can select “GTP-3.5” or “GPT-4” as a model in the OpenAI-UI. This, however, only means that the Chat Model you use has been trained on the respective foundation model. Also, please be wary of marketing material which talk about “Foundation Models” because usually they also refer to different types of models. Foundation models have obtained broad knowledge, and they can continue/extent prompts by adding the most likely next word. To create an experience like ChatGPT, a foundation model needs to be fine-tuned further by instruction tuning and reinforcement learning by human feedback (RLHF), the topics we will explore in the next sections."
  },
  {
    "objectID": "posts/2024-06-21-how-llms-are-trained/index.html#step-2-instruction-tuning",
    "href": "posts/2024-06-21-how-llms-are-trained/index.html#step-2-instruction-tuning",
    "title": "How LLMs are Trained",
    "section": "Step 2: Instruction Tuning",
    "text": "Step 2: Instruction Tuning\nTo transition a foundation model into a practical application like ChatGPT, the next step is instruction tuning. During instruction tuning, the model is introduced to various tasks such as answering questions, summarization, and more. This training helps to transition the model from simply generating text to becoming an assistant. Instruction tuning therefore is a supervised learning process because the dataset consists of labeled data where the responses in the dataset are the labels for the prompts/questions. The following illustration shows an example from the OpenOrca dataset which builds on the FLAN collection to illustrate what such a dataset looks like:\n\n\n\n\nExample from the OpenOrca Dataset\n\n\nInstruction tuning has been shown in many papers (for example, the FLAN paper [9]) to lead to better zero-shot performance in tasks like answering questions and summarization compared to the performance of foundation models. Knowing that the performance of foundation models can be significantly improved by using few-shot examples, i.e., demonstrations of how a task should be performed, one way to think about instruction tuning and building some intuition on why it is useful is to imagine it as baking few-shot examples into the foundation model, so you do not need to provide examples all the time.\nSince large language models are stateless and cannot learn from conversations (i.e., their model parameters are not updated during use (inference)), here is an analogy to help you build intuition on how instruction tuning is beneficial: Imagine two university graduates with the same final grade. One is freshly out of university without any working experience, while the other has completed several internships during their breaks. When asked on their first day of work to write meeting minutes, the graduate who only attended university classes will likely write down what happened in the meeting. In contrast, the one with internship experience, having written meeting minutes before, will likely structure the minutes in a better way, noting action items, tasks, and timelines in a well-organized way.\nThe former corresponds to a foundation model, being well-educated but inexperienced. The latter received the same pre-training but was also instruction-tuned during internships. Hence, the latter knows on day one what is expected when asked to perform certain tasks. Similarly, instruction-tuned language models have seen tasks and their expected outcomes, therefore they do not require few-shot examples to perform such tasks. Additionally, overall zero-shot performance increases because the models have gained a broader understanding of how tasks should be performed.\nInstruction Tuning is probably the least-talked about phase of LLM training, but it lays the foundation for turning foundation models into assistants. Further fine-tuning is, however, needed to create an experience like ChatGPT, namely reinforcement learning by human feedback (RLHF)."
  },
  {
    "objectID": "posts/2024-06-21-how-llms-are-trained/index.html#step-3-reinforcement-learning-by-human-feedback-rlhf",
    "href": "posts/2024-06-21-how-llms-are-trained/index.html#step-3-reinforcement-learning-by-human-feedback-rlhf",
    "title": "How LLMs are Trained",
    "section": "Step 3: Reinforcement Learning by Human Feedback (RLHF)",
    "text": "Step 3: Reinforcement Learning by Human Feedback (RLHF)\nReinforcement learning by human feedback (RLHF) is the final training stage in creating chatbots like ChatGPT. It helps to further align the model to human preferences and improve instruction following and overall quality. The following chart from the Llama2 paper [3] shows the full training flow. We have already discussed pre-training, resulting in the foundation model Llama 2. In the paper, instruction tuning (labeled as “supervised fine-tuning”) and RLHF are combined into “fine-tuning,” but you can clearly see that instruction tuning precedes the RLHF phase. The result of instruction tuning is a first version of Llama 2-Chat, which is iteratively fine-tuned with RLHF:\n\n\n\n\nHow Llama2 was trained, illustration from the Llama2 paper\n\n\nThe Llama2 paper describes the RLHF process as follows. First, a human annotator writes a prompt, and the model generates two responses (A and B). Subsequently, the annotator then ranks the responses in two dimensions:\n\nHelpfulness / Overall Preference: The annotator chooses which response is better on a scale of “significantly better,” “better,” “slightly better,” or “negligibly better/unsure.”\nSafety: The annotator assesses whether each response is safe or not, considering guidelines about harmful or unsafe content.\n\nThese labels are not directly used to give feedback to the model on its performance. Instead they are used to train two reward models, one for helpfulness and one for safety. Using two reward models is a design choice by Meta, and the paper makes a compelling argument that balancing the two is challenging. Safe responses (“Sorry, I cannot help you with this request”) may not be helpful, while helpful responses (“Here is how you can destroy humanity in 3 easy steps”) might not be safe. The reward models have the same architecture as the foundation model but they have been trained via transfer learning not to generate the next token for a prompt but to output a scalar value (a number) to rank the prompt for their respective dimension, hence learning human preference. Determining the scalar is just a regression task. The reward score is an important input variable for the reinforcement learning process because it encodes the human preference, therefore turning the reinforcement learning process into RLHF. The actual reinforcement learning is implemented via Proximal Policy Optimization (PPO).\nFollowing the flow of the chart above, the iterative fine-tuning of the model is not done based on the responses selected and scored by humans but on a different thread. The model responses for further fine-tuning are ranked and selected by “Rejection Sampling” and “Proximal Policy Optimization” (PPO). Rejection Sampling might better be called “Best Response Selection,” but the term has its roots in statistics. In this context, rejection sampling means that the model responses are ranked by the reward models’ scores. Responses that are below a certain threshold score are discarded, while those above the threshold are used for further training.\nProximal Policy Optimization (PPO) is a mechanism from reinforcement learning that allows systems to update their policy based on rewards. Oversimplified, you can think of it like lane keeping in autonomous driving. If the model keeps the car centered in the lane, it is rewarded, reinforcing its policy (i.e. its neural net parameters). If the model tends to exit the lane, it is punished, allowing it to update the policy. In the context of large language model training, the loss function of the chat model is replaced with a PPO loss function. This PPO loss function has the following input parameters:\n\nThe prediction of the chat model. In this context, the parameters of the neural net that generated the response are called the policy.\nThe reward scores from the reward models\nFrom the generated response and the reward scores, a so-called “advantage” is calculated, which describes how much better or worse the generation is compared to a defined baseline. Reusing the lane-keeping example, the advantage determines where in the lane the generated response is, right in the middle of the lane or drifting out of the lane.\n\nBy replacing the loss function used in previous learning stages with the PPO loss function, the model can generate the forward pass during prediction and calculate a loss that includes injected parameters of human preference. During backpropagation, the gradients can flow back into the model, updating the model parameters with human preference.\nIf all of this was too technical and complicated, let’s revisit the analogy of the university graduate being asked to write meeting minutes on their first day of work. Even the one with previous internship experience will most likely not write perfect meeting minutes. Instead, following up on the first meetings, an experienced colleague will (hopefully) review the meeting minutes with the graduate to provide feedback on how meeting minutes are specifically written at their company, which elements should be expanded, noted in a different format, etc. This iterative feedback corresponds to RLHF, during which a language model is further aligned with human expectations through multiple rounds of feedback from human reviewers. The feedback is quantified into rewards, which the model uses to adjust its behavior, improving overall response quality, aligning to human preferences, and adopting safety standards."
  },
  {
    "objectID": "posts/2024-06-21-how-llms-are-trained/index.html#summary-of-steps-1-to-3",
    "href": "posts/2024-06-21-how-llms-are-trained/index.html#summary-of-steps-1-to-3",
    "title": "How LLMs are Trained",
    "section": "Summary of Steps 1 to 3",
    "text": "Summary of Steps 1 to 3\nLet’s recap the key points of the training stages of large language models to start wrapping up this blog post. The first step in large language model training is self-supervised pre-training, during which a randomly initialized model learns from scratch without any human supervision. We have discussed how pre-training works and built up some intuition on how a model can learn not only grammar and syntax but also semantics and how a model can gain real-world knowledge. Instruction tuning transitions the model from mere text completion into an assistant. By baking in instructions and problem-solving skills, this phase also reduces the need for giving instructions/examples to the model. To put this in different words, the model’s zero-shot performance increases by learning few-shot variants during instruction tuning. Finally, reinforcement learning by human feedback (RLHF) aligns the model with human preferences and further improves performance to create an experience like ChatGPT.\nFor a more detailed summary, here is a table comparing the three different types of models created by the different training stages.\n\n\n\n\n\n\n\n\n\nFeature/Aspect\nFoundation Models\nInstruction-Tuned Models\nChat Models\n\n\n\n\nPurpose\nGeneral language understanding and generation\nEnhanced understanding of specific instructions\nOptimized for interactive and coherent dialogues\n\n\nText Completion\nCapable of completing or continuing a given piece of text\nCan provide task-specific completions and instructions\nFocused on generating coherent replies in a dialogue\n\n\nExample Model\nGPT-3 (pre-trained)\nGPT-3 (InstructGPT variant)\nGPT-3 (ChatGPT)\n\n\nTraining Data\nDiverse, large-scale text data\nTask-specific instructions and prompts\nSelf-generated responses and reward scores based on human feedback\n\n\nTraining Method\nSelf-supervised learning\nSupervised learning\nReinforcement learning by human feedback (RLHF)\n\n\nTraining Effort (Computation)\nExtremely high, requires massive computational resources\nHigh, significant but less than initial pre-training\nHigh, significant due to continuous tuning\n\n\nIteration Cycle\nLong, major updates are infrequent\nModerate, updates occur as needed for new tasks\nFrequent, continuous updates for improvement\n\n\nAlignment / AI Safety\nGeneral ethical considerations\nEthical alignment with specific tasks in mind\nHigh focus on ethical AI, safety, and alignment\n\n\nInteractivity\nLow, not specifically designed for interactivity\nModerate, responds to specific tasks/instructions\nHigh, designed for dynamic and interactive conversations\n\n\nUse Cases\nBroad, including text generation, summarization, translation\nTask-specific applications like question answering, summarization\nCustomer service, virtual assistants, interactive chatbots"
  },
  {
    "objectID": "posts/2024-06-21-how-llms-are-trained/index.html#step-4-optional-rag-or-additional-fine-tuning",
    "href": "posts/2024-06-21-how-llms-are-trained/index.html#step-4-optional-rag-or-additional-fine-tuning",
    "title": "How LLMs are Trained",
    "section": "Step 4 (optional): RAG or Additional Fine Tuning",
    "text": "Step 4 (optional): RAG or Additional Fine Tuning\nWhile steps 0 to 3 create extremely useful and knowledgeable models, there is still room for improvement. Depending on the use case, you might choose to enhance model performance by using one of the following techniques:\n\nRetrieval Augmented Generation (RAG): The knowledge of large language models is limited to their (extensive) training data. If you ask out-of-domain questions, you either do not get an answer, or the model starts to hallucinate. RAG is a technique that gives the model access to additional data sources, grounding it in data relevant to the prompt. RAG typically involves semantic matching of the prompt with the content of a vector database. In a previous blog post, I implemented a simple RAG scenario that gives a llama2 model access to the blog my wife and I wrote about our world trip in 2017/2018. Since we are talking about how models are trained, it is important to note that RAG is not a training technique that updates the model itself, i.e., the model parameters. RAG is “just” a sophisticated method for in-context learning.\nFine-Tuning: Chat models can be fine-tuned via additional training runs. Unlike RAG, which gives a model access to additional data, fine-tuning can be deployed to teach additional skills to large language models. The training methods are the same as those we have discussed before. Examples of such fine-tuning include training a large language model on proprietary datasets, for example, company-specific documents to make the model a better assistant for customer service or technical support."
  },
  {
    "objectID": "posts/2024-06-21-how-llms-are-trained/index.html#conclusion-large-language-models-are-trained-in-5-steps",
    "href": "posts/2024-06-21-how-llms-are-trained/index.html#conclusion-large-language-models-are-trained-in-5-steps",
    "title": "How LLMs are Trained",
    "section": "Conclusion: Large Language Models are Trained in 5 Steps",
    "text": "Conclusion: Large Language Models are Trained in 5 Steps\nWe started with the ULMFiT paper, transferred the learning approach to today’s large language models, and discovered that the three steps of the ULMFiT paper are still visible in today’s training approaches, but that the techniques of the different phases have changed in the meantime. We also mentioned two additional steps. Training a tokenizer is a necessary initial step before pre-training can start, because all subsequent training steps are performed on tokenized data. Additional fine-tuning or RAG are optional final steps. The interesting point to make is that steps 0 to 3 are almost exclusively done by large organizations because of the massive datasets and compute required for these steps. Step 4 is the primary way you can customize a large language model. Therefore, for AI practitioners, it is essential to understand the inner mechanics of large language models to choose the right tools for improving model performance.\nIn closing, the final diagram shows all the five phases which are relevant for training large language models.\n\n\n\n\n\nsequenceDiagram\n    participant DP as Data Provider\n    participant TP as Training Pipeline\n    participant Human as Human Annotator\n\n    %% Step 0: Tokenization\n    Note over DP,TP: Step 0: Tokenization\n    DP -&gt;&gt; TP: Provide Large General-Domain Corpus for Tokenization\n    TP --&gt;&gt; DP: Return Vocabulary for Tokenizer\n\n    %% Step 1: Language Model Pre-Training\n    Note over DP,TP: Step 1: Language Model Pre-Training\n    DP -&gt;&gt; TP: Provide Large General-Domain Corpus for Pre-Training\n    TP --&gt;&gt; DP: Return Pre-Trained Language Model (Foundation Model)\n\n    %% Step 2: Instruction Tuning\n    Note over DP,TP: Step 2: Instruction Tuning\n    DP -&gt;&gt; TP: Provide Instruction-Focused Corpus for Fine-Tuning\n    TP --&gt;&gt; DP: Return Instruction-Tuned Language Model (Assistant Model)\n\n    %% Step 3: Reinforcement Learning with Human Feedback (RLHF)\n    Note over DP,Human: Step 3: Reinforcement Learning with Human Feedback (RLHF)\n    DP -&gt;&gt; TP: Prompting Instruction-Tuned Language Model\n    TP -&gt;&gt; Human: Provide Generated Outputs for Feedback\n    Human -&gt;&gt; TP: Provide Feedback on Model Outputs\n    TP --&gt;&gt; DP: Return RLHF-Tuned Language Model (Chat Model)\n\n    %% Optional Step 4: Additional Fine-Tuning\n    Note over DP,TP: Optional Step 4: Additional Fine-Tuning\n    DP -&gt;&gt; TP: Provide Own Dataset for Fine-Tuning\n    TP --&gt;&gt; DP: Return Fine-Tuned Language Model (Custom Chat Model)"
  },
  {
    "objectID": "posts/2024-06-21-how-llms-are-trained/index.html#references",
    "href": "posts/2024-06-21-how-llms-are-trained/index.html#references",
    "title": "How LLMs are Trained",
    "section": "References",
    "text": "References\n[1] Howard, J. (2023). A Hackers’ Guide to Language Models\n[2] Howard, J., & Ruder, S. (2018). Universal Language Model Fine-tuning for Text Classification\n[3] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., & Lample, G. (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models\n[4] Karpathy, A. (2023). Intro to Large Language Models\n[5] Sanderson G. (3Blue1Brown). (2024). But what is a GPT? Visual intro to transformers\n[6] Karpathy, A. (2024). Let’s build the GPT Tokenizer by Andrej Karpathy\n[7] Touvron, H., Izacard, G., Joulin, A., & Lample, G. (2023). LLaMA: Open and Efficient Foundation Language Models\n[8] Sanderson, G. (3Blue1Brown). (2024). Attention in transformers, visually explained by 3Blue1Brown\n[9] Wei, J., Bosma, M., Zhao, V. Y., Guu, S., Yu, A. W., Lester, B., Du, N., Dai, A. M., & Le, Q. V. (2021). Finetuned Language Models Are Zero-Shot Learners"
  },
  {
    "objectID": "posts/2024-06-22-quarto-update-and-helpful-features/index.html",
    "href": "posts/2024-06-22-quarto-update-and-helpful-features/index.html",
    "title": "Updating Quarto and Helpful Features",
    "section": "",
    "text": "After completing the blog post on How LLMs are Trained, I took some time for blog maintenance. A Quarto upgrade was necessary, and I also wanted to introduce you to some features I find worth noting."
  },
  {
    "objectID": "posts/2024-06-22-quarto-update-and-helpful-features/index.html#updating-quarto",
    "href": "posts/2024-06-22-quarto-update-and-helpful-features/index.html#updating-quarto",
    "title": "Updating Quarto and Helpful Features",
    "section": "Updating Quarto",
    "text": "Updating Quarto\nSometimes I find myself obsessing over minor details. While publishing the blog post on How LLMs are Trained, I noticed that the old Twitter logo needed to be updated to the new X logo. Although it seemed like a small task, it turned out to be more complicated than expected. The icons are standard bootstrap icons, but the new icon (twitter-x) did not render correctly.\nUpon investigation, I discovered that my Quarto installation was outdated. Running quarto --version returned 1.3.450, but the updated icon was only supported in version 1.4.\nSince this was my first Quarto update, I followed my own guide to avoid disaster and pushed all my changes to GitHub. This way, if something went wrong, I wouldn’t lose any data.\nAs it turned out, the precautions were unnecessary, but beware of the outcome bias 😉. Anyway, here are the steps for updating Quarto:\n\nStep 1: Update nbdev\nIn analogy to the initial installation, I upgraded nbdev:\nmamba update nbdev -c fastai\n\n\nStep 2: Update Quarto\nTo update Quarto, you should be in the home directory (cd ~):\nnbdev_install_quarto\n\n\nStep 3: Verify the Update\nAfter the installation process is complete, verify that Quarto has been installed by checking its version:\nquarto --version\nI am now on version 1.4.555, and the new X icon renders as it should 😀."
  },
  {
    "objectID": "posts/2024-06-22-quarto-update-and-helpful-features/index.html#using-directives",
    "href": "posts/2024-06-22-quarto-update-and-helpful-features/index.html#using-directives",
    "title": "Updating Quarto and Helpful Features",
    "section": "Using Directives",
    "text": "Using Directives\nWorking in Jupyter notebooks is great, but converting notebook 1:1 into blog posts I find sometimes challenging due to the technical nature of a notebook, making it hard to read. For example, sometimes I need a plot, but the code to create the plot does not add any value to the blog post. Creating the plot separately and using a screenshot felt inefficient, but apparently, I was not the only one thinking along these lines. Directives help solve these formatting challenges. Here are the directives I have used (and will most certainly use in future blog posts).\n\nHiding Cell Output (Displaying the Code)\nSome cells are quite verbose, outputting text that does not add any value to the blog post. One example would be initializing an LLM, for example in Remembering the Wittmann Tours World Trip with RAG. The command llm = Llama(model_path=\"llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=4096, verbose=False) outputs a lot of text, even though verbose is set to False. To silence the chatter, you can simply use the output directive. The first line of the cell in your Jupyter notebook needs to be:\n#| output: false\n\n\nHiding the Code (Displaying the Output)\nIn contrast, some cells contain code that isn’t relevant, but you want the output to be rendered in your blog post. In Visualizing Embeddings in 2D, I had several charts that required lengthy code to generate, but only the result was important. Use the echo directive to hide the code. The first line of the cell in your Jupyter notebook needs to be:\n#| echo: false\n\n\nToggling Code (Code Fold)\nIn other cases, the code might not be highly relevant, but you still want to include it in the blog post. To make displaying the code optional, use the code fold directive. The first line of the cell in your Jupyter notebook needs to be:\n#| code-fold: true"
  },
  {
    "objectID": "posts/2024-06-22-quarto-update-and-helpful-features/index.html#including-mermaid-chartsdiagrams",
    "href": "posts/2024-06-22-quarto-update-and-helpful-features/index.html#including-mermaid-chartsdiagrams",
    "title": "Updating Quarto and Helpful Features",
    "section": "Including Mermaid Charts/Diagrams",
    "text": "Including Mermaid Charts/Diagrams\nMermaid charts or diagrams are my go-to choice for visualizations in markdown and Jupyter notebooks because the programmatic approach saves a lot of time compared to tedious PowerPoint editing. When including a mermaid diagram in a Jupyter notebook for a blog post, a code block starting with ```mermaid, however, does not rendered correctly. To use a mermaid code block in the context of Quarto, it needs to use additional curly braces {} like this:\n```{mermaid}\n\n  put mermaid diagram here\n\n```\nP.S.: Thanks to Christian Long for the markdown syntax to render a complete code block."
  },
  {
    "objectID": "posts/2025-07-10-deploying-fastapi-app-on-sap-btp/index.html",
    "href": "posts/2025-07-10-deploying-fastapi-app-on-sap-btp/index.html",
    "title": "Deploying a FastAPI App to SAP BTP",
    "section": "",
    "text": "In a previous blog post, we created a simple greeting API using FastAPI, entirely within a Jupyter Notebook. This follow-up post shows how to deploy it on SAP Business Technology Platform (SAP BTP), making it accessible from your SAP or non-SAP projects.\n(I assume you already have access to an SAP BTP account with Cloud Foundry enabled.)\nYou’ll learn how to:\nIn short, we’ll take the FastAPI greeting service from “it works on my machine” to “deployed on SAP BTP.” 🤓"
  },
  {
    "objectID": "posts/2025-07-10-deploying-fastapi-app-on-sap-btp/index.html#creating-a-local-deployment",
    "href": "posts/2025-07-10-deploying-fastapi-app-on-sap-btp/index.html#creating-a-local-deployment",
    "title": "Deploying a FastAPI App to SAP BTP",
    "section": "Creating a Local Deployment",
    "text": "Creating a Local Deployment\nOnce you’re done developing your FastAPI app in a Jupyter Notebook, you can prepare it for local deployment by following these steps:\n\nCreating app.py\nTo create the app.py file, follow these steps\n\nCopy all the code from the final cell in the Jupyter Notebook and paste it into a new Python file named app.py.\nRemove notebook-specific dependencies. Anything related to nest_asyncio is no longer needed, it is only necessary in Jupyter notebooks. Additionally, remove the uvicorn.run(...) call, as the server will be launched outside app.py.\n\n\n\nCreating requirements.txt\nFor our simple project, you could easily create the requirements.txt file manually, but I am too lazy for that, so I am using the pipreqs package to generate it for me.\nIf you don’t have the pipreqs package installed, you can simply install it using pip:\npip install pipreqs\nNow you can generate the requirements.txt. Either cd into the directory where your app.py file is located and run\npipreqs .\nAlternatively, you can specify the path to the app.py file\npipreqs path/to/app.py\n\n\nInstalling Dependencies\nOnce you have the requirements.txt file, you can install the dependencies using pip\npip install -r requirements.txt\nWhen you are developing your own app, and you started in a Jupyter notebook, the installation of the dependencies is a self-fulfilling prophecy, but once we move to a virtual environment, you need to install the dependencies.\n\n\nRunning the App\nNow you can run your app with Uvicorn from the terminal. From the directory where app.py is located, start the API server with:\nuvicorn app:app --reload\nLet’s break down the command:\n\nuvicorn: The Uvicorn server.\napp:app: This follows the format :: It tells Uvicorn to look for the FastAPI app instance named app inside the Python module app.py. Since both arguments are the same, let’s change it for clarity. If you had written greeting_app = FastAPI() inside a file called api_app.py, you would run uvicorn api_app:greeting_app --reload\n--reload: Automatically restarts the server whenever you make code changes — useful during development.\n\n\nNote for later: When deploying to SAP BTP, you’ll also need to add two files to your app folder: manifest.yaml and Procfile. These are only needed for deployment and not required when testing locally."
  },
  {
    "objectID": "posts/2025-07-10-deploying-fastapi-app-on-sap-btp/index.html#running-the-app-in-a-virtual-environment",
    "href": "posts/2025-07-10-deploying-fastapi-app-on-sap-btp/index.html#running-the-app-in-a-virtual-environment",
    "title": "Deploying a FastAPI App to SAP BTP",
    "section": "Running the App in a Virtual Environment",
    "text": "Running the App in a Virtual Environment\nSo far, we have re-created the behavior of the Jupyter Notebook in a Python file. However, when we deploy the app in the cloud, we will have a different environment. To make sure that we defined the dependencies correctly and the app runs as expected, we should test it in a virtual environment. This ensures you’re not relying on any globally installed Python packages from your system or IDE setup.\n\nCreating a Virtual Environment\nTo create a virtual environment, you can use the venv module that comes with Python. Open your terminal and navigate to the directory where your app.py file is located. Then, run the following command to create a virtual environment named venv:\npython -m venv venv\nLet’s break down the command:\n\npython: The Python interpreter.\n-m venv: The venv module, which is used to create virtual environments.\nvenv: The name of the virtual environment.\n\n\n\nActivating the Virtual Environment\nOnce the virtual environment is created, you need to activate it. The activation command depends on your operating system:\n\nWindows: venv\\Scripts\\activate\nmacOS/Linux: source venv/bin/activate\n\nAs a result, you should see the name of the virtual environment (venv) as a prefix in your terminal.\n\n\nInstalling Dependencies\nNow that you have activated the virtual environment, you can install the dependencies using pip. From the directory where app.py is located, run:\npip install -r requirements.txt\nNow the virtual environment has all the dependencies from requirements.txt installed, simulating the environment in which the app will be deployed on BTP.\n\n\nRunning the App\nNow you can run your app with Uvicorn from the terminal. From the directory where app.py is located, start the API server with:\nuvicorn app:app --reload\n\n\nDeactivating the Virtual Environment\nWhen you are done testing, you can deactivate the virtual environment by running:\ndeactivate"
  },
  {
    "objectID": "posts/2025-07-10-deploying-fastapi-app-on-sap-btp/index.html#preparing-for-deployment-to-sap-btp",
    "href": "posts/2025-07-10-deploying-fastapi-app-on-sap-btp/index.html#preparing-for-deployment-to-sap-btp",
    "title": "Deploying a FastAPI App to SAP BTP",
    "section": "Preparing for Deployment to SAP BTP",
    "text": "Preparing for Deployment to SAP BTP\nBefore deploying our API to SAP BTP using Cloud Foundry, we need to complete a few preparation steps. First, we need to install the Cloud Foundry CLI (unless you want to deploy via the BTP UI which is also possible). Additionally, we need to create two configuration files: manifest.yaml and Procfile. These will define how the application is deployed and run on BTP.\n\nInstalling the Cloud Foundry CLI\nTo interact with Cloud Foundry, the preferred way is to use the cf command-line interface. If you have not installed it yet, you can download and install it from the official Cloud Foundry CLI page.\nOn macOS, you can conveniently install the current version 8 using brew:\nbrew install cloudfoundry/tap/cf-cli@8\nOnce installed, you should be able to run:\ncf --version\nYou should see the version number of the CLI which confirms the CLI is available and ready to use.\n\n\nCreating the manifest.yaml\nThe manifest.yaml file defines how Cloud Foundry should deploy your app. It includes metadata such as the app name, memory allocation, number of instances, and route configuration. For a complete reference, please check out the Cloud Foundry Manifest Reference\nCreate a new file named manifest.yaml in the same folder as your app.py and add the following content:\n---\napplications:\n  - name: greeting-api\n    memory: 256M\n    instances: 1\n    buildpacks:\n      - python_buildpack\n    path: .\n    routes:\n      - route: greeting-api-test.cfapps.eu10-004.hana.ondemand.com\nLet’s break down the content of the manifest.yaml:\n\nname: The name of the app as it will appear in your BTP subaccount.\nmemory: The memory allocated per app instance. 256M is typically sufficient for small FastAPI services.\ninstances: Number of app instances to start (1 for now).\nbuildpacks: Sets the Python buildpack (if not set explicitly, BTP will try to detect it automatically).\npath: The path to the application directory.\nroutes: The custom URL (host + domain) you want your app to use. You can find the correct domain (e.g. cfapps.eu10-004.hana.ondemand.com) in your BTP subaccount under “Cloud Foundry → Overview → API Endpoint.”\n\n\n\nCreating the Procfile\nThe purpose of a Procfile is to define how your app should be started. While you can specify the start command manually during cf push (this is the command which finally deploy the API on BTP), it’s good practice to include a Procfile for clarity, consistency, and maintainability.\nCreate a plain text file named Procfile (no extension) in your app’s root directory with the following content:\nweb: uvicorn app:app --host=0.0.0.0 --port=${PORT:-8080}\nLet’s break it down:\n\nweb: tells the platform that this process handles incoming HTTP requests.\nuvicorn app:app loads your FastAPI app instance named app from the app.py file.\n--host=0.0.0.0 makes Uvicorn listen on all network interfaces (required for deployment).\n--port=${PORT:-8080} uses the port environment variable ($PORT) provided by Cloud Foundry or falls back to port 8080 when running locally."
  },
  {
    "objectID": "posts/2025-07-10-deploying-fastapi-app-on-sap-btp/index.html#deploying-the-api-on-sap-btp",
    "href": "posts/2025-07-10-deploying-fastapi-app-on-sap-btp/index.html#deploying-the-api-on-sap-btp",
    "title": "Deploying a FastAPI App to SAP BTP",
    "section": "Deploying the API on SAP BTP",
    "text": "Deploying the API on SAP BTP\nAfter we’ve successfully tested our API locally and completed all the necessary preparations, it’s time to deploy it to SAP BTP using Cloud Foundry.\nTo keep things organized, I created a new folder called app-btp and copied the following files into it:\n\napp.py – the main FastAPI application\nrequirements.txt – the list of dependencies\nProcfile – the startup instruction\nmanifest.yaml – the deployment descriptor\n\nTo start the deployment, open a terminal, cd into the app-btp directory, and log in to Cloud Foundry using the SAP BTP CLI:\ncf login\nYou’ll be prompted to enter your API endpoint, email, password, org, and space. You can find the API endpoint in your BTP cockpit by navigating to your subaccount and checking the “Cloud Foundry Environment” section.\nOnce logged in, push your app to BTP:\ncf push\nThink of cf push like pushing code to GitHub: It copies your application to the BTP environment, sets up the app using the manifest.yaml, installs the dependencies from requirements.txt, and starts the app using the Procfile.\nAfter deployment, your app will be assigned a public URL, as specified in your manifest.yaml. For example:\nhttps://greeting-api-test.cfapps.eu10-004.hana.ondemand.com\nYou can now test the API using the GET client notebook or the POST client notebook."
  },
  {
    "objectID": "posts/2025-07-10-deploying-fastapi-app-on-sap-btp/index.html#alternative-deploying-via-zip-upload-in-the-sap-btp-cockpit",
    "href": "posts/2025-07-10-deploying-fastapi-app-on-sap-btp/index.html#alternative-deploying-via-zip-upload-in-the-sap-btp-cockpit",
    "title": "Deploying a FastAPI App to SAP BTP",
    "section": "Alternative: Deploying via ZIP Upload in the SAP BTP Cockpit",
    "text": "Alternative: Deploying via ZIP Upload in the SAP BTP Cockpit\nIf you prefer to deploy your app via the SAP BTP UI instead of using the CLI, you can also create a ZIP archive and upload it directly through the cockpit.\nSimply create a ZIP archive of your app folder containing the following files:\n\napp.py\nrequirements.txt\nProcfile\n\nMake sure there are no folders in the ZIP-file, the files should be in the root of the ZIP archive.\nKeep the manifest.yaml as a separate file.\nTo upload the file for deployment, follow these steps:\n\nNavigate to a space in your subaccount\nClick on “Deploy Application”\nUpload your app-btp.zip file and the manifest.yaml file\nClick on “Deploy”\n\nAfter the deployment is complete, your app will be live under the route you defined in manifest.yaml.\nYou can now test the API using the GET client notebook or the POST client notebook."
  },
  {
    "objectID": "posts/2025-07-10-deploying-fastapi-app-on-sap-btp/index.html#conclusion",
    "href": "posts/2025-07-10-deploying-fastapi-app-on-sap-btp/index.html#conclusion",
    "title": "Deploying a FastAPI App to SAP BTP",
    "section": "Conclusion",
    "text": "Conclusion\nTaking the API from a Jupyter Notebook (my preferred environment for rapid prototyping) to a working cloud deployment took a few steps. First, we refactored the code into a standalone Python file (app.py). Then we created a requirements.txt file and tested everything locally in a clean virtual environment. After creating manifest.yaml and Procfile files, we deployed the API to the cloud using the SAP BTP CLI or the SAP BTP Cockpit.\nAfter building the greeting API in my previous blog post, we’ve now taken it all the way to the cloud. The goal here wasn’t to explore every FastAPI feature or every cloud deployment nuance, but to provide a reusable, step-by-step template for future projects.\nThere’s lots more you could do, e.g. adding authentication, rate limiting, or monitoring, but we’ll leave that for another time. I Hope this was helpful, and happy coding!"
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html",
    "title": "Running LLama2 locally on a Mac",
    "section": "",
    "text": "Running a large language models (LLM), namely llama2, locally on my Mac was the next logical step for me working through the hacker’s guide by Jeremy Howard. While it was possible to adjust Jeremy’s approach on Hugging Face to also work on Apple Silicon, I focussed on llama.cpp and its python binding llama-cpp-python to talk to llama2.\nThe whole journey consisted of the following steps, and I am going to take you though all of them to share my learnings along the way:"
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#getting-access-to-llama2",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#getting-access-to-llama2",
    "title": "Running LLama2 locally on a Mac",
    "section": "Getting Access to Llama2",
    "text": "Getting Access to Llama2\nFirst things first: Before, you can access the Llama2 model, you need to agree to Meta’s the terms and conditions for Llama2. As per the time of writing this, the process was as follows:\n\nVisit the model’s home page at Hugging Face\nGo to Meta’s website, and complete the registration form\nConfirm the terms and conditions on the Hugging Face Website (see screenshot)\n\nThe approval only took a couple of minutes."
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#running-llama2-via-hugging-face",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#running-llama2-via-hugging-face",
    "title": "Running LLama2 locally on a Mac",
    "section": "Running Llama2 via Hugging Face",
    "text": "Running Llama2 via Hugging Face\nTrying to stick as closely as possible to the original hacker’s guide, I wanted to run LLama2 locally on my Mac using the Hugging Face API, just to see if it worked. Without Nvidia support, I needed to adapt the code to make it compatible with Apple’s Metal Framework. For all all the details, what needed to be done to run llama2 via the Hugging Face API, please check out this notebook.\nThe final result was academically interesting, but performance left much to be desired 😉: What Jeremy’s machine did in 2 seconds took my MacBook more than 3 minutes. There are probably a couple of reasons which produced this dramatic difference in performance:\n\nNvidia memory throughput is a lot better then Apple’s unified RAM\nThe model I used was originally optimized and quantized for Nvidia GPUs. To run this model on my MacBook, I had to disable the 8-bit quantization (load_in_8bit=False) among other changes. While this adaptation was necessary for compatibility with Apple Silicon, it discarded all the optimizations.\nPyTorch’s optimization for CUDA is probably still way better than its MPS optimization.\n\nHere is a key learning: Running large language models (LLMs) locally requires more than brute force. Instead, hardware and software need to be aligned. Apple Silicon machines are extremely capable, but they need a different kind of optimization then Nvidia hardware. Consider the following analogy: Imagine you need to travel from Hamburg to Munich, and you have 2 hardware setups available, a car (let’s say this represents Nvidia hardware) or a plane (let’s say this represents Apple Silicon). Both these hardware setups require different optimizations to get from A to B.\nDriving from Hamburg to Munich by car (representing Nvidia hardware), you optimize your path along the roads. If you used the plane instead (representing Apple Silicon), the same optimization would not work well. Attempting to navigate the plane on the roads, as you would a car, is highly impractical. Therefore, you would use a different way to optimize the path: You take public transport or a taxi to the airport, you fly from Hamburg to Munich, and again, you take public transport or a taxi to reach your final destination. On both hardware setups you have reached your Munich, but the underlying setup and optimizations differed significantly.\nTherefore, let’s hop on the plane, and let’s explore a different way to run llama2 to on a Mac: Let’s turn our attention to llama.cpp.\n\n\n\n\nDalle: A llama driving a car and another llama flying a plane on the road from Hamburg to Munich"
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#what-is-llama.cpp",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#what-is-llama.cpp",
    "title": "Running LLama2 locally on a Mac",
    "section": "What is llama.cpp?",
    "text": "What is llama.cpp?\nLlama.cpp is an optimized library to run a large language model (LLM) like Llama2 on a Mac, but it also supports other platforms. How is this possible? For the details, please let me refer to this tweet by Andrej Karpathy and for even more details to this blog post by Finbarr Timbers. Here are my takeaways:\n\nLlama.cpp runs inference of LLMs in pure C/C++, therefore, it is significantly faster than implementations in higher languages like python.\nAdditionally, the mission of the project “is to run the LLaMA model using 4-bit integer quantization on a MacBook”. This means that numbers used to represent model weights and activations downsized from 32- or 16- bit floating points (the format of the base models) with 4-bit integers. This reduces memory usage and improves the performance and efficiency of the model during inference. The somewhat surprising thing is that model performance does not degrade by this downsizing.\n\nWhen I mentioned before that I had to turn off quantization on Hugging Face, here we turn it on a again, just differently."
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#how-you-can-use-llama.cpp-from-python",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#how-you-can-use-llama.cpp-from-python",
    "title": "Running LLama2 locally on a Mac",
    "section": "How You Can Use llama.cpp from Python",
    "text": "How You Can Use llama.cpp from Python\nThe project llama-cpp-python serves as a binding for llama.cpp, providing access to the C++ API to Llama2 from Python.\nIn this context, a “binding” is a bridge that facilitates interaction between two programming languages, i.e. a layer of code that allows two programming languages to interact with each other. Llama.cpp is written in C/C++, and the llama-cpp-python binding allows this C/C++ library to be utilized within a Python environment. Essentially, the Python code wraps around the C/C++ code so that it can be called from a Python environment.\nWhile it might sound complicated, the concept is surprisingly accessible when you reduce the context to a simple example. To keep the focus in this blog post, I separated the exploration of C bindings into this blog post (LINK)."
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#installing-llama-cpp-python",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#installing-llama-cpp-python",
    "title": "Running LLama2 locally on a Mac",
    "section": "Installing llama-cpp-python",
    "text": "Installing llama-cpp-python\nFirst, we need to install llama-cpp-python via pip install llama-cpp-python.\nUpgrading is done via pip install llama-cpp-python  --upgrade --force-reinstall --no-cache-dir.\n\n💡 Note: To execute the steps interactively, please check out my related notebook."
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#downloading-the-model",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#downloading-the-model",
    "title": "Running LLama2 locally on a Mac",
    "section": "Downloading the Model",
    "text": "Downloading the Model\nFor all my experiments, I used the following model: TheBloke/Llama-2-7b-Chat-GGUF\nTo download the model, please please run the code below, assuming that you have stored your Hugging Face access token in the .env-file. For additional insights/troubleshooting, please also check out my previous blog post / my previous notebook:\n\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\ntoken = os.getenv('HF_TOKEN')\nos.environ['HF_TOKEN'] = token\n!huggingface-cli login --token $HF_TOKEN\n!wget -P ../models https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf"
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#loading-the-model",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#loading-the-model",
    "title": "Running LLama2 locally on a Mac",
    "section": "Loading the Model",
    "text": "Loading the Model\nLoading the model, only required 2 lines of code. Let’s talk about the parameters:\n\nn_ctx=2048: This sets the context window to 2048 tokens. The maximum number of tokens for this model is 4096.\nverbose=False: This makes the model less talkative. It only prints the actual results when prompted. Please feel free try turning it to True to get additional information from the model, not just the generated response.\n\n\nfrom llama_cpp import Llama\nllm = Llama(model_path=\"../models/Llama-2-7b-chat/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=2048, verbose=False)\n#llm = Llama(model_path=\"../../../lm-hackers/models/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=2048, verbose=False)"
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#completion-vs.-chat-completion-example",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#completion-vs.-chat-completion-example",
    "title": "Running LLama2 locally on a Mac",
    "section": "Completion vs. Chat Completion Example",
    "text": "Completion vs. Chat Completion Example\nThere are 2 ways we can talk to the LLM: The completion method literally does what it promises: It completes a prompt. For having a conversation with the LLM, we need to use Chat Completion.\nAs per the Getting Started guide, let’s look at one example each on how to use the API:\nLet’s do text completion first.\n\noutput = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=128, stop=[\"Q:\", \"\\n\"], echo=True)\nprint(output['choices'][0]['text'])\n\nQ: Name the planets in the solar system? A: 1. Mercury 2. Venus 3. Earth 4. Mars 5. Jupiter 6. Saturn 7. Uranus 8. Neptune\n\n\nFor the chat completion, let’s re-write the code to reproduce the example from the hackers guide to make the LLM talk about money in aussie slang.\n\naussie_sys = \"You are an Aussie LLM that uses Aussie slang and analogies whenever possible.\"\n\nmessages=[\n    {\"role\": \"system\", \"content\": aussie_sys},\n    {\"role\": \"user\", \"content\": \"What is money?\"}]\n\nmodel_response = llm.create_chat_completion(messages = messages, stream=False)\nprint(model_response['choices'][0]['message']['content'])\n\n  Fair dinkum, mate! Money, eh? It's like the oxygen we breathe, ya know? (laughs) Just kiddin', but seriously, money is like the lifeblood of society. It's what keeps the economy tickin' over and allows us to buy the things we need and want.\nThink of it like this: money is like a big ol' pile of dough (get it? Dough? Like bread dough? Ah, never mind). We all gotta work hard to earn that dough, whether it's through our day job or by startin' our own business. And then we can use that dough to buy things like food, shelter, and a cold one at the pub after work.\nBut here's the thing: money ain't everything, mate. There's more to life than just makin' dough. We gotta find meaning and purpose in our lives, or else we'll be livin' like a dog (sorry, dogs!). So, while money's important, it's not the only thing that matters.\nNow, I know some blokes might say, \"Money, money, money! That's all that matters!\" But let me tell you, mate, they're barkin' up the wrong tree (get it? Barkin' up the wrong tree? Ah, never mind). There's more to life than just chasin' after the green.\nSo there you have it, mate! Money's like a big ol' pile of dough that we all gotta work hard to earn. But don't forget, there's more to life than just makin' dough. Keep on keepin' on, and always remember: money may not buy happiness, but it can buy a cold one at the pub after work! (laughs)"
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#conclusion",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#conclusion",
    "title": "Running LLama2 locally on a Mac",
    "section": "Conclusion",
    "text": "Conclusion\nUsing the right approach, running an LLM, in this case llama2, in a Jupyter notebook on a Mac is not really difficult. Once you sorted out the setup (like terms and conditions), starting up llama2 via llama-cpp-python only requires a few lines of code. Happy chatting!\n\n\n\n\nDalle: Aussie LLM depicted as kangaroo talking about money with Aussie slang and analogies"
  },
  {
    "objectID": "posts/2025-03-28-llm-web-search/index.html",
    "href": "posts/2025-03-28-llm-web-search/index.html",
    "title": "Implementing Web Search for Large Language Models from Scratch",
    "section": "",
    "text": "One major limitation of large language models (LLMs) is that their knowledge is typically constrained by a fixed cutoff date, beyond which they’re unaware of recent events or developments. Fortunately, there’s an effective solution to overcome this limitation: integrating real-time web search capabilities. Although web search functionality is now a standard feature in many LLM web interfaces, it isn’t typically available by default when interacting with an LLM through an API. At first glance, implementing such a feature yourself might seem complicated, but approaching it from first principles simplifies things considerably.\nIn this post, I’ll walk you through how you can quickly and effectively integrate web search functionality into your own LLM projects, from conceptualizing the solution to a practical, step-by-step implementation.\n\n\n\n\nLLM Web Search\n\n\n\n\nTo break down the challenge, consider two key ideas:\n\nA web search is simply a tool available to the LLM.\nA web search operation is essentially just a straightforward API call.\n\nKeeping these principles in mind, integrating web search fits neatly into the established “ReAct” [1] prompting framework, where an LLM iteratively reasons (thinks) and acts (uses tools) until it achieves its goal. The following diagram illustrates clearly how this integration works in practice:\n\n\n\n\n\nsequenceDiagram\n    autonumber\n\n    actor User\n    participant WebSearch as Web Search\n    participant Function as Tool\n    participant ChatClient as Chat Client\n    participant LLM\n\n\n    User-&gt;&gt;Function: Define the web search (API call)\n    Function-&gt;&gt;User: Retrieve JSON function definition\n    User-&gt;&gt;ChatClient: Create Chat Client including web search tool\n    User-&gt;&gt;ChatClient: Send prompt\n    ChatClient-&gt;&gt;LLM: Send prompt with web search tool\n\n    loop Reasoning and Acting\n        LLM-&gt;&gt;LLM: Reasoning: Analyze prompt and tools\n        LLM-&gt;&gt;ChatClient: Acting: Generate tool call: web search\n        ChatClient-&gt;&gt;Function: Call web search function\n        Function-&gt;&gt;WebSearch: Call web search API\n        WebSearch-&gt;&gt;Function: Return web search result\n        Function-&gt;&gt;ChatClient: Return web search result\n        ChatClient-&gt;&gt;LLM: Acting: Pass on web search result\n        LLM-&gt;&gt;LLM: Reasoning: Incorporate result and continue reasoning\n    end\n\n    LLM-&gt;&gt;ChatClient: Return final result\n    ChatClient-&gt;&gt;User: Output final result\n\n\n\n\n\n\n\nNow that we have a clear conceptual understanding, let’s walk through the actual implementation step-by-step.\n\n\n\nWhen choosing a web search API for this project, simplicity was my top priority. My goal was straightforward: I wanted an API where I could simply input a search query and immediately receive a response that was easy for both me and the LLM to parse.\nInitially, I considered the obvious choices: popular search engines like Google and Bing. However, I quickly realized that both come with a level of complexity that exceeded my needs.\nContinuing my search, I came across Tavily, a service I have no affiliation with, but found refreshingly straightforward. Tavily offers an API tailored specifically for LLM use cases, returning concise, well-structured results. It also provides a generous free tier of 1,000 requests per month, making it ideal for experimentation and quick prototyping.\nAnother potential option I considered was Brave Search, which also appears to offer an accessible API with minimal overhead. It may be worth exploring if you’re looking for alternatives with similar simplicity.\nUltimately, I chose Tavily because of its minimal setup, clean responses, and ease of integration—all of which aligned perfectly with the goals of this project.\n\n\n\nLet’s get started by installing the Tavily Python library: (As usual, there’s a Jupyter notebook version of this blog post if you want to run the code yourself.)\n!pip install tavily-python\nBefore integrating Tavily with our LLM, it’s a good practice to test the service independently. While the free tier provides plenty of room for testing, I chose to initially use a cached mock response to minimize unnecessary API calls. This approach ensures our logic and parsing methods work correctly before we start consuming real API credits.\nHere is an example of a mock response from Tavily for the query “Who is Leo Messi?”:\n\n\nCode\nmock_response = \"\"\"{\n    \"answer\": null,\n    \"follow_up_questions\": null,\n    \"images\": [],\n    \"query\": \"Who is Leo Messi?\",\n    \"response_time\": 1.75,\n    \"results\": [\n        {\n            \"content\": \"Lionel Messi is an Argentine-born football (soccer) player who has been named the world’s best men’s player of the year seven times (2009–12, 2015, 2019, and 2021). In 2022 he helped Argentina win the World Cup. Naturally left-footed, quick, and precise in control of the ball, Messi is known as a keen pass distributor and can readily thread his way through packed defenses. He led Argentina’s national team to win the 2021 Copa América and the 2022 World Cup, when he again won the Golden Ball award.\",\n            \"raw_content\": null,\n            \"score\": 0.84027237,\n            \"title\": \"Lionel Messi | Biography, Trophies, Records, Ballon d'Or, Inter Miami ...\",\n            \"url\": \"https://www.britannica.com/biography/Lionel-Messi\"\n        },\n        {\n            \"content\": \"Widely regarded as one of the greatest players of all time, Messi set numerous records for individual accolades won throughout his professional footballing career such as eight Ballon d'Or awards and four the Best FIFA Men's Player awards. A prolific goalscorer and creative playmaker, Messi has scored over 850 senior career goals and has provided over 380 assists for club and country. [16] Born in Rosario, Argentina, Messi relocated to Spain to join Barcelona at age 13, and made his competitive debut at age 17 in October 2004. An Argentine international, Messi is the national team's all-time leading goalscorer and most-capped player. His style of play as a diminutive, left-footed dribbler drew career-long comparisons with compatriot Diego Maradona, who described Messi as his successor.\",\n            \"raw_content\": null,\n            \"score\": 0.8091708,\n            \"title\": \"Lionel Messi - Wikipedia\",\n            \"url\": \"https://en.wikipedia.org/wiki/Lionel_Messi\"\n        }\n    ]\n}\"\"\"\n\n\n\n\nCode\nfrom IPython.display import display, Code\n\ndef display_json(data):\n    \"\"\"\n    Nicely displays JSON content: indented + syntax-highlighted.\n    \n    Args:\n        data (str | dict | list): The JSON or string to display.\n    \"\"\"\n    # Parse if input is a string\n    if isinstance(data, str):\n        try:\n            data = ast.literal_eval(data)\n        except Exception as e:\n            print(\"Failed to parse string input as JSON-like structure.\")\n            print(\"Error:\", e)\n            return\n\n    # Convert to pretty JSON string\n    pretty_json = json.dumps(data, indent=4, sort_keys=True, ensure_ascii=False)\n\n    # Display with syntax highlighting\n    display(Code(pretty_json, language='json'))\n\nimport json\nimport ast\n\ndef parse_mock_response(response_str: str):\n    try:\n        return json.loads(response_str)\n    except json.JSONDecodeError:\n        try:\n            return ast.literal_eval(response_str)\n        except Exception as e:\n            print(\"❌ Failed to parse mock response:\", e)\n            return {}\n\n\nLet’s run our first query.\n\n\nCode\nimport os\nfrom dotenv import load_dotenv\n\nuse_api = False\n\nload_dotenv()\napi_key = os.getenv(\"TAVILY_API_KEY\")\nif not api_key:\n    raise ValueError(\"TAVILY_API_KEY not found in .env file.\")\n\n\n\nfrom tavily import TavilyClient\n\nquery = \"Who is Leo Messi?\"\n\nif use_api:\n    tavily_client = TavilyClient(api_key=api_key)\n    response = tavily_client.search(\n        query=query,\n        search_depth=\"basic\",\n        include_answer=False,\n        include_raw_content=False,\n        max_results=2\n    )\nelse:\n    response = parse_mock_response(mock_response)\n\ndisplay_json(response)\n\n{\n    \"answer\": null,\n    \"follow_up_questions\": null,\n    \"images\": [],\n    \"query\": \"Who is Leo Messi?\",\n    \"response_time\": 1.75,\n    \"results\": [\n        {\n            \"content\": \"Lionel Messi is an Argentine-born football (soccer) player who has been named the world’s best men’s player of the year seven times (2009–12, 2015, 2019, and 2021). In 2022 he helped Argentina win the World Cup. Naturally left-footed, quick, and precise in control of the ball, Messi is known as a keen pass distributor and can readily thread his way through packed defenses. He led Argentina’s national team to win the 2021 Copa América and the 2022 World Cup, when he again won the Golden Ball award.\",\n            \"raw_content\": null,\n            \"score\": 0.84027237,\n            \"title\": \"Lionel Messi | Biography, Trophies, Records, Ballon d'Or, Inter Miami ...\",\n            \"url\": \"https://www.britannica.com/biography/Lionel-Messi\"\n        },\n        {\n            \"content\": \"Widely regarded as one of the greatest players of all time, Messi set numerous records for individual accolades won throughout his professional footballing career such as eight Ballon d'Or awards and four the Best FIFA Men's Player awards. A prolific goalscorer and creative playmaker, Messi has scored over 850 senior career goals and has provided over 380 assists for club and country. [16] Born in Rosario, Argentina, Messi relocated to Spain to join Barcelona at age 13, and made his competitive debut at age 17 in October 2004. An Argentine international, Messi is the national team's all-time leading goalscorer and most-capped player. His style of play as a diminutive, left-footed dribbler drew career-long comparisons with compatriot Diego Maradona, who described Messi as his successor.\",\n            \"raw_content\": null,\n            \"score\": 0.8091708,\n            \"title\": \"Lionel Messi - Wikipedia\",\n            \"url\": \"https://en.wikipedia.org/wiki/Lionel_Messi\"\n        }\n    ]\n}\n\n\n\n\n\n\nAlthough large language models are capable of parsing raw JSON, this format isn’t ideal. It introduces unnecessary token overhead and lacks the readability and structure that both humans and LLMs benefit from. To make the results easier to consume, we’ll reformat the API response into clean, human-readable Markdown. This improves clarity, ensures more predictable behavior from the LLM, and also makes the output easier to debug and inspect during development.\n\n\nCode\ndef format_tavily_results(response: dict, max_results: int = 5, snippet_length: int = 5000) -&gt; str:\n    \"\"\"\n    Formats the Tavily search API JSON response into a readable, LLM-friendly string.\n\n    Args:\n        response (dict): The Tavily API response.\n        max_results (int): Maximum number of results to include.\n        snippet_length (int): Max number of characters to show from each result's content.\n\n    Returns:\n        str: Formatted, readable string for LLM consumption.\n    \"\"\"\n    results = response.get(\"results\", [])\n    if not results:\n        return \"No results found.\"\n\n    formatted = \"### Web Search Results:\\n\\n\"\n    for i, result in enumerate(results[:max_results], start=1):\n        title = result.get(\"title\", \"Untitled\")\n        url = result.get(\"url\", \"\")\n        content = result.get(\"content\", \"\") or \"\"\n        snippet = content.strip().replace(\"\\n\", \" \")[:snippet_length].rstrip()\n        \n        # Clean up unfinished sentences if needed\n        if snippet and not snippet.endswith(('.', '!', '?')):\n            snippet += \"...\"\n\n        formatted += f\"{i}. **[{title}]({url})**\\n   - {snippet}\\n\\n\"\n\n    return formatted.strip()\n\n\n\nformatted = format_tavily_results(response, snippet_length=300)\nprint(formatted)\n\n### Web Search Results:\n\n1. **[Lionel Messi | Biography, Trophies, Records, Ballon d'Or, Inter Miami ...](https://www.britannica.com/biography/Lionel-Messi)**\n   - Lionel Messi is an Argentine-born football (soccer) player who has been named the world’s best men’s player of the year seven times (2009–12, 2015, 2019, and 2021). In 2022 he helped Argentina win the World Cup. Naturally left-footed, quick, and precise in control of the ball, Messi is known as a ke...\n\n2. **[Lionel Messi - Wikipedia](https://en.wikipedia.org/wiki/Lionel_Messi)**\n   - Widely regarded as one of the greatest players of all time, Messi set numerous records for individual accolades won throughout his professional footballing career such as eight Ballon d'Or awards and four the Best FIFA Men's Player awards. A prolific goalscorer and creative playmaker, Messi has scor...\n\n\nThe LLM can now easily consume the text summaries.\n\n\n\nNext, we’ll encapsulate our functionality into a single, reusable function that performs both API calls and formatting. Additionally, we need to define a proper documentation so that the LLM can understand how to use our tool:\n\ndef search_web(query: str) -&gt; str:\n    \"\"\"\n    Searches the web using Tavily and returns a formatted result.\n\n    Args:\n        query (str): The search query string.\n\n    Returns:\n        str: Formatted search results for LLM input.\n    \"\"\"\n    if use_api:\n        tavily_client = TavilyClient(api_key=api_key)\n        response = tavily_client.search(\n            query=query,\n            search_depth=\"basic\",\n            include_answer=False,\n            include_raw_content=False,\n            max_results=5\n        )\n    else:\n        response = parse_mock_response(mock_response)\n\n    return format_tavily_results(response)\n\nHere is an example-call including the result the LLM would receive:\n\n\nCode\nresult = search_web(\"Who is Leo Messi?\")\nprint(result)\n\n\n### Web Search Results:\n\n1. **[Lionel Messi | Biography, Trophies, Records, Ballon d'Or, Inter Miami ...](https://www.britannica.com/biography/Lionel-Messi)**\n   - Lionel Messi is an Argentine-born football (soccer) player who has been named the world’s best men’s player of the year seven times (2009–12, 2015, 2019, and 2021). In 2022 he helped Argentina win the World Cup. Naturally left-footed, quick, and precise in control of the ball, Messi is known as a keen pass distributor and can readily thread his way through packed defenses. He led Argentina’s national team to win the 2021 Copa América and the 2022 World Cup, when he again won the Golden Ball award.\n\n2. **[Lionel Messi - Wikipedia](https://en.wikipedia.org/wiki/Lionel_Messi)**\n   - Widely regarded as one of the greatest players of all time, Messi set numerous records for individual accolades won throughout his professional footballing career such as eight Ballon d'Or awards and four the Best FIFA Men's Player awards. A prolific goalscorer and creative playmaker, Messi has scored over 850 senior career goals and has provided over 380 assists for club and country. [16] Born in Rosario, Argentina, Messi relocated to Spain to join Barcelona at age 13, and made his competitive debut at age 17 in October 2004. An Argentine international, Messi is the national team's all-time leading goalscorer and most-capped player. His style of play as a diminutive, left-footed dribbler drew career-long comparisons with compatriot Diego Maradona, who described Messi as his successor.\n\n\n\n\n\nTo expose our web search to an LLM, we need to provide the tool definition to the LLM. Jeremy Howard shared a practical and flexible approach for this in his Hacker’s Guide. The core idea is to use Python’s introspection capabilities to extract the function signature and documentation, and convert it into a schema the LLM can understand. The version used here builds on that idea, with minor updates to match recent changes in the OpenAI tools API.\nThe most important part of this process is clearly documenting the function’s interface: Its name, parameters, and behavior—so that the LLM knows how and when to call it. This allows the model to use the tool automatically, without any additional prompting logic or manual wiring.\n\nfrom pydantic import create_model\nimport inspect, json\nfrom inspect import Parameter\n\ndef get_schema(f):\n    kw = {n:(o.annotation, ... if o.default==Parameter.empty else o.default)\n          for n,o in inspect.signature(f).parameters.items()}\n    # update: schema -&gt; model_json_schema\n    s = create_model(f'Input for `{f.__name__}`', **kw).model_json_schema()\n    # update: added function level in tools json\n    function_params = dict(name=f.__name__, description=f.__doc__, parameters=s)\n    return dict(type=\"function\", function=function_params)\n\nfuncs_ok = {'search_web'}\n\n\ndef get_tools():\n    return [get_schema(search_web)]\n\nget_tools()\n\n[{'type': 'function',\n  'function': {'name': 'search_web',\n   'description': '\\n    Searches the web using Tavily and returns a formatted result.\\n\\n    Args:\\n        query (str): The search query string.\\n\\n    Returns:\\n        str: Formatted search results for LLM input.\\n    ',\n   'parameters': {'properties': {'query': {'title': 'Query',\n      'type': 'string'}},\n    'required': ['query'],\n    'title': 'Input for `search_web`',\n    'type': 'object'}}}]\n\n\nNow, any compatible LLM can automatically invoke search_web when needed.\n\n\n\nLet’s use a simple custom client (which you can learn more about in this blog post) to try out our search tool.\n\n\nCode\nfrom IPython.display import display, Markdown\n\nclass ChatMessages:\n\n    def __init__(self):\n        \"\"\"Initializes the Chat.\"\"\"\n        self._messages = []\n\n    def _append_message(self, role, content):\n        \"\"\"Appends a message with specified role and content to messages list.\"\"\"\n        self._messages.append({\"role\": role, \"content\": content})\n\n    def append_system_message(self, content):\n        \"\"\"Appends a system message with specified content to messages list.\"\"\"\n        self._append_message(\"system\", content)\n    \n    def append_tool_message(self, content, tool_call_id):\n        \"\"\"Appends a tool message with specified content to messages list.\"\"\"\n        self._messages.append({\"role\": \"tool\", \"content\": content, \"tool_call_id\": tool_call_id})\n\n    def append_user_message(self, content=None, base64_image=None):\n        \"\"\"Appends a user message with specified content to messages list.\"\"\"\n        if base64_image:\n            image_content = [\n                {\"type\": \"text\", \"text\": content},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}}\n            ]\n            self._messages.append({\"role\": \"user\", \"content\": image_content})  \n        else:\n            self._append_message(\"user\", content)\n\n#    def append_assistant_message(self, content=None, tool_calls=None):\n#        \"\"\"Appends an assistant message with specified content to messages list.\"\"\"\n#        if content:\n#            self._append_message(\"assistant\", content)\n#        else:\n#            self._messages.append({\"role\": \"assistant\", \"tool_calls\": tool_calls})\n\n    def append_assistant_message(self, content=None, tool_calls=None):\n        \"\"\"Appends an assistant message with optional content and tool calls.\"\"\"\n        message = {\"role\": \"assistant\"}\n        \n        if content is not None:\n            message[\"content\"] = content\n        \n        if tool_calls is not None:\n            message[\"tool_calls\"] = tool_calls\n\n        self._messages.append(message)\n\n    def get_messages(self):\n        \"\"\"Returns a shallow copy of the messages list.\"\"\"\n        return self._messages[:]\n    \n    def get_last_assistant_message(self):\n        \"\"\"Returns the content of the last assistant message\"\"\"\n        return self._messages[-1]['content']\n    \n    def get_debug_view(self):\n        \"\"\"Returns the debug view of the chat messages formatted as Markdown.\"\"\"\n        debug_view = []\n        for message in self._messages:\n            role = message.get('role')\n            content = message.get('content', '')\n\n            if role == 'system' or role == 'user':\n                debug_view.append(f\"**{role}**: {content}\\n\")\n\n            elif role == 'assistant':\n                if 'tool_calls' in message:\n                    debug_view.append(\"**tool calls**\\n\")\n                    for i, tool_call in enumerate(message['tool_calls'], start=1):\n                        function_name = tool_call.function.name\n                        arguments = tool_call.function.arguments\n                        tool_call_id = tool_call.id\n                        debug_view.append(f\"{i}. tool: {function_name}: {arguments} (tool call id: {tool_call_id})\\n\")\n                else:\n                    debug_view.append(f\"**assistant**: {content}\\n\")\n\n            elif role == 'tool':\n                tool_call_id = message.get('tool_call_id', '')\n                debug_view.append(f\"**tool result**: {content} (tool call id: {tool_call_id})\\n\")\n\n        return Markdown('\\n'.join(debug_view))\n\n\n\n\nCode\nmodel_name = \"gpt-4o\"\n\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv(\".env\")\n\nfrom openai import chat\n\nclass ChatClient:\n\n    def __init__(self, system_message=None, tools=None):\n        \"\"\"Initializes the Chat with the system message.\"\"\"\n        self._chat_messages = ChatMessages()\n        if system_message:\n            self._chat_messages.append_system_message(system_message)\n        self._tools = tools\n\n    def call_tool(self, tool_call):\n        \"\"\"returns the result of an LLM tool call\"\"\"\n        fc = tool_call.function #Updated\n        if fc.name not in funcs_ok: return print(f'Not allowed: {fc.name}')\n        f = globals()[fc.name]\n        return f(**json.loads(fc.arguments))\n\n    def call_tools(self, tool_calls):\n        \"\"\"Processes the tool calls of the LLM response and calls the LLM API again\"\"\"\n        for tool_call in tool_calls:\n            chat_client._chat_messages.append_tool_message(\n                content=str(self.call_tool(tool_call)),\n                tool_call_id=tool_call.id)\n            \n        self.ask_gpt()\n\n    def get_model_response(self):\n        \"\"\"Calls the LLM chat completion API\"\"\"\n        return chat.completions.create(\n            model=model_name,\n            messages=self._chat_messages.get_messages(),\n            tools=self._tools)\n\n    def ask_gpt(self, prompt=None, base64_image=None):\n        \n        if base64_image:\n            self._chat_messages.append_user_message(content=prompt, base64_image=base64_image)\n\n        if prompt:\n            self._chat_messages.append_user_message(prompt)\n\n        c = self.get_model_response()\n        content = c.choices[0].message.content\n        tool_calls = c.choices[0].message.tool_calls\n\n        self._chat_messages.append_assistant_message(\n            content=content,\n            tool_calls=tool_calls)\n        \n        if tool_calls:\n            self.call_tools(tool_calls)\n\n        return Markdown(self._chat_messages.get_last_assistant_message())\n\n\nLet’s quickly confirm that we can talk to the large language model:\n\nchat_client = ChatClient(\"Answer in a very concise and accurate way\")\nchat_client.ask_gpt(\"Name the planets in the solar system\")\n\nMercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune.\n\n\n\n\n\nNow that we have established communication with the LLM, let’s try out our mock search\n\nsystem_prompt = \"\"\"You are a helpful assistant. \\\n                   When you search the web, make sure to cite your sources.\"\"\"\nchat_client = ChatClient(system_message=system_prompt, tools=get_tools())\nchat_client.ask_gpt(\"Search the web on a random topic and tell me what you find. \\\n                     - do not be surprised if the result does not match the query\")\n\nI searched for “bioluminescent algae” but received results about Lionel Messi, a famed Argentine-born football player. Messi is widely regarded as one of the greatest footballers of all time, having won numerous accolades including multiple Ballon d’Or and FIFA Men’s Player awards. Despite his achievements in football, my search did not yield any information relevant to bioluminescent algae. This kind of unexpected result can sometimes happen during searches. If you’d like to try another topic, feel free to ask!\n\n\nWithout real-time search (use_api = False), the model always receives the search results about Messi.\n\n\n\nLet’s put everything to the test with a different search query: “Who won the German elections in 2025?”\nBefore enabling the real-time web search, we’ll first run this prompt with no tools attached. This allows us to confirm the baseline: The LLM cannot answer the question, because of its earlier cut-off date.\n\nchat_client = ChatClient(system_message=system_prompt)\nchat_client.ask_gpt(\"Who won the German elections in 2025?\")\n\nI’m unable to provide information on events beyond October 2023, as my training data only goes up until that point. You may want to check the latest news or the official German election website for up-to-date information on the 2025 German elections.\n\n\nWhen we activate tool use, we get an answer which is grounded in our Internet search.\n\nuse_api = True\nchat_client = ChatClient(system_message=system_prompt, tools=get_tools())\nchat_client.ask_gpt(\"Who won the German elections in 2025?\")\n\nThe German federal election in 2025 was won by the Christian Democratic Union (CDU), led by Friedrich Merz. The CDU secured 28.5% of the popular vote and won 208 seats in the Bundestag, making them the majority party in the election source.\n\n\nThe LLM now successfully retrieves and incorporates current information directly from the web.\n\n\n\nWhen we set out to implement real-time web search for large language models, we defined two key principles:\n\nWeb search is just a tool for the LLM.\nWeb search is just a straightforward API call.\n\nBy sticking closely to these ideas, we’ve successfully implemented a real-time web search functionality for large language models in just a few lines of code. We created a practical and lightweight integration that significantly improves the usefulness of LLMs when accessed via APIs.\nThis approach shows that enhancing your model’s capabilities doesn’t require complicated setups or extensive boilerplate. With minimal effort, you can empower your models to get access to up-to-date, accurate information, making them even more valuable in everyday use.\nFeel free to use this simple integration pattern as a starting point to extend your own LLM-based projects further.\n\n\n\n[1] Yao, S., Yu, T., Wu, Y., Zhao, Z., Yu, K., & Liu, S. (2022). ReAct: Synergizing Reasoning and Acting in Language Models\n[2] Howard, J. (2023). A Hackers’ Guide to Language Models"
  },
  {
    "objectID": "posts/2025-03-28-llm-web-search/index.html#conceptual-implementation",
    "href": "posts/2025-03-28-llm-web-search/index.html#conceptual-implementation",
    "title": "Implementing Web Search for Large Language Models from Scratch",
    "section": "",
    "text": "To break down the challenge, consider two key ideas:\n\nA web search is simply a tool available to the LLM.\nA web search operation is essentially just a straightforward API call.\n\nKeeping these principles in mind, integrating web search fits neatly into the established “ReAct” [1] prompting framework, where an LLM iteratively reasons (thinks) and acts (uses tools) until it achieves its goal. The following diagram illustrates clearly how this integration works in practice:\n\n\n\n\n\nsequenceDiagram\n    autonumber\n\n    actor User\n    participant WebSearch as Web Search\n    participant Function as Tool\n    participant ChatClient as Chat Client\n    participant LLM\n\n\n    User-&gt;&gt;Function: Define the web search (API call)\n    Function-&gt;&gt;User: Retrieve JSON function definition\n    User-&gt;&gt;ChatClient: Create Chat Client including web search tool\n    User-&gt;&gt;ChatClient: Send prompt\n    ChatClient-&gt;&gt;LLM: Send prompt with web search tool\n\n    loop Reasoning and Acting\n        LLM-&gt;&gt;LLM: Reasoning: Analyze prompt and tools\n        LLM-&gt;&gt;ChatClient: Acting: Generate tool call: web search\n        ChatClient-&gt;&gt;Function: Call web search function\n        Function-&gt;&gt;WebSearch: Call web search API\n        WebSearch-&gt;&gt;Function: Return web search result\n        Function-&gt;&gt;ChatClient: Return web search result\n        ChatClient-&gt;&gt;LLM: Acting: Pass on web search result\n        LLM-&gt;&gt;LLM: Reasoning: Incorporate result and continue reasoning\n    end\n\n    LLM-&gt;&gt;ChatClient: Return final result\n    ChatClient-&gt;&gt;User: Output final result\n\n\n\n\n\n\n\nNow that we have a clear conceptual understanding, let’s walk through the actual implementation step-by-step."
  },
  {
    "objectID": "posts/2025-03-28-llm-web-search/index.html#step-1-choosing-the-right-search-engine",
    "href": "posts/2025-03-28-llm-web-search/index.html#step-1-choosing-the-right-search-engine",
    "title": "Implementing Web Search for Large Language Models from Scratch",
    "section": "",
    "text": "When choosing a web search API for this project, simplicity was my top priority. My goal was straightforward: I wanted an API where I could simply input a search query and immediately receive a response that was easy for both me and the LLM to parse.\nInitially, I considered the obvious choices: popular search engines like Google and Bing. However, I quickly realized that both come with a level of complexity that exceeded my needs.\nContinuing my search, I came across Tavily, a service I have no affiliation with, but found refreshingly straightforward. Tavily offers an API tailored specifically for LLM use cases, returning concise, well-structured results. It also provides a generous free tier of 1,000 requests per month, making it ideal for experimentation and quick prototyping.\nAnother potential option I considered was Brave Search, which also appears to offer an accessible API with minimal overhead. It may be worth exploring if you’re looking for alternatives with similar simplicity.\nUltimately, I chose Tavily because of its minimal setup, clean responses, and ease of integration—all of which aligned perfectly with the goals of this project."
  },
  {
    "objectID": "posts/2025-03-28-llm-web-search/index.html#step-2-test-driving-the-tavily-api",
    "href": "posts/2025-03-28-llm-web-search/index.html#step-2-test-driving-the-tavily-api",
    "title": "Implementing Web Search for Large Language Models from Scratch",
    "section": "",
    "text": "Let’s get started by installing the Tavily Python library: (As usual, there’s a Jupyter notebook version of this blog post if you want to run the code yourself.)\n!pip install tavily-python\nBefore integrating Tavily with our LLM, it’s a good practice to test the service independently. While the free tier provides plenty of room for testing, I chose to initially use a cached mock response to minimize unnecessary API calls. This approach ensures our logic and parsing methods work correctly before we start consuming real API credits.\nHere is an example of a mock response from Tavily for the query “Who is Leo Messi?”:\n\n\nCode\nmock_response = \"\"\"{\n    \"answer\": null,\n    \"follow_up_questions\": null,\n    \"images\": [],\n    \"query\": \"Who is Leo Messi?\",\n    \"response_time\": 1.75,\n    \"results\": [\n        {\n            \"content\": \"Lionel Messi is an Argentine-born football (soccer) player who has been named the world’s best men’s player of the year seven times (2009–12, 2015, 2019, and 2021). In 2022 he helped Argentina win the World Cup. Naturally left-footed, quick, and precise in control of the ball, Messi is known as a keen pass distributor and can readily thread his way through packed defenses. He led Argentina’s national team to win the 2021 Copa América and the 2022 World Cup, when he again won the Golden Ball award.\",\n            \"raw_content\": null,\n            \"score\": 0.84027237,\n            \"title\": \"Lionel Messi | Biography, Trophies, Records, Ballon d'Or, Inter Miami ...\",\n            \"url\": \"https://www.britannica.com/biography/Lionel-Messi\"\n        },\n        {\n            \"content\": \"Widely regarded as one of the greatest players of all time, Messi set numerous records for individual accolades won throughout his professional footballing career such as eight Ballon d'Or awards and four the Best FIFA Men's Player awards. A prolific goalscorer and creative playmaker, Messi has scored over 850 senior career goals and has provided over 380 assists for club and country. [16] Born in Rosario, Argentina, Messi relocated to Spain to join Barcelona at age 13, and made his competitive debut at age 17 in October 2004. An Argentine international, Messi is the national team's all-time leading goalscorer and most-capped player. His style of play as a diminutive, left-footed dribbler drew career-long comparisons with compatriot Diego Maradona, who described Messi as his successor.\",\n            \"raw_content\": null,\n            \"score\": 0.8091708,\n            \"title\": \"Lionel Messi - Wikipedia\",\n            \"url\": \"https://en.wikipedia.org/wiki/Lionel_Messi\"\n        }\n    ]\n}\"\"\"\n\n\n\n\nCode\nfrom IPython.display import display, Code\n\ndef display_json(data):\n    \"\"\"\n    Nicely displays JSON content: indented + syntax-highlighted.\n    \n    Args:\n        data (str | dict | list): The JSON or string to display.\n    \"\"\"\n    # Parse if input is a string\n    if isinstance(data, str):\n        try:\n            data = ast.literal_eval(data)\n        except Exception as e:\n            print(\"Failed to parse string input as JSON-like structure.\")\n            print(\"Error:\", e)\n            return\n\n    # Convert to pretty JSON string\n    pretty_json = json.dumps(data, indent=4, sort_keys=True, ensure_ascii=False)\n\n    # Display with syntax highlighting\n    display(Code(pretty_json, language='json'))\n\nimport json\nimport ast\n\ndef parse_mock_response(response_str: str):\n    try:\n        return json.loads(response_str)\n    except json.JSONDecodeError:\n        try:\n            return ast.literal_eval(response_str)\n        except Exception as e:\n            print(\"❌ Failed to parse mock response:\", e)\n            return {}\n\n\nLet’s run our first query.\n\n\nCode\nimport os\nfrom dotenv import load_dotenv\n\nuse_api = False\n\nload_dotenv()\napi_key = os.getenv(\"TAVILY_API_KEY\")\nif not api_key:\n    raise ValueError(\"TAVILY_API_KEY not found in .env file.\")\n\n\n\nfrom tavily import TavilyClient\n\nquery = \"Who is Leo Messi?\"\n\nif use_api:\n    tavily_client = TavilyClient(api_key=api_key)\n    response = tavily_client.search(\n        query=query,\n        search_depth=\"basic\",\n        include_answer=False,\n        include_raw_content=False,\n        max_results=2\n    )\nelse:\n    response = parse_mock_response(mock_response)\n\ndisplay_json(response)\n\n{\n    \"answer\": null,\n    \"follow_up_questions\": null,\n    \"images\": [],\n    \"query\": \"Who is Leo Messi?\",\n    \"response_time\": 1.75,\n    \"results\": [\n        {\n            \"content\": \"Lionel Messi is an Argentine-born football (soccer) player who has been named the world’s best men’s player of the year seven times (2009–12, 2015, 2019, and 2021). In 2022 he helped Argentina win the World Cup. Naturally left-footed, quick, and precise in control of the ball, Messi is known as a keen pass distributor and can readily thread his way through packed defenses. He led Argentina’s national team to win the 2021 Copa América and the 2022 World Cup, when he again won the Golden Ball award.\",\n            \"raw_content\": null,\n            \"score\": 0.84027237,\n            \"title\": \"Lionel Messi | Biography, Trophies, Records, Ballon d'Or, Inter Miami ...\",\n            \"url\": \"https://www.britannica.com/biography/Lionel-Messi\"\n        },\n        {\n            \"content\": \"Widely regarded as one of the greatest players of all time, Messi set numerous records for individual accolades won throughout his professional footballing career such as eight Ballon d'Or awards and four the Best FIFA Men's Player awards. A prolific goalscorer and creative playmaker, Messi has scored over 850 senior career goals and has provided over 380 assists for club and country. [16] Born in Rosario, Argentina, Messi relocated to Spain to join Barcelona at age 13, and made his competitive debut at age 17 in October 2004. An Argentine international, Messi is the national team's all-time leading goalscorer and most-capped player. His style of play as a diminutive, left-footed dribbler drew career-long comparisons with compatriot Diego Maradona, who described Messi as his successor.\",\n            \"raw_content\": null,\n            \"score\": 0.8091708,\n            \"title\": \"Lionel Messi - Wikipedia\",\n            \"url\": \"https://en.wikipedia.org/wiki/Lionel_Messi\"\n        }\n    ]\n}"
  },
  {
    "objectID": "posts/2025-03-28-llm-web-search/index.html#step-3-formatting-search-results-for-the-llm",
    "href": "posts/2025-03-28-llm-web-search/index.html#step-3-formatting-search-results-for-the-llm",
    "title": "Implementing Web Search for Large Language Models from Scratch",
    "section": "",
    "text": "Although large language models are capable of parsing raw JSON, this format isn’t ideal. It introduces unnecessary token overhead and lacks the readability and structure that both humans and LLMs benefit from. To make the results easier to consume, we’ll reformat the API response into clean, human-readable Markdown. This improves clarity, ensures more predictable behavior from the LLM, and also makes the output easier to debug and inspect during development.\n\n\nCode\ndef format_tavily_results(response: dict, max_results: int = 5, snippet_length: int = 5000) -&gt; str:\n    \"\"\"\n    Formats the Tavily search API JSON response into a readable, LLM-friendly string.\n\n    Args:\n        response (dict): The Tavily API response.\n        max_results (int): Maximum number of results to include.\n        snippet_length (int): Max number of characters to show from each result's content.\n\n    Returns:\n        str: Formatted, readable string for LLM consumption.\n    \"\"\"\n    results = response.get(\"results\", [])\n    if not results:\n        return \"No results found.\"\n\n    formatted = \"### Web Search Results:\\n\\n\"\n    for i, result in enumerate(results[:max_results], start=1):\n        title = result.get(\"title\", \"Untitled\")\n        url = result.get(\"url\", \"\")\n        content = result.get(\"content\", \"\") or \"\"\n        snippet = content.strip().replace(\"\\n\", \" \")[:snippet_length].rstrip()\n        \n        # Clean up unfinished sentences if needed\n        if snippet and not snippet.endswith(('.', '!', '?')):\n            snippet += \"...\"\n\n        formatted += f\"{i}. **[{title}]({url})**\\n   - {snippet}\\n\\n\"\n\n    return formatted.strip()\n\n\n\nformatted = format_tavily_results(response, snippet_length=300)\nprint(formatted)\n\n### Web Search Results:\n\n1. **[Lionel Messi | Biography, Trophies, Records, Ballon d'Or, Inter Miami ...](https://www.britannica.com/biography/Lionel-Messi)**\n   - Lionel Messi is an Argentine-born football (soccer) player who has been named the world’s best men’s player of the year seven times (2009–12, 2015, 2019, and 2021). In 2022 he helped Argentina win the World Cup. Naturally left-footed, quick, and precise in control of the ball, Messi is known as a ke...\n\n2. **[Lionel Messi - Wikipedia](https://en.wikipedia.org/wiki/Lionel_Messi)**\n   - Widely regarded as one of the greatest players of all time, Messi set numerous records for individual accolades won throughout his professional footballing career such as eight Ballon d'Or awards and four the Best FIFA Men's Player awards. A prolific goalscorer and creative playmaker, Messi has scor...\n\n\nThe LLM can now easily consume the text summaries."
  },
  {
    "objectID": "posts/2025-03-28-llm-web-search/index.html#step-4-defining-web-search-tool-for-the-llm",
    "href": "posts/2025-03-28-llm-web-search/index.html#step-4-defining-web-search-tool-for-the-llm",
    "title": "Implementing Web Search for Large Language Models from Scratch",
    "section": "",
    "text": "Next, we’ll encapsulate our functionality into a single, reusable function that performs both API calls and formatting. Additionally, we need to define a proper documentation so that the LLM can understand how to use our tool:\n\ndef search_web(query: str) -&gt; str:\n    \"\"\"\n    Searches the web using Tavily and returns a formatted result.\n\n    Args:\n        query (str): The search query string.\n\n    Returns:\n        str: Formatted search results for LLM input.\n    \"\"\"\n    if use_api:\n        tavily_client = TavilyClient(api_key=api_key)\n        response = tavily_client.search(\n            query=query,\n            search_depth=\"basic\",\n            include_answer=False,\n            include_raw_content=False,\n            max_results=5\n        )\n    else:\n        response = parse_mock_response(mock_response)\n\n    return format_tavily_results(response)\n\nHere is an example-call including the result the LLM would receive:\n\n\nCode\nresult = search_web(\"Who is Leo Messi?\")\nprint(result)\n\n\n### Web Search Results:\n\n1. **[Lionel Messi | Biography, Trophies, Records, Ballon d'Or, Inter Miami ...](https://www.britannica.com/biography/Lionel-Messi)**\n   - Lionel Messi is an Argentine-born football (soccer) player who has been named the world’s best men’s player of the year seven times (2009–12, 2015, 2019, and 2021). In 2022 he helped Argentina win the World Cup. Naturally left-footed, quick, and precise in control of the ball, Messi is known as a keen pass distributor and can readily thread his way through packed defenses. He led Argentina’s national team to win the 2021 Copa América and the 2022 World Cup, when he again won the Golden Ball award.\n\n2. **[Lionel Messi - Wikipedia](https://en.wikipedia.org/wiki/Lionel_Messi)**\n   - Widely regarded as one of the greatest players of all time, Messi set numerous records for individual accolades won throughout his professional footballing career such as eight Ballon d'Or awards and four the Best FIFA Men's Player awards. A prolific goalscorer and creative playmaker, Messi has scored over 850 senior career goals and has provided over 380 assists for club and country. [16] Born in Rosario, Argentina, Messi relocated to Spain to join Barcelona at age 13, and made his competitive debut at age 17 in October 2004. An Argentine international, Messi is the national team's all-time leading goalscorer and most-capped player. His style of play as a diminutive, left-footed dribbler drew career-long comparisons with compatriot Diego Maradona, who described Messi as his successor."
  },
  {
    "objectID": "posts/2025-03-28-llm-web-search/index.html#step-5-exposing-the-web-search-tool-to-the-llm",
    "href": "posts/2025-03-28-llm-web-search/index.html#step-5-exposing-the-web-search-tool-to-the-llm",
    "title": "Implementing Web Search for Large Language Models from Scratch",
    "section": "",
    "text": "To expose our web search to an LLM, we need to provide the tool definition to the LLM. Jeremy Howard shared a practical and flexible approach for this in his Hacker’s Guide. The core idea is to use Python’s introspection capabilities to extract the function signature and documentation, and convert it into a schema the LLM can understand. The version used here builds on that idea, with minor updates to match recent changes in the OpenAI tools API.\nThe most important part of this process is clearly documenting the function’s interface: Its name, parameters, and behavior—so that the LLM knows how and when to call it. This allows the model to use the tool automatically, without any additional prompting logic or manual wiring.\n\nfrom pydantic import create_model\nimport inspect, json\nfrom inspect import Parameter\n\ndef get_schema(f):\n    kw = {n:(o.annotation, ... if o.default==Parameter.empty else o.default)\n          for n,o in inspect.signature(f).parameters.items()}\n    # update: schema -&gt; model_json_schema\n    s = create_model(f'Input for `{f.__name__}`', **kw).model_json_schema()\n    # update: added function level in tools json\n    function_params = dict(name=f.__name__, description=f.__doc__, parameters=s)\n    return dict(type=\"function\", function=function_params)\n\nfuncs_ok = {'search_web'}\n\n\ndef get_tools():\n    return [get_schema(search_web)]\n\nget_tools()\n\n[{'type': 'function',\n  'function': {'name': 'search_web',\n   'description': '\\n    Searches the web using Tavily and returns a formatted result.\\n\\n    Args:\\n        query (str): The search query string.\\n\\n    Returns:\\n        str: Formatted search results for LLM input.\\n    ',\n   'parameters': {'properties': {'query': {'title': 'Query',\n      'type': 'string'}},\n    'required': ['query'],\n    'title': 'Input for `search_web`',\n    'type': 'object'}}}]\n\n\nNow, any compatible LLM can automatically invoke search_web when needed."
  },
  {
    "objectID": "posts/2025-03-28-llm-web-search/index.html#step-6-reuse-chat-client-for-llm-communication",
    "href": "posts/2025-03-28-llm-web-search/index.html#step-6-reuse-chat-client-for-llm-communication",
    "title": "Implementing Web Search for Large Language Models from Scratch",
    "section": "",
    "text": "Let’s use a simple custom client (which you can learn more about in this blog post) to try out our search tool.\n\n\nCode\nfrom IPython.display import display, Markdown\n\nclass ChatMessages:\n\n    def __init__(self):\n        \"\"\"Initializes the Chat.\"\"\"\n        self._messages = []\n\n    def _append_message(self, role, content):\n        \"\"\"Appends a message with specified role and content to messages list.\"\"\"\n        self._messages.append({\"role\": role, \"content\": content})\n\n    def append_system_message(self, content):\n        \"\"\"Appends a system message with specified content to messages list.\"\"\"\n        self._append_message(\"system\", content)\n    \n    def append_tool_message(self, content, tool_call_id):\n        \"\"\"Appends a tool message with specified content to messages list.\"\"\"\n        self._messages.append({\"role\": \"tool\", \"content\": content, \"tool_call_id\": tool_call_id})\n\n    def append_user_message(self, content=None, base64_image=None):\n        \"\"\"Appends a user message with specified content to messages list.\"\"\"\n        if base64_image:\n            image_content = [\n                {\"type\": \"text\", \"text\": content},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}}\n            ]\n            self._messages.append({\"role\": \"user\", \"content\": image_content})  \n        else:\n            self._append_message(\"user\", content)\n\n#    def append_assistant_message(self, content=None, tool_calls=None):\n#        \"\"\"Appends an assistant message with specified content to messages list.\"\"\"\n#        if content:\n#            self._append_message(\"assistant\", content)\n#        else:\n#            self._messages.append({\"role\": \"assistant\", \"tool_calls\": tool_calls})\n\n    def append_assistant_message(self, content=None, tool_calls=None):\n        \"\"\"Appends an assistant message with optional content and tool calls.\"\"\"\n        message = {\"role\": \"assistant\"}\n        \n        if content is not None:\n            message[\"content\"] = content\n        \n        if tool_calls is not None:\n            message[\"tool_calls\"] = tool_calls\n\n        self._messages.append(message)\n\n    def get_messages(self):\n        \"\"\"Returns a shallow copy of the messages list.\"\"\"\n        return self._messages[:]\n    \n    def get_last_assistant_message(self):\n        \"\"\"Returns the content of the last assistant message\"\"\"\n        return self._messages[-1]['content']\n    \n    def get_debug_view(self):\n        \"\"\"Returns the debug view of the chat messages formatted as Markdown.\"\"\"\n        debug_view = []\n        for message in self._messages:\n            role = message.get('role')\n            content = message.get('content', '')\n\n            if role == 'system' or role == 'user':\n                debug_view.append(f\"**{role}**: {content}\\n\")\n\n            elif role == 'assistant':\n                if 'tool_calls' in message:\n                    debug_view.append(\"**tool calls**\\n\")\n                    for i, tool_call in enumerate(message['tool_calls'], start=1):\n                        function_name = tool_call.function.name\n                        arguments = tool_call.function.arguments\n                        tool_call_id = tool_call.id\n                        debug_view.append(f\"{i}. tool: {function_name}: {arguments} (tool call id: {tool_call_id})\\n\")\n                else:\n                    debug_view.append(f\"**assistant**: {content}\\n\")\n\n            elif role == 'tool':\n                tool_call_id = message.get('tool_call_id', '')\n                debug_view.append(f\"**tool result**: {content} (tool call id: {tool_call_id})\\n\")\n\n        return Markdown('\\n'.join(debug_view))\n\n\n\n\nCode\nmodel_name = \"gpt-4o\"\n\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv(\".env\")\n\nfrom openai import chat\n\nclass ChatClient:\n\n    def __init__(self, system_message=None, tools=None):\n        \"\"\"Initializes the Chat with the system message.\"\"\"\n        self._chat_messages = ChatMessages()\n        if system_message:\n            self._chat_messages.append_system_message(system_message)\n        self._tools = tools\n\n    def call_tool(self, tool_call):\n        \"\"\"returns the result of an LLM tool call\"\"\"\n        fc = tool_call.function #Updated\n        if fc.name not in funcs_ok: return print(f'Not allowed: {fc.name}')\n        f = globals()[fc.name]\n        return f(**json.loads(fc.arguments))\n\n    def call_tools(self, tool_calls):\n        \"\"\"Processes the tool calls of the LLM response and calls the LLM API again\"\"\"\n        for tool_call in tool_calls:\n            chat_client._chat_messages.append_tool_message(\n                content=str(self.call_tool(tool_call)),\n                tool_call_id=tool_call.id)\n            \n        self.ask_gpt()\n\n    def get_model_response(self):\n        \"\"\"Calls the LLM chat completion API\"\"\"\n        return chat.completions.create(\n            model=model_name,\n            messages=self._chat_messages.get_messages(),\n            tools=self._tools)\n\n    def ask_gpt(self, prompt=None, base64_image=None):\n        \n        if base64_image:\n            self._chat_messages.append_user_message(content=prompt, base64_image=base64_image)\n\n        if prompt:\n            self._chat_messages.append_user_message(prompt)\n\n        c = self.get_model_response()\n        content = c.choices[0].message.content\n        tool_calls = c.choices[0].message.tool_calls\n\n        self._chat_messages.append_assistant_message(\n            content=content,\n            tool_calls=tool_calls)\n        \n        if tool_calls:\n            self.call_tools(tool_calls)\n\n        return Markdown(self._chat_messages.get_last_assistant_message())\n\n\nLet’s quickly confirm that we can talk to the large language model:\n\nchat_client = ChatClient(\"Answer in a very concise and accurate way\")\nchat_client.ask_gpt(\"Name the planets in the solar system\")\n\nMercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune."
  },
  {
    "objectID": "posts/2025-03-28-llm-web-search/index.html#step-7-chat-with-mock-web-search",
    "href": "posts/2025-03-28-llm-web-search/index.html#step-7-chat-with-mock-web-search",
    "title": "Implementing Web Search for Large Language Models from Scratch",
    "section": "",
    "text": "Now that we have established communication with the LLM, let’s try out our mock search\n\nsystem_prompt = \"\"\"You are a helpful assistant. \\\n                   When you search the web, make sure to cite your sources.\"\"\"\nchat_client = ChatClient(system_message=system_prompt, tools=get_tools())\nchat_client.ask_gpt(\"Search the web on a random topic and tell me what you find. \\\n                     - do not be surprised if the result does not match the query\")\n\nI searched for “bioluminescent algae” but received results about Lionel Messi, a famed Argentine-born football player. Messi is widely regarded as one of the greatest footballers of all time, having won numerous accolades including multiple Ballon d’Or and FIFA Men’s Player awards. Despite his achievements in football, my search did not yield any information relevant to bioluminescent algae. This kind of unexpected result can sometimes happen during searches. If you’d like to try another topic, feel free to ask!\n\n\nWithout real-time search (use_api = False), the model always receives the search results about Messi."
  },
  {
    "objectID": "posts/2025-03-28-llm-web-search/index.html#step-8-chat-with-real-web-search",
    "href": "posts/2025-03-28-llm-web-search/index.html#step-8-chat-with-real-web-search",
    "title": "Implementing Web Search for Large Language Models from Scratch",
    "section": "",
    "text": "Let’s put everything to the test with a different search query: “Who won the German elections in 2025?”\nBefore enabling the real-time web search, we’ll first run this prompt with no tools attached. This allows us to confirm the baseline: The LLM cannot answer the question, because of its earlier cut-off date.\n\nchat_client = ChatClient(system_message=system_prompt)\nchat_client.ask_gpt(\"Who won the German elections in 2025?\")\n\nI’m unable to provide information on events beyond October 2023, as my training data only goes up until that point. You may want to check the latest news or the official German election website for up-to-date information on the 2025 German elections.\n\n\nWhen we activate tool use, we get an answer which is grounded in our Internet search.\n\nuse_api = True\nchat_client = ChatClient(system_message=system_prompt, tools=get_tools())\nchat_client.ask_gpt(\"Who won the German elections in 2025?\")\n\nThe German federal election in 2025 was won by the Christian Democratic Union (CDU), led by Friedrich Merz. The CDU secured 28.5% of the popular vote and won 208 seats in the Bundestag, making them the majority party in the election source.\n\n\nThe LLM now successfully retrieves and incorporates current information directly from the web."
  },
  {
    "objectID": "posts/2025-03-28-llm-web-search/index.html#conclusion",
    "href": "posts/2025-03-28-llm-web-search/index.html#conclusion",
    "title": "Implementing Web Search for Large Language Models from Scratch",
    "section": "",
    "text": "When we set out to implement real-time web search for large language models, we defined two key principles:\n\nWeb search is just a tool for the LLM.\nWeb search is just a straightforward API call.\n\nBy sticking closely to these ideas, we’ve successfully implemented a real-time web search functionality for large language models in just a few lines of code. We created a practical and lightweight integration that significantly improves the usefulness of LLMs when accessed via APIs.\nThis approach shows that enhancing your model’s capabilities doesn’t require complicated setups or extensive boilerplate. With minimal effort, you can empower your models to get access to up-to-date, accurate information, making them even more valuable in everyday use.\nFeel free to use this simple integration pattern as a starting point to extend your own LLM-based projects further."
  },
  {
    "objectID": "posts/2025-03-28-llm-web-search/index.html#references",
    "href": "posts/2025-03-28-llm-web-search/index.html#references",
    "title": "Implementing Web Search for Large Language Models from Scratch",
    "section": "",
    "text": "[1] Yao, S., Yu, T., Wu, Y., Zhao, Z., Yu, K., & Liu, S. (2022). ReAct: Synergizing Reasoning and Acting in Language Models\n[2] Howard, J. (2023). A Hackers’ Guide to Language Models"
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "",
    "text": "Based on the Fast.AI lesson 4, I transferred the approach from Jeremy’s notebook “Getting started with NLP for absolute beginners” to the Kaggle competition “Natural Language Processing with Disaster Tweets”. This post describes my approach and the key learnings.\nMy approach was the following: Classifying tweets or patent phrases is essentially “the same” task:"
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#key-learnings",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#key-learnings",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Key Learnings",
    "text": "Key Learnings\nIn the last about 4 week I probably trained 250+ model and made 70+ submissions, trying to build up intuition on what works and what not. Therefore, please treat this as an empirical report, not as a scientific paper. I try to back my claims with actual data of my model training, but sometimes I can only report on observations and I try to reason why these observations makes sense. Here are my key learnings:\n\nCleaning the data helps, both syntactically and semantically: Not only did I clean up special (rubbish) character, but also re-classified some tweets, mainly automatically.\nUpon cleaning the data, keep a close eye on what is noise and what is signal, for example, converting everything to lower case did not help, because I believe that it removed signal.\nHelping the model understand the data helps by using special tokens, for example by replacing URLs with a special token.\nUsing bigger models helps, on average moving from the small to the base to the large versions of the deberta models increased the score by a few tenth. However, for training large models on Kaggle, you need to apply some tricks not to run out of memory. Additionally, training bigger models is comparable time-consuming.\nSmall batch sizes help to train models more quickly.\nShowing the model more data then just the initial training set helps.\nOverall, the pre-trained models are already very good. My first version submission of the baseline notebook scored 0.81949, in the current final iteration my best submission scored 0.84676. That is an increase of only 2.7 percentage points. The bigger difference is the rank on the leaderboard. At the time of writing 0.81949 would have put me on 218/938 while 0.84278 put me on rank 34/938. If you deduct the submission which scored 100% (29 submission), I am pretty happy with rank 5. 😀\n\n\nOn a side note: I wonder how I would score by hand-labeling all tweets in the test set. Would I beat AI or would AI beat me? Find out at the end."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#syntactical-data-cleansing",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#syntactical-data-cleansing",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Syntactical Data Cleansing",
    "text": "Syntactical Data Cleansing\nThe dataset for the disaster tweets is far from perfect in terms of data quality, I guess it is just real world data 😉. Even though the model was doing a very good job at classifying tweets even without any additional work, some data cleaning was called for.\nThere are quite a few non-sense special characters in the dataset, for example there are HTML representations spaces as %20, or leftovers of an unsuccessful unicode conversion, for example Ûª which should be '. I did replace the rubbish characters where possible, and I simply deleted any other non-ascii characters. I suspect that more could be done here to, for example trying to reconstruct emojis, but I could not find a way to do that. (suggestions welcome)\nI did not take the route of converting everything to lowercase or removing stop-word, because it did not have any positive training effect (just the opposite!). My theory on this would be that I would have removed signal by doing these optimizations (more on that in a bit). On another train of thought, a modern language model should be able to deal with stop word, abbreviation etc. anyway, so why bother to put in time and effort to clean data where it is not required. What is your experience or opinion? Are these kinds of optimizations becoming obsolete as we move from classical machine learning to large language models?"
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#semantic-data-cleansing-correcting-incorrectly-labeled-tweets",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#semantic-data-cleansing-correcting-incorrectly-labeled-tweets",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Semantic Data Cleansing: Correcting incorrectly labeled Tweets",
    "text": "Semantic Data Cleansing: Correcting incorrectly labeled Tweets\nBrowsing through discussions on the competition, there was the claim that some tweets in the training set were not labeled correctly. As it turned out: That is true. But how to correct this without reading every tweet and potentially re-labeling it manually? (Something I definitely would not do!)\nAs it turned out, there are also tweet duplicates which are sometimes not labeled identically. These were pretty easy to catch:\n\nIntroducing a field which counts the occurrences of the tweets. (Since the duplicates were not 100% identical, for example, they contained different URLs, the counting was only possible after converting the URLs to special tokens (see below)).\nCreating a new field which contains the average tweet label\nRounding the label to get a majority vote\n\nThe only catch with this procedure is that tweets which have exactly one duplicate cannot be corrected this way. Well, what can you do about it, I had to re-label these ones manually."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#noise-vs.-signal",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#noise-vs.-signal",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Noise vs. Signal",
    "text": "Noise vs. Signal\nWhen introducing new data or removing data, you need to think about what is noise (i.e. unwanted interference) and what is signal (i.e. data which is helpful). Or putting it into other words: Which elements of the data help the model to learn and which elements create confusion or distraction? Let’s consider the following examples.\nInspired by the notebook “Getting started with NLP for absolute beginners” my first “improvement” was to concatenate the additional columns (keyword, location) of the dataset into the tweets. I was pretty disappointed that the result was worse then the baseline, and even with additional optimization (e.g. special tokens) it was impossible to beat the baseline. Finally, I removed the additional fields, and voilà, the result was immediately better! Why? The keywords added noise, not signal. The keyword “collision”, for example, is also assigned to many non-disaster tweets.\nAnother source of noise can be the URLs which are part of the tweets: The content of the URLs themselves is pretty random, especially for shortened ones. To make the URLs more meaningful for the language model, I tried the following:\n\nEnclosing the URLs with special tokens for URL beginning and URL end -&gt; did not help\nRemoving the URLs completely -&gt; better but not the best solution\nReplacing the URLs with a special token -&gt; empirically the best solution\n\nWhen thinking about it, it makes sense: The fact that there is a URL in a tweet seems to contain signal, but the content of the URL (i.e. random characters) does not.\nAs a final topic for noise vs. signal, let’s consider capitalization. Even though suggested in many sources, converting the text to lower case did not yield a better result (quite the opposite!). Again, I could imagine that capitalization carries signal. Think of capitalization as shouting. Therefore, it makes sense not to convert everything to lower-case because you would remove signal from the tweets. Consider these 2 examples where there is a difference between “burning” and “BURNING”:\n\nimport warnings, logging\n\nwarnings.simplefilter('ignore')\nlogging.disable(logging.WARNING)\n\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\ntokz = AutoTokenizer.from_pretrained('microsoft/deberta-v3-small')\n\ntokz.tokenize(\"The house is BURNING.\")\n\n['▁The', '▁house', '▁is', '▁BURN', 'ING', '.']\n\n\n\ntokz.tokenize(\"The house is burning.\")\n\n['▁The', '▁house', '▁is', '▁burning', '.']"
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#working-with-special-tokens",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#working-with-special-tokens",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Working with Special Tokens",
    "text": "Working with Special Tokens\nOne of the steps of syntactically cleaning the dataset was the removal of special characters, many of them being misrepresentations of unicode characters. There are, however, also meaningful special characters. In the tweets, not prominent the hashtag # and the mention @. Since these special characters carry signal, I decided to turn them into special tokens. This time, however, I did not replace the whole mention or keyword with a special token, but I wrapped the mentions in special tokens:\n\n[MB]: Mention Beginning\n[ME]: Mention End\n[HB]: Hashtag Beginning\n[HE]: Hashtag End\n\nAfter stating what I have been doing in full confidence, I have to admit that, at the time of coding, I do not really know what I was doing, I was rather implementing it in analogy to the two notebooks “Getting started with NLP for absolute beginners” and “Iterate like a grandmaster”.\nWhile writing up this blog post I was doing a little bit of research on special tokens, which was not as straight-forward as I imagined it to be. Chatting with ChatGTP quite was insightful, and I learned about the usage of the predefined special tokens, which can be retrieved by inspecting the attribute tokz.all_special_tokens: (I hope the following is accurate)\n\nThe “[CLS]”-special token (“classification token”) is used to represent the entire input sequence in tasks such as text classification, sentiment analysis and named entity recognition etc. The token is added to the beginning of the input sequence and is then passed through the model along with the rest of the input. In the context of disaster tweets, if a tweet was “There is a fire”, after adding the token, the tweet looked like this: “[CLS] There is a fire”. In the final version of the notebook I did add the [CLS]-token to all the tweets, but I did not notice any improvement in model performance.\nThe “[SEP]”-special token (“separation token”) is used to separate multiple sentences or segments within a single input sequence. The token is added at the end of each sentence or segment, and it is used to indicate the end of one segment and the beginning of another. I did not add any [SEP]-tokens to the tweets because a tweet is a unit of its own. Interestingly, the tokens for . and [SEP] are not the same, which makes sense, because not any .-character is automatically a separator.\n\nThe “[UNK]”-special token is used to represent unknown or out-of-vocabulary words that the model has not seen during training. During the preprocessing step, any word that is not in the vocabulary is replaced with the “[UNK]” token. Somehow I would have expected the tokenizer to replace unknown words with [UNK], but that did not happen whatever I tried.\nThe “[PAD]”-special token is used to pad the input sequences to a fixed length. A BERT (and deberta) model is a transformer-based model which requires that all input sequences have the same length before they can be passed through the model. I did not use this token, but I trust the inner mechanics of the model to take care of this.\nThe “[MASK]”-special token is used in the pre-training process called Masked Language Model (MLM). In this task, a proportion of tokens in the input sequence is replaced with the “[MASK]” token, and the model is trained to predict the original token that was replaced by the “[MASK]” token. This pre-training process allows the model to learn the context of the words in the sentence and understand the relationship between words in the sentence. Since this is a token used in training, I did not use it in my notebooks."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#training-bigger-models",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#training-bigger-models",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Training Bigger Models",
    "text": "Training Bigger Models\nAfter I was done cleaning the data, I tried to work with bigger models, and the result is: Yes, size does matter. The improvements were not dramatically better, nonetheless significant. I worked with microsoft/deberta-v3-small, microsoft/deberta-v3-base and microsoft/deberta-v3-large. Here are the best scores by model (even though the training approaches are not 100% comparable):\n\nmicrosoft/deberta-v3-small: 0.83757\nmicrosoft/deberta-v3-base : 0.84002\nmicrosoft/deberta-v3-large: 0.84676\n\nUpgrading from the small model to the base model was a smooth process. The expected trade off between better results and longer training time materialized as expected. When moving to the large model, that process was not so smooth, because the kaggle runtime was running out of memory both in the GPU and on disk. Here is how I fixed it:\n\nFixing running out of GPU memory: Larger models require more memory in the GPU. This can easily be fixed by reducing the batch size. A small batch size may be a desired training parameter (as I will discuss later), however, if you do not want to increase the number of gradient descent steps, you can use gradient accumulation (also more on that in a later section).\nFixing running out of disk memory: The Hugging Face Trainer saves huge checkpoint files after every 500 training steps by default. When working with small batch sizes or larger numbers of epochs, this can exceed the allowed disk space by Kaggle. The easy fix is to disable the checkpoint file creation. This can be done with the parameter save_steps=-1 which you need to pass to the TrainingArguments class."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#gradient-accumulation",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#gradient-accumulation",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Gradient Accumulation",
    "text": "Gradient Accumulation\nAs presented in Live Coding Session 10, Gradient Accumulation is a technique to train large models on regular hardware.\nIf you run out of memory on a GPU, you need to decrease the batch size. With the same number of epochs this increases the number or optimization cycles. To train a larger model with the same number of iteration cycles, gradient accumulation can be used, because it results in updating the model only after x number of batches.\nAs it turns out, gradient accumulation can also easily be used with a Hugging Face Trainer, you just need to pass the parameter gradient_accumulation_steps to the TrainingArguments-class:\n\n    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n        num_train_epochs=epochs, weight_decay=0.01, report_to='none', gradient_accumulation_steps=gradient_accumulation_steps)\n\nWhen I first tried gradient accumulation, I was puzzled by the result. Smaller batch sizes yielded better results, and in my initial attempts, the bigger models even performed worse than small ones. This was, I assume by now, because I was accumulating the gradients too much.\nWhile gradient accumulation is for sure a good tool for some problems, it was not part of my best submissions because I also came to learn that the small batch sizes were more beneficial in training. What started as a perceived problem (being forced to lower the batch size), turned out to create better results and quicker training."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#implementing-metrics",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#implementing-metrics",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Implementing Metrics",
    "text": "Implementing Metrics\nBefore turning to the discussion of smaller batch sizes, let me first address the computation of metrics, because it will be useful when evaluating the batch size.\nI did not pay too much attention to this at first because implementing metrics (in addition to the loss) was not part of my baseline, and I was focussing on other topics first (as outlines above), but upon writing this post, I noticed the white spot.\nThe evaluation of the kaggle competition is based on the F1 score. Based on this tutorial leveraging the evaluate library, the implementation was surprisingly easy.\n\nImplement a function based on the pattern described in the tutorial or in the evaluate docs\nAs metrics, there is a lot to choose from: This is the full list.\n\nMy final metric implementation looks like this:\nimport numpy as np\nimport evaluate\n\ndef compute_metrics(eval_preds):\n    metric = evaluate.load(\"f1\")\n    preds, labels = eval_preds\n    #predictions = np.argmax(logits, axis=-1) #not needed\n    return metric.compute(predictions=preds, references=labels)\n\nPersonal note: With the disaster tweet dataset, the line predictions = np.argmax(logits, axis=-1) caused a type error. For solving it, this tutorial had exactly the right input I needed to debug the problem."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#using-small-batch-sizes",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#using-small-batch-sizes",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Using Small Batch Sizes",
    "text": "Using Small Batch Sizes\nWhat started as a curiosity got solidified when I found this tweet by Yan LeCun:\n\nTherefore, small batch sizes are a recognized way to train faster in a way which can generalize better. There seems to be a tradeoff between smaller batch size and training time per epoch because the GPU is not used as efficiently with smaller batch sizes. In my disaster tweet project, small batch sizes proved to be helpful.\nIs there a way to intuit why a small batch size is a good thing? Yes, I think so: When you think about the training set as book with 1000 pictures that we want to learn the labels of, the batch size symbolizes how many pictures you look at before you think about what you have seen. With a batch size of 100, you would look at 100 pictures, reflect (i.e. doing the learning, or updating the model based on the gradients), look at another 100 pictures, reflect etc. Within an epoch, i.e. looking though the whole book, you would reflect on what you have seen 10 times. At a batch size of 10, you would look at 10 picture, reflect, look at 10 more pictures, reflect… Within an epoch, you would reflect 100 times (not 10 times), i.e. you would actually do more learning instead of just looking at the pictures. Therefore, it makes sense that a smaller batch size can result in better and quicker training results.\nAnother way to thin about this: By lowering the batch size, the iteration speed is increased. Quoting Andrej Karpathy with some additions [in bracket]:\n\n“[When] dealing with mini batches the quality of our gradient is lower, so the direction is not as reliable, it’s not the actual gradient direction [compared to taking a larger batch], but the gradient direction is good enough even when it’s estimating on only 32 examples that it is useful and so it’s much better to have an approximate gradient and just make more steps than it is to evaluate the exact gradient and take fewer steps. So that’s why in practice this works quite well.”\n\nTo support the claim that smaller batch sized help/work, here is a comparison of training 4 epochs with different batch sizes. As a result, you can see that training time increases, but the training quality increases as well, so much that the model already shows signs of overfitting when training for more than 2 epochs at a batch size of 8 and below:\n\nYou can also see how the model is getting more confident about the predictions in the histograms: The smaller the batch size, the more the predictions peak at 0 and 1, and the number of uncertain cases decreases:\n\nI wonder how general this finding about smaller learning rates is… At least I am now sensitive to the topic and I will continue to watch it."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#showing-the-model-more-data",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#showing-the-model-more-data",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Showing the model more data",
    "text": "Showing the model more data\nIn the notebook “Iterate like a grandmaster”, Jeremy suggested for further improvement: “Before submitting a model, retrain it on the full dataset, rather than just the 75% training subset we’ve used here.” I was wondering how I could follow the advice, and I came up with 2 ideas. The first one did not work, the second did:\n\nTraining 3 models with differently shuffled training and validation sets. In the end the models would be averaged. I did not notice a significant effect, therefore I did not pursue this approach any further. When you think about it, it also makes sense, because in the end you are training identical model, because, if done right, even given different data, the model capabilities should converge to the same capabilities. Only if different training approaches had been used, the ensemble would help.\nTrain a model, then re-shuffle the dataset again to show the model a different training set and a different validation set. Therefore a generically pre-trained model becomes a pre-pre-trained model. In this particular case the deberta model was trained on tweet classification now. It actually worked, shuffling had a good positive effect.\n\nShuffling once: Notebook V5 scored 0.83481\nShuffling twice: Notebook V6 scored 0.83879\nShuffling a third time: Notebook V7 scored 0.84002\nShuffling more often had a negative effect, my notebook V8 scored only 0.83542, most likely due to over-fitting.\n\n\nTo cross check that the improved training results were not just caused by more training, I did the following experiment, training the small model at batch size 32:\n\nTraining 4 epochs: Score 0.82715\nTraining 6 epochs: Score 0.8345\nTraining 8 epochs: Score 0.83297\nTraining 10 epochs: Score 0.8345\nTraining 4 epochs, but re-shuffling 3 times and training 2 more epochs: Score 0.83634\n\nWhile the effect of “more training help” is definitively visible, the attempt including the reshuffling was the best result. This is not surprising, because the model can learn more by looking at more diverse data.\nDiscussing the question of how to use 100% of the training data on the forums lead to a third approach: First, I trained the model as “as usual”, specifically with training for 2 epochs with batch size 4, afterwards I showed the model the full dataset for another epoch. The results were better than doing the re-shuffling approach. Again, as a disclaimer, the training approaches were not identical, so it is difficult to directly compare the results. Nonetheless, my best result of 0.84676 used the following training approach: Training the large model for 2 epochs at bs=4. Afterwards training another epoch with the full training set and at bs=4 but half the learning rate.\nIt would be interesting to systematically explore the matter. Maybe I will return to this another time."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#conclusion",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#conclusion",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Conclusion",
    "text": "Conclusion\nWhen starting out with lessen 4, I did not expect it to become such an extended endeavor. Porting the notebook notebook “Getting started with NLP for absolute beginners” to the Kaggle competition “Natural Language Processing with Disaster Tweets” introduced me to many different aspects of natural language processing in particular and machine learning in general.\nMaybe the most profound learning is that natural language processing with pre-trained models is already very robust. Consider all the effort I put in to improve from a baseline of 0.81949 to 0.84676, where the majority of the improvement was due to better training parameters or bigger models. The dataset is challenging and an honest 100% score is impossible, because there is also quite some subjectivity in there, and quite a few tweets are ambiguous. Putting myself to the test: I tried to compete against my models, just relying on my pre-trained brain: Would I beat 84.6%?\nI must admit, I did not hand-label the whole dataset, but only the first 100 tweets of the test set. Then I compared my results to the leaked 100%-solution. My score was 81% (even worse than my baseline!) - so I lost the competition: AI beat me in this competition - who would have thought?"
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#credits-working-with-ai-as-a-team",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#credits-working-with-ai-as-a-team",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Credits: Working with AI as a Team",
    "text": "Credits: Working with AI as a Team\nWhile I wrote this blog post myself (no longer a given these days…), I need to give credit to ChatGPT for helping me in the research and also for the implementation of some functions, especially the regular expressions. While not all the replies from ChatGPT were correct, they were mostly helpful and frequently pointed me in the right directions. I also enjoy just ChatGTP is a non-annoying website in the sense that is does not ask for cookies after each question, and it does not want me to sign up for newsletters etc. More time is spent on thinking and problem solving than on these distractions.\nThe title picture of this blog post was generated by DALL-E 2 upon the prompt: “twitter logo on fire digital art”."
  },
  {
    "objectID": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html",
    "href": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html",
    "title": "How to Convert a Wordpress Blog into Markdown",
    "section": "",
    "text": "In this blog post, I will guide you through the steps to convert a Wordpress blog into markdown. While this my seem like a unrelated subject of this blog, it is a preparative for writing a Retrieval Augmented Generation (RAG) blog post / notebook.\nWhy did I turn this conversion into a blog post of its own? First of all, the conversion process was more difficult and extensive than expected, therefore I felt that this is worth sharing. Additionally, it highlights (again) that data is key in any machine learning project, and that data preparation can be a project of its own.\nBy now, the Wittmann-Tours blog is available my Wittmann-Tours GitHub repo under license CC-BY NC."
  },
  {
    "objectID": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#before-we-start",
    "href": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#before-we-start",
    "title": "How to Convert a Wordpress Blog into Markdown",
    "section": "Before we Start",
    "text": "Before we Start\nPlease treat this blog post as the personal documentation of how I did the conversion. I was somewhat surprised that there were so few resources on the internet covering the topic of converting Wordpress to markdown. I am definitively no expert on this subject, but following the steps documented in this blog post, I got the job done.\nAfter a bit of research I ended up working with this repo from Swizec. Thanks for putting this repo out there!"
  },
  {
    "objectID": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-1-export-the-xml-from-wordpress",
    "href": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-1-export-the-xml-from-wordpress",
    "title": "How to Convert a Wordpress Blog into Markdown",
    "section": "Step 1: Export the XML from Wordpress",
    "text": "Step 1: Export the XML from Wordpress\nThe first step the conversion process is to export your Wordpress blog content as an XML file. Here’s how to do it:\nNavigate to the export function of Wordpress blog by entering your site’s URL followed by /wp-admin/export.php, for example, https://wittmann-tours.de/wp-admin/export.php. Alternatively, you can navigate like this:\n\nLog into your Wordpress Dashboard. Navigate to the admin area of your Wordpress blog by entering your site’s URL followed by /wp-admin. Use your credentials to log in.\nAccess the Tools section. Once logged in, look for the Tools option in the left-hand sidebar. Hover over it, and you will see a dropdown menu.\nSelect Export: In the dropdown menu under Tools, click on Export. This will take you to a page where you can choose what content you want to export. For a complete backup of your site, select All content.\n\nFinally, you can download the export file: After selecting All content, click on the Download Export File button. Wordpress will generate an XML file containing all your selected data. Save this file to your computer."
  },
  {
    "objectID": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-2-check-software-requirements",
    "href": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-2-check-software-requirements",
    "title": "How to Convert a Wordpress Blog into Markdown",
    "section": "Step 2: Check Software Requirements",
    "text": "Step 2: Check Software Requirements\nDepending on your setup, you might need to install some software first. Here is what we need:\n\nNode.js: Node.js is a runtime environment that allows you to run JavaScript code outside of a web browser. It’s commonly used for server-side scripting and building backend services (like APIs), but it’s also used in tooling for front-end development, automation tasks, and more. In this case, Node.js is used to run the wordpress-to-markdown conversion script.\nnpm (Node Package Manager): npm is the default package manager for Node.js. It is used to install and manage dependencies (libraries, frameworks, tools, etc.) required by Node.js applications. npm facilitates easy sharing and reuse of code. When you install Node.js, npm should be included in the installation. In this case, we need npm to install Yarn.\nYarn: Yarn is an alternative package manager to npm. It performs the same basic function as npm (managing dependencies for Node.js applications) but often with some differences in performance, features, and the way dependencies are handled. In this case, Yarn was used to manage the dependencies of the wordpress-to-markdown script."
  },
  {
    "objectID": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-3-install-node.js-and-yarn",
    "href": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-3-install-node.js-and-yarn",
    "title": "How to Convert a Wordpress Blog into Markdown",
    "section": "Step 3: Install Node.js and Yarn",
    "text": "Step 3: Install Node.js and Yarn\nIf your system already fulfills these software requirements, feel free to skip this section.\n\nInstalling Node.js\n\nDownload Node.js. Visit the official Node.js website to download the latest version of Node.js. Choose the version that is compatible with your operating system.\nInstall Node.js. Follow the installation prompts to install Node.js on your system. The installer will guide you through the process.\nVerify the installation. To ensure that Node.js was installed correctly, open a terminal or command prompt and type the following commands: bash     node -v     npm -v These commands will display the versions of Node.js and npm installed on your system. Seeing the version numbers confirms that the installation was successful.\n\n\n\nInstalling Yarn\n\nOpen your terminal or command prompt.\nInstall Yarn globally using npm. Type the following command: bash     npm install -g yarn If you encounter permission errors, it might be necessary to run the command as an administrator or with superuser rights. In such cases, use: bash     sudo npm install -g yarn This will prompt you for your password to grant the necessary permissions.\nVerify the installation. To check if Yarn has been installed correctly, run: bash     yarn -v This command will display the version of Yarn installed, indicating that the installation was successful.\n\n\n\nFinal Checks\n\nCheck the PATH. It’s important to ensure that the installation paths for Node.js and Yarn are correctly added to your system’s PATH environment variable. This allows you to run these tools from any directory in your terminal. To check your PATH, type: bash     echo $PATH Verify that the paths to Node.js and Yarn are included in the output.\n\nAfter completing these steps, your system will be equipped with Node.js and Yarn, ready for the next phase of converting your Wordpress blog into Markdown."
  },
  {
    "objectID": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-4-clone-the-repository-and-run-the-conversion-script",
    "href": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-4-clone-the-repository-and-run-the-conversion-script",
    "title": "How to Convert a Wordpress Blog into Markdown",
    "section": "Step 4: Clone the Repository and Run the Conversion Script",
    "text": "Step 4: Clone the Repository and Run the Conversion Script\nIn this step we clone the GitHub repository and run the conversion script:\n\nOpen your terminal or command prompt: Ensure you’re in the directory where you want to clone the repository.\nClone the repository: Execute the following command to clone the wordpress-to-markdown repository created by Swizec: bash     git clone https://github.com/Swizec/wordpress-to-markdown This command downloads the repository to your local machine in a folder named wordpress-to-markdown.\nNavigate to the repository directory: Change into the newly cloned directory to run the conversion commands: bash     cd wordpress-to-markdown\nInstall dependencies: Before running the conversion script, you must install its dependencies. Use Yarn to install them by executing: bash     yarn install This command reads the package.json file in the repository and installs all the necessary packages and dependencies required to run the conversion script.\nCopy XML for wordpress-to-markdown directory: Copy the XML-file you downloaded in step 1 into wordpress-to-markdown directory.\nAdjust script or rename XML-file: Either rename your XML-file to test-wordpress-dump.xml or change line 25 of convert.js to the file name of your XML.\nRun the conversion script: After installing the dependencies, you can now run the conversion script with Yarn: bash     yarn convert This command initiates the conversion process, which reads your exported Wordpress XML file and converts its contents into Markdown files.\n\nOnce this step is completed, you have successfully converted your Wordpress blog content into Markdown mdx-files. The files are store in a new out-directory, containing one sub-directory per blog post."
  },
  {
    "objectID": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-5-convert-mdx-files-to-md-files",
    "href": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-5-convert-mdx-files-to-md-files",
    "title": "How to Convert a Wordpress Blog into Markdown",
    "section": "Step 5: Convert mdx-files to md-files",
    "text": "Step 5: Convert mdx-files to md-files\nSo far so good, but I was not yet 100% happy, because the mdx-files did not contain a proper level-1-heading, and Obsidian ignored the files.\nTo convert the mdx-files to md-files, I created a quick conversion notebook which made these final adjustments."
  },
  {
    "objectID": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#conclusion",
    "href": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#conclusion",
    "title": "How to Convert a Wordpress Blog into Markdown",
    "section": "Conclusion",
    "text": "Conclusion\nConverting a Wordpress blog into Markdown turned out to be more complex than anticipated. Somehow I had anticipated there would be a simple, straightforward Wordpress plugin to get this done quickly, but no…\nIn the process to doing the conversion, I decided to document each step of the conversion in detail within this blog post. Not only did I want a reference for myself, knowing that revisiting the process even after a few weeks could be challenging without detailed notes, but I hope this guide is also useful for you reading this blog post.\nFinally, preparing the data source for my RAG project is done, which turned out to be a project of its own."
  },
  {
    "objectID": "posts/2024-01-27-how-to-call-openai-api/index.html",
    "href": "posts/2024-01-27-how-to-call-openai-api/index.html",
    "title": "How to call the OpenAI API from a Jupyter Notebook",
    "section": "",
    "text": "Exploring Large Language Models (LLMs) through their Web-based User Interfaces (WebUIs) is indeed insightful, particularly for experimenting with various prompt engineering techniques. However, accessing LLMs via their API unlocks a many additional possibilities. This approach not only allows you to craft your own applications but also enables the integration of LLMs into existing solutions. The use cases are endless: You can leverage LLMs for constructing comprehensive datasets, automating content creation, enhancing user interaction with natural language, personalizing user experiences, etc. The API access essentially opens doors to a more tailored LLM experience, more than just chatting with it.\nTo demonstrate how to access the OpenAI API for text generation, I created a Jupyter Notebook with all the steps from installing the necessary python packages, via managing access keys to calling the API with some examples. While you can perform all the steps in the Jupyter Notebook, in this blog post I would like to explore the concepts and take a look at what is between the lines of code, including my biggest learning: How does the chat with an LLM actually work.\nThis is the first blog post of a series in which I am reworking the hackers guide by Jeremy Howard and the accompanying notebook."
  },
  {
    "objectID": "posts/2024-01-27-how-to-call-openai-api/index.html#setup",
    "href": "posts/2024-01-27-how-to-call-openai-api/index.html#setup",
    "title": "How to call the OpenAI API from a Jupyter Notebook",
    "section": "Setup",
    "text": "Setup\nBefore we can start calling the OpenAI API, we need to setup a few thing:\n\nInstalling python packages\nGetting an API Key\nSecurely storing the API-key\n\n\nInstallation\nIf you have not done so already, pip install the openai package:\npip install openai\n\n\nGenerate API key\nTo be able to access the OpenAI API, you need an API access key. To obtain/generate the API-key from the Open.AI Website as also explained in the docs\n\n\nHow to securely store your API Access Key\nSince you do not want to put your API key into a Jupyter notebook, it is recommended that you store the API-key in a your python environment using python-dotenv.\npip install python-dotenv\nUsing dotenv, you store your API key in an environment file which you can easily access from within your Jupyter notebook. Here is a quick example, using an example file foobar.env which has the following content:\n# Exapmple\nFOO=\"BAR\"\nYou can import the variables like this:\n\nfrom dotenv import dotenv_values\n\nfoobar_config = dotenv_values(\"foobar.env\")\nprint(foobar_config)\n\nOrderedDict([('FOO', 'BAR')])\n\n\nIn real life, the usage looks like this, leveraging the environment variables from the os package:\n\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv(\"foobar.env\")  # This loads the .env file into the environment\n\nfoo_env_value = os.getenv('FOO')\nprint(foo_env_value)  # This will also print \"BAR\"\n\nBAR\n\n\nThe final step to real life is not to use foobar.env, but .env. Therefore, you need to add the following section to your .env-file:\n# Open AI\nOPENAI_API_KEY=\"My API Key\"\nOnce you load the .env-file, you are in business to call the OpenAI API\n\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv(\".env\")\n\nTrue\n\n\nImportant Note: Make sure, the .env file is not published to GitHub by including *.env in the .gitignore-file:\necho \".env\" &gt;&gt; .gitignore \nafterwards:\ngit add .gitignore \ngit commit -m \"Updated .gitignore to ignore .env files\"\ngit push"
  },
  {
    "objectID": "posts/2024-01-27-how-to-call-openai-api/index.html#calling-the-api",
    "href": "posts/2024-01-27-how-to-call-openai-api/index.html#calling-the-api",
    "title": "How to call the OpenAI API from a Jupyter Notebook",
    "section": "Calling the API",
    "text": "Calling the API\nSince the time of publication of Jeremy’s hackers guide the Open.AI API had changed. Therefore, the original code needed from some minor refactoring, essentially 2 thing:\n\nReplace ChatCompletion.create with chat.completions.create\nReplace c['choices'][0]['message']['content'] with c.choices[0].message.content\n\n\n#from openai import ChatCompletion,Completion\nfrom openai import chat\n\naussie_sys = \"You are an Aussie LLM that uses Aussie slang and analogies whenever possible.\"\n\n#c = ChatCompletion.create(\nc = chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n              {\"role\": \"user\", \"content\": \"What is money?\"}])\n\n#c['choices'][0]['message']['content']\nc.choices[0].message.content\n\n'Money, mate, is like the fuel that powers your financial engine. It\\'s the cold, hard cash and digital digits you use to buy stuff, pay your bills, and live your life. It\\'s a medium of exchange that keeps the economic gears churning. Think of it as the \"dollarydoos\" that keep the economic barbie cookin\\'!'\n\n\n\nNote for Enhanced Readability\nTo improve the readability of of the model responses in the notebook, especially if it contains long lines of text or code, you may want to enable word wrap in your development environment.\nFor Visual Studio Code Users:\n\nOpen the Command Palette (Ctrl+Shift+P or Cmd+Shift+P).\nSearch for Preferences: Open Settings (JSON) and select it.\nAdd \"notebook.wordWrap\": \"on\" to your settings.\nSave the settings.json file.\n\nEnabling word wrap will make long lines of code or text wrap to the next line, fitting within the cell’s width and eliminating the need for horizontal scrolling."
  },
  {
    "objectID": "posts/2024-01-27-how-to-call-openai-api/index.html#learnings",
    "href": "posts/2024-01-27-how-to-call-openai-api/index.html#learnings",
    "title": "How to call the OpenAI API from a Jupyter Notebook",
    "section": "Learnings",
    "text": "Learnings\nMy biggest take-away from having done this implementation is the realization how the chat with an LLM actually works. It is surprisingly simple, yet I had not realized this before: The chat with an LLM is stateless, which means that ChatGPT does not have a session open with you. Instead, the whole chat is passed to the model as context with every new prompt. This is the way the model knows what you have been talking about, and it can answer follow-up questions.\n\nc = chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n              {\"role\": \"user\", \"content\": \"What is money?\"},\n              {\"role\": \"assistant\", \"content\": \"Well, mate, money is like kangaroos actually.\"},\n              {\"role\": \"user\", \"content\": \"Really? In what way?\"}])\n\nc.choices[0].message.content\n\n\"Ah, glad you asked! Money, just like kangaroos, is all about value and trading, you see. Just as kangaroos hop around, money hops from one person to another in exchange for goods and services. It's the key to getting what you need and want in this modern world. Just like kangaroos in the outback, money roams around the economy, jumpin' here and there, makin' things happen. It's the backbone of our economic system, mate!\"\n\n\nOnce I had understood this, I started interacting with ChatGPT differently:\n\nInstead of having long chats which drifted from topic to topic, I try to keep the chats more focused. If the topic changes too much, I open up a new chat.\nI go back to prompts which did not yield the desired result more frequently, i.e. I edit the prompt instead of asking ChatGPT to correct something. This way you can keep undesired results out of the conversation which otherwise would be stuck in the conversation as context.\n\nOverall, I think the technical implementation was quite easy, and the docs nicely guided me to dive one level deeper than in Jeremy’s original notebook. Learning more about the inner mechanics of how chatting with an LLM actually works, was the best part of this project."
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html",
    "href": "posts/2022-11-05-kaggle-titanic/index.html",
    "title": "My First Kaggle Competition: Titanic",
    "section": "",
    "text": "For more practical experience with gradient descent, I decided to participate in the Titanic Competition. Here is how I did it and what I learned.\nI took the following approach:"
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html#installing-kaggle",
    "href": "posts/2022-11-05-kaggle-titanic/index.html#installing-kaggle",
    "title": "My First Kaggle Competition: Titanic",
    "section": "Installing Kaggle",
    "text": "Installing Kaggle\nGetting ready for the Kaggle competition requires registering for the competition (a few clicks on the kaggle website), and installing kaggle on your local machine. The following is based on the Live-Coding Session 7 and the related official topic in the forums.\nThe first step is to install kaggle:\npip install --user kaggle\nAs a result, the following warning is displayed: The script kaggle is installed in '/home/&lt;your user&gt;/.local/bin' which is not on PATH. This means that the you need to add the path to the PATH-variable. This is done by adding the following line to the .bashrc-file and restarting the terminal:\nPATH=~/.local/bin:$PATH\n\nNote: To display the current PATH-variable use: echo $PATH\n\nAs a result, typing the kaggle-command on the command line works, but the next error shows up (as expected): OSError: Could not find kaggle.json. Make sure it's located in /home/chrwittm/.kaggle. Or use the environment method.\nThis means that you cannot authorize against the kaggle platform. To solve this, download your personal kaggle.json On the kaggle website, navigate to: “Account” and click on “Create New API Token”. As a result, the kaggle.json is downloaded.\nCopy the kaggle.json-file into the .kaggle-directory in your home directory.\nTyping the kaggle-command on the command line gives you the final clue as to what is missing: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/chrwittm/.kaggle/kaggle.json'\nTherefore, type:\nchmod 600 /home/&lt;your user&gt;/.kaggle/kaggle.json\nTyping the kaggle-command on the command line again confirms: We are in business :)"
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html#downloading-the-dataset",
    "href": "posts/2022-11-05-kaggle-titanic/index.html#downloading-the-dataset",
    "title": "My First Kaggle Competition: Titanic",
    "section": "Downloading the dataset",
    "text": "Downloading the dataset\nTo download the dataset, run the following command (which you can also find on the kaggle website):\nkaggle competitions download -c titanic\nAs a result, the file titanic.zip is downloaded.\nTo unzip type:\nunzip titanic.zip\nDoing this for the first time, this resulted in an error: /bin/bash: unzip: command not found\nTo install zip and unzip, type:\nsudo apt-get install zip\nsudo apt-get install unzip\nAs a result, unzipping works, and we have a dataset to work with :).\n\nimport pandas as pd\n\ntrain = pd.read_csv(\"train.csv\")\ntrain.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS"
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html#implementing-a-fast.ai-tabular-learner",
    "href": "posts/2022-11-05-kaggle-titanic/index.html#implementing-a-fast.ai-tabular-learner",
    "title": "My First Kaggle Competition: Titanic",
    "section": "Implementing a Fast.ai Tabular Learner",
    "text": "Implementing a Fast.ai Tabular Learner\nThe goal was not to create a perfect submission, but to simply train a model as fast as possible to\n\nget a baseline\nto get to know how a kaggle competition works (remember, this is my first one)\n\nTherefore, I created a dataloaders as shown in lesson 1 or in the docs by sorting the variables into categorical or continuos one, excluding irrelevant ones).\n\nNote 1: In this blog post, I am presenting the steps in a fast-forward way, here is the original notebook.\n\n\nNote 2: When writing this up, I was not able to 100% re-produce the same results, but basically this is how the story went.\n\n\nfrom fastai.tabular.all import *\n\npath = \".\"\n\ndls = TabularDataLoaders.from_csv('train.csv', path=path, y_names=\"Survived\",\n    cat_names = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked'],\n    cont_names = ['Age', 'Fare'],\n    procs = [Categorify, FillMissing, Normalize])\n\nNow we can train a model:\n\nlearn = tabular_learner(dls, metrics=accuracy)\nlearn.fit_one_cycle(10) #change this variable for more/less training\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.548652\n0.315984\n0.640449\n00:00\n\n\n1\n0.454461\n0.325496\n0.640449\n00:00\n\n\n2\n0.373511\n0.289948\n0.640449\n00:00\n\n\n3\n0.319270\n0.251090\n0.640449\n00:00\n\n\n4\n0.280473\n0.196879\n0.640449\n00:00\n\n\n5\n0.249269\n0.173640\n0.640449\n00:00\n\n\n6\n0.225535\n0.152192\n0.640449\n00:00\n\n\n7\n0.207350\n0.141283\n0.640449\n00:00\n\n\n8\n0.192223\n0.137462\n0.640449\n00:00\n\n\n9\n0.180697\n0.137344\n0.640449\n00:00\n\n\n\n\n\nWith this learner, we can make the predictions on the test-dataset.\n\ntest = pd.read_csv(\"test.csv\")\n\n# replacing null values with 0\ntest['Fare'] = test['Fare'].fillna(0)\n\n# create Predictions as suggested here:\n# https://forums.fast.ai/t/tabular-learner-prediction-using-data-frame/90534/2\ntest_dl = learn.dls.test_dl(test)\npreds, _ = learn.get_preds(dl=test_dl)\n\ntest['Survived_pred'] = preds.squeeze()\ntest.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nSurvived_pred\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n0.064765\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n0.454887\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n-0.025921\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n-0.015690\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n0.508172\n\n\n\n\n\n\n\nInterpreting the values in column Survived_pred is important, because we need to turn these values into 0 and 1 for the submission. The submission file should only have the columns PassengerId and Survived. For the first submission, I did not worry about it too much and simply picked a value 0.5. (Let’s come back to that a little later)\n\nthreshold = 0.5 #change this variable for more/less training\ntest['Survived'] = [ 1 if element &gt; threshold else 0 for element in preds.squeeze()]\n\nsubmission1 = test[['PassengerId', 'Survived']]\nsubmission1.to_csv('submission1.csv', index=False)\n\nI uploaded the results, and they were better then random ;) - Score 0.73923\n\nThe score is not great, but the whole point was to get a baseline as quickly as possible, and to “play the whole kaggle game”. Actually, the fact that I produced this result in about 1-2 hours felt pretty good :).\n\nNote: Running this notebook, I got a score of 0.75119, I am not sure, what caused the difference… but better is always good ;)\n\nSo how can we improve the score? More training, interpreting the results differently? As it turns out: Both.\nLet’s look at the distribution of Survived_pred:\n\ntest.Survived_pred.hist();\n\n\n\n\n\n\n\n\nAs it turned out, setting my threshold to 0.6 created a better result: Score: 0.74162. (this I could not reproduce with this notebook while writing up the blog post)\nAlso more training, produced better results, running for 50 cycles, resulted in a lower loss and a better result. Training with 50 cycles and threshold 0.7, this was the result: Score: 0.76794 (with this notebook 0.77033)\nSo there is some randomness when training, and it is important to properly interpret the results. Getting about 77% right with this simple approach is not to bad."
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html#re-implementing-the-excel-model",
    "href": "posts/2022-11-05-kaggle-titanic/index.html#re-implementing-the-excel-model",
    "title": "My First Kaggle Competition: Titanic",
    "section": "Re-Implementing the Excel Model",
    "text": "Re-Implementing the Excel Model\nAfter the quick win with Fast.AI, I decided to re-implement what Jeremy did in the Excel in video lecture 3 to predict the survivors. Let’s see how it performs against the Fast.AI tabular learner.\nSince that involved quite a bit of code, let me simply link to notebook and discuss the learnings / results.\nAs it turned out:\n\nI had to do a bit of data cleansing.\nThe feature engineering took some time which taught me some general python lessons.\nImplementing the optimizer was a nice exercise, revisiting gradient descent and matrix multiplication, and doing some hands-on work with tensors.\n\nThe first model with just one layer scored 0.75837, even better than the my Fast.AI baseline, but not quite as good as the optimized version.\nThe next iteration with 2 and 3 layers scored better:\n\nScore: 0.77033 (2-layers)\nScore: 0.77272 (3-layers)\n\n\nThis was quite surprising: The self-written algorithm is better than the Fast.AI one, any ideas why that would be?\nNonetheless, it seems to hit a ceiling at 77%, and it would make sense to dive deeper into tabular data, but that is for another time. My goal was not to optimize the competition result, but to participate in my first kaggle competition, and to re-visit the topic of gradient descent and matrix multiplication. I will most likely return to this dataset/challenge in the future."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html",
    "title": "Wrapping-up Lesson 3",
    "section": "",
    "text": "Lesson 3 took me a while to rework, because it churned out quite a few interesting projects:\nAlongside these 3 main activities, I also started a blog on my machine learning journey, which is based on Quarto.\nLet me summarize what I have done and learned."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html#gradient-descent",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html#gradient-descent",
    "title": "Wrapping-up Lesson 3",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nThe main focus of lesson 3 for me was/is gradient descent, which I found pretty easy to understand on a high level:\n\nCalculate the predictions and the loss (forward-pass)\nCalculate the derivatives of the parameters (i.e. how does changing the parameters change the loss) (backward-pass)\nUpdate the parameters (via the learning rate)\nDon’t forget to initialize the gradients\nRestart\n\nHowever, in the actual implementation and its simplicity, there is a lot of magic, which I tried to unpack for myself. Working through Jeremy’s notebook “How does a neural net really work?”, I tried to not only think through the concept, but also to visualize it. The result is available as\n\na blog post\na forum post\na Kaggle notebook in which you can easily also play with the visualizations interactively (by copying and running the notebook)\na GitHub notebook\n\nI was excited and honored to read that Ben, a fellow Fast.AI student, created even better visualizations building on my work. I highly recommend playing with it and also checking out his other projects.\nWhile I truly love the Fast.AI content, I also need to mention the great video “The spelled-out intro to neural networks and backpropagation: building micrograd” from Andrej Karpathy, which dives at least one level deeper. If there is one key takeaway from this video, it is this one: “A single gradient tells us the effect changing a parameter has on the loss”. This insight is powerful, and somehow it tends to get lost from my point of view, because you either have a very complex model with many, many parameters so that this is difficult to grasp, or you have a seemingly simple model in which you mix up the slope of the quadratic with a gradient. Implementing the visualization of gradient descent was also about building the intuition of what is actually going on under to hood."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html#the-titanic-competition",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html#the-titanic-competition",
    "title": "Wrapping-up Lesson 3",
    "section": "The Titanic Competition",
    "text": "The Titanic Competition\nInspired by the Excel-based version of the Titanic Competition, I decided to enter the kaggle competition. As with many good project, this resulted in a few other mini-projects:\n\nThe logistics on how a kaggle competition actually works, which included installing kaggle on my local machine. The Live-Coding Sessions of the 2022 Fast.AI course are probably quite underrated (at least looking at the number of views they get on Youtube). I find them a great addition to the official lessons because the tackle side problems like installing kaggle (in Live-Coding Session 7 and the related official topic in the forums) which otherwise would set you back some hours (or more). A big shout-out for these sessions!\nRevisiting matrix multiplication. Apart from the math, this also was about some python basics for me. While the result of implementing matrix multiplication from scratch has probably been done a million times, it still taught me some valuable lessons.\n\nIn the actual Titanic Competition, I did not focus too much on submitting a perfect result, but I rather aimed at re-visiting/solidifying the topic of gradient descent by replicating the actual lesson content. I built for following 2 notebooks\n\nThe first one uses a Fast.AI tabular learner to create a baseline while getting the know the data.\nNext, I re-implemented the Excel-based version from the video in python in this notebook.\n\nWhile my final high score of 77.2% is far away from perfect, I decided to come back to this competition another time, focusing more on the content, not just on gradient descent (like this time)."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html#mnist-the-hello-world-of-computer-vision",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html#mnist-the-hello-world-of-computer-vision",
    "title": "Wrapping-up Lesson 3",
    "section": "MNIST, the ‘Hello World’ of Computer Vision",
    "text": "MNIST, the ‘Hello World’ of Computer Vision\nAs Jeremy points out at the end of lesson 3, this lesson corresponds to chapter 4 of the book. Indeed, it covers very similar topics, but the example used is the light version of the MNIST dataset (which only contains 3s and 7s). Following the recommendation for further research, I implemented a model for the complete MNIST dataset. As predicted: “This was a significant project and took you quite a bit of time to complete! I needed to do some of my own research to figure out how to overcome some obstacles on the way”.\nAfter all the previous activities around gradient descent, the actual mechanics of what needed to be done were not too difficult. Nonetheless, I found the competition to be hard, because of the actual technicalities of the python implementation. Put differently, I think I could have easily written a good specification on how to solve the MNIST competition, but actually doing it yourself is a different thing.\nSeemingly simple tasks like converting the csv-files to images, converting a PIL image to a Fast.AI PIL image, or getting the tensors in the right shape took me some time to implement in python. I am still struggling with python as a language but I am seeing good progress, and the only way to improve is to keep coding."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html#wrapping-up-lesson-3",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html#wrapping-up-lesson-3",
    "title": "Wrapping-up Lesson 3",
    "section": "Wrapping-up Lesson 3",
    "text": "Wrapping-up Lesson 3\nWhile I could improve the results of my projects, both for Titanic and MNIST, it feels like it would be some way over-optimizing. I did not enter the competitions to win, but to learn about gradient descent. Having spent the last 8 weeks with my lesson 3-projects (and allowing myself to get somewhat side-tracked), I feel it is time to move on to the next lesson. I am looking forward to the next challenging projects!"
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html",
    "href": "posts/2022-10-28-matrix-multiplication/index.html",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "Since matrix multiplication is a big thing for deep learning and visualizations like http://matrixmultiplication.xyz/ were a bit to fast for me to properly re-understand what I learned in highschool, I decided dive in more systematically without falling into the trap of learning lots of math before continuing with deep learning. This will be short and sweet:\nThis has been done a million times before already, but nonetheless, let me explain what I learned along the way."
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#takeaways-from-khan-academy",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#takeaways-from-khan-academy",
    "title": "Matrix Multiplication",
    "section": "Takeaways from Khan Academy",
    "text": "Takeaways from Khan Academy\nOne thing I was struggling with was to intuit the dimensions of the target matrix.\nLet’s assume two matrixes: \\(A = (m \\times n)\\) and \\(B = (n \\times k)\\)\nThis picture from Khan Academy sums it all up for me:\n Illustration by Khan Academy CC BY-NC-SA 3.0 US  Note: All Khan Academy content is available for free at (www.khanacademy.org)“\nTherefore, a matrix multiplication is defined if the number of columns of matrix \\(A\\) matches the number of rows of matrix \\(B\\).\nThe resulting matrix \\(C = A \\times B\\) has the same number of rows as matrix \\(A\\) and the same number of columns as matrix \\(B\\)."
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#implementing-matrix-multiplication-in-python-from-scratch",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#implementing-matrix-multiplication-in-python-from-scratch",
    "title": "Matrix Multiplication",
    "section": "Implementing Matrix Multiplication in Python from scratch",
    "text": "Implementing Matrix Multiplication in Python from scratch\nOnce that was done, I decided to implement matrix multiplication in python. I found this tutorial which provided me with the task and some guidance along the way, especially on a few things in python.\nAs a starting point, here are 2 matrixes that we want to multiply (example from tutorial sightly adjusted):\n\nimport numpy as np\nnp.random.seed(27)\nA = np.random.randint(1,10,size = (4,3))\nB = np.random.randint(1,10,size = (3,2))\nprint(f\"Matrix A:\\n {A}\\n\")\nprint(f\"Matrix B:\\n {B}\\n\")\n\nMatrix A:\n [[4 9 9]\n [9 1 6]\n [9 2 3]\n [2 2 5]]\n\nMatrix B:\n [[7 4]\n [4 1]\n [6 4]]\n\n\n\nThis is the final result, we want to re-implement from scratch:\n\nA@B\n\narray([[118,  61],\n       [103,  61],\n       [ 89,  50],\n       [ 52,  30]])\n\n\n\nIndexing in Python\nMaybe this is too obvious for many, but I find it worth noting, that the sequence in which python addresses arrays (or tensors) is first by row, than by column. What do I mean by saying that?\nWhen you want to index into an array, you do this by array_name[row:column], for example A[1,2] return 6, it is the second line (which is index 1 when starting to count at 0), and the third column (which is index 1 when starting to count at 0):\n\nA[1,2]\n\n6\n\n\nIs there a way to not only remember this, but to also understand this? Yes, I think so: The most basic array (tensor) is a list (rank 1 tensor), which we can think of as one row of numbers. Therefore, the first index represents the row. You can think of a 2-dimensional array (a rank 2 tensor) as adding the columns to a row of numbers (by adding more rows), therefore the second index represents the columns. Hence to access an element in a 2D-array (rank-2 tensor), this is done by array_name[row:column].\nWhy do we think about indexing? First, to determine if a matrix multiplication is defined, we need to find the dimensions of the matrixes, and later on we need to access the matrix content for the calculation.\nTo access a complete row or column, we use:\n\nFor a row: array_name[row, : ] or the short form array_name[row]\nFor a column: array_name[ : ,column]\n\nThis means: We access a specific row or column by index, and from the other dimension, we access all elements. For example:\n\n# accessing the first row of matrix A\n\nA[0] #same as A[0,:]\n\narray([4, 9, 9])\n\n\n\n# accessing the first column of matrix B\n\nB[:,0]\n\narray([7, 4, 6])"
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#constructing-a-target-matrix-of-zeros",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#constructing-a-target-matrix-of-zeros",
    "title": "Matrix Multiplication",
    "section": "Constructing a target matrix of zeros",
    "text": "Constructing a target matrix of zeros\nThe \\(C\\) target matrix has the same number of rows as A and the same number of columns of B, so in our example that is a matrix with 4 rows and 2 columns:\n\nnp.zeros((4, 2), dtype = int)\n\narray([[0, 0],\n       [0, 0],\n       [0, 0],\n       [0, 0]])\n\n\nThe number of rows is the length of a column, therefore, to get the number of rows of matrix A, we can write:\n\nlen(A[:,0]) #i.e. the length of the first column\n\n4\n\n\nSimilarly, the number of elements in a row if the number of columns, Therefore, the number of columns of B is:\n\nlen(B[0]) #the number of entries in the first row\n\n2\n\n\nWhile to above is correct, there is a more elegant way to write this. Each array (tensor) has an attribute .shape which tells us how many rows and columns an array has (notice the sequence in the tuple: (row,column)):\n\nprint(A.shape)\nprint(B.shape)\n\n(4, 3)\n(3, 2)\n\n\nTherefore, we can re-write:\n\nprint(f'Number of rows in matrix A: {A.shape[0]}') \nprint(f'Number of columns in matrix B: {B.shape[1]}')\n\nNumber of rows in matrix A: 4\nNumber of columns in matrix B: 2\n\n\nNow we can generically construct the target matrix \\(C\\):\n\nC = np.zeros((A.shape[0], B.shape[1]), dtype = int)\nC.shape\n\n(4, 2)"
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#exercise-implement-matrix-multiplication-with-numpy-arrays",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#exercise-implement-matrix-multiplication-with-numpy-arrays",
    "title": "Matrix Multiplication",
    "section": "Exercise: Implement Matrix Multiplication with numpy arrays",
    "text": "Exercise: Implement Matrix Multiplication with numpy arrays\nImplement a function multiply_matrix(A,B) which does the following:\n\nAccept two matrices, A and B, as inputs.\nCheck if matrix multiplication between A and B is valid, if not raise an error.\nIf valid, multiply the two matrices A and B, and return the product matrix C.\n\n\ndef multiply_matrix(A,B):\n    \n    if A.shape[1] != B.shape[0]:\n        raise ValueError('Number of columns of A and number of rows of B do not match')\n    \n    C = np.zeros((A.shape[0], B.shape[1]), dtype=int)\n\n    for row in range(C.shape[0]):\n        for column in range(C.shape[1]):\n            for step in range(A.shape[1]):\n                C[row, column] += A[row, step] * B[step, column]\n    \n    return C\n\nC1 = multiply_matrix(A, B)\nC1\n\narray([[118,  61],\n       [103,  61],\n       [ 89,  50],\n       [ 52,  30]])\n\n\n\nC2 = A@B\nassert np.array_equal(C1, C2)"
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#exercise-implement-matrix-multiplication-with-tensors",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#exercise-implement-matrix-multiplication-with-tensors",
    "title": "Matrix Multiplication",
    "section": "Exercise: Implement Matrix Multiplication with tensors",
    "text": "Exercise: Implement Matrix Multiplication with tensors\nJust for the fun of it, let’s re-implement the same with pytorch tensors. It turns out it same, same, but a little different:\n\nimport torch\n\ntorch.manual_seed(27) #https://pytorch.org/docs/stable/notes/randomness.html\nX = torch.randint(1,10,size = (4,3)) #https://pytorch.org/docs/stable/generated/torch.randint.html\nY = torch.randint(1,10,size = (3,2))\nprint(f\"Matrix X:\\n {X}\\n\")\nprint(f\"Matrix Y:\\n {Y}\\n\")\n\nMatrix X:\n tensor([[1, 1, 5],\n        [8, 8, 6],\n        [1, 7, 1],\n        [4, 4, 1]])\n\nMatrix Y:\n tensor([[7, 6],\n        [3, 7],\n        [9, 5]])\n\n\n\n\nZ = torch.zeros((4, 2), dtype = int)\nZ\n\ntensor([[0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0]])\n\n\n\nZ.shape\n\ntorch.Size([4, 2])\n\n\n\ndef multiply_matrix_torch(A,B):\n    \n    if A.shape[1] != B.shape[0]:\n        raise ValueError('Number of columns of A and number of rows of B do not match')\n    \n    C = torch.zeros((A.shape[0], B.shape[1]), dtype=int)\n\n    for row in range(C.shape[0]):\n        for column in range(C.shape[1]):\n            for step in range(A.shape[1]):\n                C[row, column] += A[row, step] * B[step, column]\n    \n    return C\n\nZ1 = multiply_matrix_torch(X, Y)\nZ1\n\ntensor([[ 55,  38],\n        [134, 134],\n        [ 37,  60],\n        [ 49,  57]])\n\n\n\nZ2 = X@Y\nassert torch.equal(Z1, Z2) == True\n\nThat concludes the “exploration” of matrix multiplication, I learned a lot along the way :)."
  },
  {
    "objectID": "posts/2023-03-05-titanic-with-chatgpt/index.html",
    "href": "posts/2023-03-05-titanic-with-chatgpt/index.html",
    "title": "Titanic with ChatGPT",
    "section": "",
    "text": "Reworking Lesson 5, I returned to the Titanic Competition to learn more Fast.AI-concepts. Additionally, I explored how ChatGPT could increase my productivity.\nAs a result I created the following 2 notebooks\nIn this blog post, I do not was to talk about the Titanic Competition too much, but rather about the co-working experience with ChatGPT and what I learned about how to interact with it. (Prompts will be indicated by the chat emoji 💬).\nThe main findings are that you need to be precise in your prompting, but there is a limit to what you can prompt for. When it comes to interpreting the result, you need to guide the conversation with ChatGPT. While it can save a lot of time, it cannot do all the work for you."
  },
  {
    "objectID": "posts/2023-03-05-titanic-with-chatgpt/index.html#the-hottest-new-programming-language-is-english",
    "href": "posts/2023-03-05-titanic-with-chatgpt/index.html#the-hottest-new-programming-language-is-english",
    "title": "Titanic with ChatGPT",
    "section": "The hottest new programming language is English",
    "text": "The hottest new programming language is English\nAs Andrej Karpathy tweeted: “The hottest new programming language is English”. Indeed, especially in the EDA-notebook (GitHub/Kaggle), I actually used English as a programming language, and ChatGPT was merely translating to python. Additionally, working with ChatGPT really felt like a conversation: I asked for some visualizations, ChatGPT created the code, I ran the code, and based on the result, I created the next prompt, either asking ChatGPT to fix an error, or to build a new visualization.\nSomehow this felt like “real life” (whatever that is 😉), in which I am a software consultant: A typical workflow in that business is writing a specification document, giving it to a developer who writes the code, and getting the result after some time. With ChatGPT, however, the iteration cycles are 1 to 2 orders of magnitude faster (quasi instant), and iteration speed is essential. Even a super-skilled human developer would be considerably slower, because ChatGPT is just “typing so much faster”. Without being overly enthusiastic, not all tasks are equally well-suited for this workflow with ChatGPT (as of today). The more complex the task are, the more “random” the result will get. With EDA (GitHub/Kaggle), this interaction felt productive. For feature engineering (GitHub/Kaggle), I had to break down the task for ChatGPT into smaller chunks, and for model creation and fine tuning it, I took over in the end.\nThe key to getting good results are well-stated prompts. It is really important to be explicit in your prompts: State what you really want, ChatGPT is not really great at reading between the lines, or extrapolating what might be inferred. As you go through the notebooks, you will see me make that mistake. But than again, by talking to ChatGPT, you can refine the prompts along the way. Or you can simply build upon the results. If ChatGPT produced a function/visualization you do not like, you can simply ask for a new version.\nBut why are some coding tasks more difficult than others? The big limitation of ChatGPT is that it can neither run/debug the code or see the result the code produces, also it does not know the real data. All it can do is read the code and respond (with variations). Therefore, it is understandable that it makes mistakes or produces code which is syntactically incorrect. The best feature from ChatGPT on the other side is that you can talk though the bugs and resolve them quickly (“💬: There is an error: AssertionError: nan values in Fare but not in setup training set - can you please correct the code.”). This works surprisingly well, especially when it is related to the syntax of the code. Tweaking the output of working code, however, is a bigger challenge for ChatGPT because it can neither run/debug the code, read the data, or see the result the code produces. As you can see, you need to remain the driver of the conversion. But done correctly, ChatGPT can be a great “productivity buddy” where ChatGPT can to 80-90% of the work (i.e. write the code), and you take the role of the instructor / interpreter / reviewer."
  },
  {
    "objectID": "posts/2023-03-05-titanic-with-chatgpt/index.html#you-need-to-be-the-eyes-and-ears-of-chatgpt",
    "href": "posts/2023-03-05-titanic-with-chatgpt/index.html#you-need-to-be-the-eyes-and-ears-of-chatgpt",
    "title": "Titanic with ChatGPT",
    "section": "You need to be the Eyes and Ears of ChatGPT",
    "text": "You need to be the Eyes and Ears of ChatGPT\nFor some tasks, however well prompted, ChatGPT struggles to produce great results when it needs the context of the real data, or if it should build on results the code it generated revealed. This is looping back to the same limitations I mentioned before. Let me illustrate what I mean via an example: In feature engineering, ChatGPT had no trouble writing a function to create new feature “Age Group” (“💬: Create a python functions to create the feature”Age Group”“). This prompt would create the following function (in the context of a discussion of the Titanic dataset):\n\ndef get_age_group(age):\n    \"\"\"\n    Bins the age column into different age groups\n    \"\"\"\n    if age &lt; 18:\n        return 'Child'\n    elif age &lt; 65:\n        return 'Adult'\n    else:\n        return 'Senior'\n\nAt first glance, this is reasonable, but it does not take into account, the specific content of our dataset. You could argue that the prompt was not very specific (which was intentional). The point I am trying to make is that ChatGPT creates bins which are logical, but they are not connected to the Titanic dataset. This is the case even though in EDA (GitHub/Kaggle), jointly with ChatGPT, I created the following chart which illustrates the relation between age and chance of survival:\n\nWe can see that small children had a much higher chance of survival (because they probably received the most help and preferential treatment) than any other age group, and people above the age of 60 had a significant lower chance of survival.\nChatGPT cannot see this result even though it generated the code to create the visualization. Again, this is because it can neither run the code nor can it inspect the resulting graph, this remains a human task. But for the refinement, ChatGPT can assist:\n\n💬: Rewrite the get_age_group function to generate the following bins [0, 6, 12, 18, 30, 60, 100], name the bins appropriately\n\nHere is the resulting code:\n\ndef get_age_group(age):\n    \"\"\"\n    Returns the age group based on the given age.\n    \"\"\"\n    if pd.isnull(age):\n        return 'Unknown'\n    elif age &lt;= 6:\n        return 'Infant'\n    elif age &lt;= 12:\n        return 'Child'\n    elif age &lt;= 18:\n        return 'Teenager'\n    elif age &lt;= 30:\n        return 'Young Adult'\n    elif age &lt;= 60:\n        return 'Adult'\n    else:\n        return 'Senior'\n\nMy notebooks contain more examples, but what we can take away is that you need to be the eyes and ears of ChatGPT. ChatGPT is great at generating a lot of code, but you need to interpret the results and guide the way from prompt to prompt. So far, ChatGPT cannot do this. AI cannot take over - yet."
  },
  {
    "objectID": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html",
    "href": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html",
    "title": "Discovering Disaster Tweet Dataset Limitations",
    "section": "",
    "text": "What started out as a simple exercise to visualize model performance using a confusion matrix revealed that the training set contains lots of incorrectly labeled tweets, and that my trained model actually performed better than the score suggests.\n\n\nAfter I had published my first blog post on “Natural Language Processing with Disaster Tweets”), I realized that I had forgotten one element I planned to incorporate into the notebook: I wanted to visualize the model performance using a confusion matrix. Since the implementation was pretty straightforward, the subsequent data analysis will be the real content of this blog post."
  },
  {
    "objectID": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#implementing-the-confusion-matrix",
    "href": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#implementing-the-confusion-matrix",
    "title": "Discovering Disaster Tweet Dataset Limitations",
    "section": "Implementing the Confusion Matrix",
    "text": "Implementing the Confusion Matrix\nTo create the confusion matrix, we need the ground truth from the training data and the predictions from the model on the training data:\n\n# ground truth from the training data\ntrain_true = [int(x) for x in tok_ds['labels']]\n\n# model predictions on the training data\ntrain_preds = trainer.predict(tok_ds).predictions.astype(float)\ntrain_preds = [ 1 if element &gt; 0.6 else 0 for element in train_preds.squeeze()]\n\nLeveraging scikit-learn, here is how I created the confusion matrix\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ncm = confusion_matrix(train_true, train_preds, labels=[0, 1])\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot()\n\nplt.show()\n\n\nThe confusion matrix above displays the best training result I could get using all the methods in my previous blog post/notebook: Training on the model microsoft/deberta-v3-large with 2 epochs at batch size 4 and afterwards another epoch on the full training set. The model nicely classifies many tweets correctly, but there are still 196 false-positives and 553 false negatives.\nThis results in a surprisingly high F1-score of 0.87691 - a lot higher than any of the F1 scores in my submissions.\nSimilar to the approach taken in lesson 2 (where we used the first trained model to find the picture which had the top losses), I reviewed at the incorrectly labeled tweets and found surprising results."
  },
  {
    "objectID": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#data-analysis-discovering-mislabeled-tweets",
    "href": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#data-analysis-discovering-mislabeled-tweets",
    "title": "Discovering Disaster Tweet Dataset Limitations",
    "section": "Data Analysis / Discovering Mislabeled Tweets",
    "text": "Data Analysis / Discovering Mislabeled Tweets\nInspecting the false positives and the true positives revealed the following:\n\nThe false positives (the model classified a tweet as disaster, but there was none), were indeed mostly incorrect predictions made by the model - even though there is quite some room for interpretation.\nThe false negatives (the model classified a tweet as non-disaster, but there was one), however, revealed a significant number of mislabeled tweets.\n\nThe following tweets for sure are no disaster tweets, nonetheless they are labeled as disasters in the training data:\n\n\n\n\n\n\n\n\n\nid\nTweet\nlabel\npred\n\n\n\n\n443\nShort Reading Apocalypse 21:1023 In the spirit the angel took me to the top of an enormous high mountain and…\n1\n0\n\n\n794\nChevrolet : Avalanche LT 2011 lt used 5.3 l v 8 16 v automatic 4 wd pickup truck premium b_\n1\n0\n\n\n1051\nI waited 2.5 hours to get a cab my feet are bleeding\n1\n0\n\n\n1239\npeople with a #tattoo out there.. Are u allowed to donate blood and receive blood as well or not?\n1\n0\n\n\n\nWhile all these tweets contain words which could potentially indicate a disaster (apocalypse, avalanche, bleeding, blood)\n\nthe first tweet is a quote from the bible,\nthe second one is related to a car,\nthe third one undoubtedly feels unpleasant,\nand fourth one is simply a question on donating blood.\n\nAs it turned out, these 4 examples are not the only ones. After having read through the IDs up to 1000 in the false negatives, I had found about 80 mis-labeled tweets."
  },
  {
    "objectID": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#re-training-a-model-using-corrected-labels",
    "href": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#re-training-a-model-using-corrected-labels",
    "title": "Discovering Disaster Tweet Dataset Limitations",
    "section": "Re-Training a model using corrected labels",
    "text": "Re-Training a model using corrected labels\nEven though re-labeling tweets is a tedious work, in this notebook, which is a copy of my previous one, I have added a new section which re-labels tweets based on the results in the first notebook.\nMy clear expectation was that model performance would go up because of the increased quality of the updated training set. The opposite, however, was the case: Model performance dropped from 84.6% to 84% 😮 - what was going on?\nThe only explanation I have for this result is that both the training and the test data systematically contain mislabeled tweets. Reasons could be\n\nmisinterpretation of the tweets, some tweets are obviously ambiguous\nsimple error in the monotonous task of labeling data\nusing an outdated algorithm for labeling tweets - just speculating 😉\n\nThe current latest version of the notebook (V8) further supports this claim. It has scored a little bit better (84.3%). This increase was caused by mislabeling a bunch of tweets: There are quite a few tweets in the dataset which follow this pattern: “#FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps”. This tweet simply informs about a new FedEx policy, thus it is no disaster tweet from my point of view. But in the dataset 66% of tweets with this content are labeled as disasters. By accepting the majority rule, the score increased, even though the label is incorrect in my opinion.\nAll of this tells us that there is a problem in the dataset of the Kaggle competition, some of it may be subjective, other tweets are objectively mislabeled. Therefore, if you are working on the Kaggle competition for disaster tweets and the improvements you make to your model do not materialize in a better result, it might also be the dataset’s fault, not yours. Consider this: If your model would classify every tweet correctly in the test set, you would not score 100% because of the mislabeled tweets in the test set.\nBut I think we can gain some deeper insights than the simple observation that the dataset contains some issues."
  },
  {
    "objectID": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#conclusion",
    "href": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#conclusion",
    "title": "Discovering Disaster Tweet Dataset Limitations",
    "section": "Conclusion",
    "text": "Conclusion\nWhen working in machine learning, we sometimes tend to focus a lot on the technology aspects, trying to tune hyperparameters, using bigger or more advanced models etc. In this case, the ground truth is the limiting factor to what the model can achieve. The pre-trained model has actually outperformed the data set by clearly pointing out mislabeled data. This shows that the pre-trained model understands quite a bit about language, not only the grammar, but also about the content. This is not super-surprising when you think about the task for the model. A language model is basically trained to predict the next word for a prompt, and we have re-trained it (the last layer or a bit more) to classify tweets. So it is not just trained on the classification task, but the model still understands quite a lot about the content about the tweets.\nThe (training) data is maybe the most important asset in machine learning, and it can come with quite a few problems: Not only can it be biased, difficult to obtain, hard to parse, it can also be simply wrong! So the true lesson is not to blindly trust the data, but thorough inspection of the data is always called for. By using faulty data, our model has become biased: It has learned from the mislabeled tweets, and I suspect it has become too sensitive when detecting disasters. When non-disaster tweets are classified as disasters in the training data, the model calls out a disaster when there is none - similar to if you called an ambulance for a slight cough. The pre-training, however, still seems to contain enough “common sense” that the model can still call out mislabeled tweets.\nTherefore, the F1-score of the submission does not represent the true model performance: The score should be higher. My subjective estimate is that at least 30% of the false negatives should be true positives. This would lift the F1-score by about 2.5 percentage points.\nWhat do you think? Did I assign too much credit to the pre-trained model, for example, by crediting it with the capability to do a bit of common-sense reasoning? Does the data quality of the competition define the ceiling of the score that you can achieve, or do you see ways to navigate around this obstacle?"
  },
  {
    "objectID": "posts/2022-11-27-rebuilding-quarto-blog/index.html",
    "href": "posts/2022-11-27-rebuilding-quarto-blog/index.html",
    "title": "When disaster strikes: Re-building a Quarto Blog",
    "section": "",
    "text": "For the last 2 months I have been a proud writer of this blog, until yesterday disaster struck: Upon publishing of my MNIST-blog post via the usual quarto publish gh-pages, I received the following error message 😨:\nThe publishing was not completed, and my blog only showed a naked header, but no posts any more, essentially everything was gone 😱 (at least online)\nTrying to google a quick fix did not reveal any real result. Due to lack of time, I had to (officially) stop for the day, but back in my mind, this was really nagging me…"
  },
  {
    "objectID": "posts/2022-11-27-rebuilding-quarto-blog/index.html#what-is-needed-to-re-build-a-quarto-blog",
    "href": "posts/2022-11-27-rebuilding-quarto-blog/index.html#what-is-needed-to-re-build-a-quarto-blog",
    "title": "When disaster strikes: Re-building a Quarto Blog",
    "section": "What is needed to Re-build a Quarto Blog?",
    "text": "What is needed to Re-build a Quarto Blog?\nAs I kept thinking about this, the error message clearly pointed to something on my local machine. Additionally, I previously posted on how to avoid disaster, so what would be the best way to re-build everything?\nBut let’s think through the matter: What do you actually need to re-build a Quarto blog? You only need your posts-directory and a few other files (the ones mentioned here).\nThis is important, so let me re-phrase this: On your GitHub repo, there are (should be) 2 versions of your blog:\n\nIn the main-branch, you store your source-files:\n\nThe Jupyter notebooks, markdown files, some pictures used within your posts\nThe configuration files: Some .yml-, .qmd- and .css-files\n\nThe gh-pages branch gets generated to contain the rendered versions of your posts: When you check out your _site directory, it contains a file structure similar to the posts-directory in your main-branch, but the _site directory deals in html-, xml-, and json-files.\n\nTherefore, to re-build your site, as far as I understand it, you only need the content of your main branch, and the content of the gh-pages gets generated once you run quarto publish gh-pages."
  },
  {
    "objectID": "posts/2022-11-27-rebuilding-quarto-blog/index.html#re-building-my-quarto-blog",
    "href": "posts/2022-11-27-rebuilding-quarto-blog/index.html#re-building-my-quarto-blog",
    "title": "When disaster strikes: Re-building a Quarto Blog",
    "section": "Re-building my Quarto Blog",
    "text": "Re-building my Quarto Blog\nWith the above in mind, here is what I did:\n\nI moved my local copy of the blog’s repo’s main branch to my temp folder (the milder version of deleting it)\nI re-cloned the repo (the main branch): git clone git@github.com:chrwittm/chrwittm.github.io.git\nI re-published the blog: quarto publish gh-pages\n\nAnd violà: My blog was back online. 😃"
  },
  {
    "objectID": "posts/2022-11-27-rebuilding-quarto-blog/index.html#conclusion",
    "href": "posts/2022-11-27-rebuilding-quarto-blog/index.html#conclusion",
    "title": "When disaster strikes: Re-building a Quarto Blog",
    "section": "Conclusion",
    "text": "Conclusion\nFirst of all: With these kind of seeming disasters: Sit back and relax: Is this a big deal? Well my blog was down, who cared? Probably I cared the most about it - my ego 😉.\nDon’t panic: Think through a problem: What could be the root cause, from what angle can you approach the problem?\nI think, the solution is actually really nice. The setup with the 2 branches has some nice redundancy built-in, and without having tried it before, the disaster recovery performed very well.\nThe solution also reminded me of the fast-setup approach, which Jeremy discussed in the Live-Coding session 1: You should spend time using your tools, not configuring them. Quarto, at least to me, nicely proved that point that even if something goes wrong, you can quickly recover.\nHappy blogging!"
  },
  {
    "objectID": "posts/2024-08-02-llm-calculator2-vision/index.html",
    "href": "posts/2024-08-02-llm-calculator2-vision/index.html",
    "title": "Building the Apple Calculator in a Jupyter Notebook",
    "section": "",
    "text": "Did you see Apple’s iPad calculator demo on WWDC 2024? Apple calls this feature “Math Notes”. It looks pretty cool how you can hand-write calculations, and it solves them on the fly. Apple created a seamless UI, but if you remove the interactivity, it is “just” a vision-based calculator which solves the equations on the screen. Building on my previous blog post (“How to Turn GPT into a Calculator”), let’s implement a vision-based calculator!\nIn the first two sections of this blog post, we re-create the calculator of “How to Turn GPT into a Calculator”. Essentially, there is nothing new in these two sections, it is just a summary of my previous blog post.\nThe implementation of the vision-based calculator starts in section “Implementing the Vision-based Calculator”. Please feel free to skip ahead to this section.\nBefore we jump into the code, if you prefer the interactive experience of a Jupyter notebook, please hop over to GitHub for the Jupyter Notebook version of this blog post."
  },
  {
    "objectID": "posts/2024-08-02-llm-calculator2-vision/index.html#implementing-chat-basics",
    "href": "posts/2024-08-02-llm-calculator2-vision/index.html#implementing-chat-basics",
    "title": "Building the Apple Calculator in a Jupyter Notebook",
    "section": "Implementing Chat Basics",
    "text": "Implementing Chat Basics\nLet’s start with the final versions of the chat messages and chat client classes based on Building Chat for Jupyter Notebooks from Scratch and How to Turn GPT into a Calculator.\n\n\nCode\nfrom IPython.display import display, Markdown\n\nclass ChatMessages:\n\n    def __init__(self):\n        \"\"\"Initializes the Chat.\"\"\"\n        self._messages = []\n\n    def _append_message(self, role, content):\n        \"\"\"Appends a message with specified role and content to messages list.\"\"\"\n        self._messages.append({\"role\": role, \"content\": content})\n\n    def append_system_message(self, content):\n        \"\"\"Appends a system message with specified content to messages list.\"\"\"\n        self._append_message(\"system\", content)\n    \n    def append_tool_message(self, content, tool_call_id):\n        \"\"\"Appends a tool message with specified content to messages list.\"\"\"\n        self._messages.append({\"role\": \"tool\", \"content\": content, \"tool_call_id\": tool_call_id})\n\n    def append_user_message(self, content):\n        \"\"\"Appends a user message with specified content to messages list.\"\"\"\n        self._append_message(\"user\", content)\n\n    def append_assistant_message(self, content=None, tool_calls=None):\n        \"\"\"Appends an assistant message with specified content to messages list.\"\"\"\n        if content:\n            self._append_message(\"assistant\", content)\n        else:\n            self._messages.append({\"role\": \"assistant\", \"tool_calls\": tool_calls})\n\n    def get_messages(self):\n        \"\"\"Returns a shallow copy of the messages list.\"\"\"\n        return self._messages[:]\n    \n    def get_last_assistant_message(self):\n        \"\"\"Returns the content of the last assistant message\"\"\"\n        return self._messages[-1]['content']\n    \n    def get_debug_view(self):\n        \"\"\"Returns the debug view of the chat messages formatted as Markdown.\"\"\"\n        debug_view = []\n        for message in self._messages:\n            role = message.get('role')\n            content = message.get('content', '')\n\n            if role == 'system' or role == 'user':\n                debug_view.append(f\"**{role}**: {content}\\n\")\n\n            elif role == 'assistant':\n                if 'tool_calls' in message:\n                    debug_view.append(\"**tool calls**\\n\")\n                    for i, tool_call in enumerate(message['tool_calls'], start=1):\n                        function_name = tool_call.function.name\n                        arguments = tool_call.function.arguments\n                        tool_call_id = tool_call.id\n                        debug_view.append(f\"{i}. tool: {function_name}: {arguments} (tool call id: {tool_call_id})\\n\")\n                else:\n                    debug_view.append(f\"**assistant**: {content}\\n\")\n\n            elif role == 'tool':\n                tool_call_id = message.get('tool_call_id', '')\n                debug_view.append(f\"**tool result**: {content} (tool call id: {tool_call_id})\\n\")\n\n        return Markdown('\\n'.join(debug_view))\n\n\n\n\nCode\n#model_name = \"gpt-3.5-turbo\"\n#model_name = \"gpt-4o-mini\"\nmodel_name = \"gpt-4o\"\n\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv(\".env\")\n\nfrom openai import chat\n\nclass ChatClient:\n\n    def __init__(self, system_message=None, tools=None):\n        \"\"\"Initializes the Chat with the system message.\"\"\"\n        self._chat_messages = ChatMessages()\n        if system_message:\n            self._chat_messages.append_system_message(system_message)\n        self._tools = tools\n\n    def call_tool(self, tool_call):\n        \"\"\"returns the result of an LLM tool call\"\"\"\n        fc = tool_call.function #Updated\n        if fc.name not in funcs_ok: return print(f'Not allowed: {fc.name}')\n        f = globals()[fc.name]\n        return f(**json.loads(fc.arguments))\n\n    def call_tools(self, tool_calls):\n        \"\"\"Processes the tool calls of the LLM response and calls the LLM API again\"\"\"\n        for tool_call in tool_calls:\n            chat_client._chat_messages.append_tool_message(\n                content=str(self.call_tool(tool_call)),\n                tool_call_id=tool_call.id)\n            \n        self.ask_gpt()\n\n    def get_model_response(self):\n        \"\"\"Calls the LLM chat completion API\"\"\"\n        return chat.completions.create(\n            model=model_name,\n            messages=self._chat_messages.get_messages(),\n            tools=self._tools)\n\n    def ask_gpt(self, prompt=None):\n        \"\"\"Calls the LLM chat completion API and returns the response message\"\"\"\n        \n        if prompt:\n            self._chat_messages.append_user_message(prompt)\n\n        c = self.get_model_response()\n        content = c.choices[0].message.content\n        tool_calls = c.choices[0].message.tool_calls\n\n        self._chat_messages.append_assistant_message(\n            content=content,\n            tool_calls=tool_calls)\n        \n        if tool_calls:\n            self.call_tools(tool_calls)\n\n        return Markdown(self._chat_messages.get_last_assistant_message())\n\n\nLet’s run a quick test:\n\nchat_client = ChatClient(\"Answer in a very concise and accurate way\")\nchat_client.ask_gpt(\"Name the planets in the solar system\")\n\nMercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune.\n\n\n\nchat_client.ask_gpt(\"Reverse the list\")\n\nNeptune, Uranus, Saturn, Jupiter, Mars, Earth, Venus, Mercury."
  },
  {
    "objectID": "posts/2024-08-02-llm-calculator2-vision/index.html#re-creating-the-calculator",
    "href": "posts/2024-08-02-llm-calculator2-vision/index.html#re-creating-the-calculator",
    "title": "Building the Apple Calculator in a Jupyter Notebook",
    "section": "Re-creating the Calculator",
    "text": "Re-creating the Calculator\nWith the chat up and running, let’s add function calling to enable reliable calculations.\n\n\nCode\nfrom pydantic import create_model\nimport inspect, json\nfrom inspect import Parameter\n\ndef get_schema(f):\n    kw = {n:(o.annotation, ... if o.default==Parameter.empty else o.default)\n          for n,o in inspect.signature(f).parameters.items()}\n    # update: schema -&gt; model_json_schema\n    s = create_model(f'Input for `{f.__name__}`', **kw).model_json_schema()\n    # update: added function level in tools json\n    function_params = dict(name=f.__name__, description=f.__doc__, parameters=s)\n    return dict(type=\"function\", function=function_params)\n\n\n\n\nCode\ndef add(a: float, b: float = 1.0):\n    \"Adds a + b\"\n    return a + b\n\ndef subtract(a: float, b: float = 1.0):\n    \"Subtracts a - b\"\n    return a - b\n\ndef multiply(a: float, b: float = 1.0):\n    \"Multiplies a * b\"\n    return a * b\n\ndef divide(a: float, b: float = 1.0):\n    \"Divides a / b\"\n    if b == 0:\n        return \"Division by zero is not allowed.\"\n    return a / b\n\nfuncs_ok = {'add', 'subtract', 'multiply', 'divide'}\n\ndef get_calc_tools():\n    return [get_schema(add), get_schema(subtract), get_schema(multiply), get_schema(divide)]\n\n\n\n\nCode\nsystem_prompt = (\n    \"You are a calculator. \\n\"\n    \"Do not do even the simplest computations on your own, \\n\"\n    \"but use the tools provided. \\n\"\n    \"After the tool calls, explain the steps you took when answering. \\n\"\n    \"Answer with an accuracy of 3 decimals. \\n\"\n    \"Format all mathematical equations with LaTeX syntax so that they will render correctly in a Jupyter notebook, make absolutely sure to enclose LaTeX in `$`or `$$`.\"\n)\n\n\nLet’s run a quick test:\n\nprint(f\"Expected result: {(6573 + 1) * 9132}\")\nprint(f\"LLM response:\")\nchat_client = ChatClient(system_message=system_prompt, tools=get_calc_tools())\nchat_client.ask_gpt(\"What is (6573 + 1) * 9132?\")\n\nExpected result: 60033768\nLLM response:\n\n\nThe result of the expression \\((6573 + 1) * 9132\\) is \\(60,033,768\\).\nHere are the steps taken:\n\nCompute \\(6573 + 1\\) which results in \\(6574\\).\nMultiply \\(6574\\) by \\(9132\\), giving \\(60,033,768\\).\n\n\n\nAt this point, we have re-created the calculator from “How to Turn GPT into a Calculator”. Let’t turn it into a vision-based calculator."
  },
  {
    "objectID": "posts/2024-08-02-llm-calculator2-vision/index.html#implementing-the-vision-based-calculator",
    "href": "posts/2024-08-02-llm-calculator2-vision/index.html#implementing-the-vision-based-calculator",
    "title": "Building the Apple Calculator in a Jupyter Notebook",
    "section": "Implementing the Vision-based Calculator",
    "text": "Implementing the Vision-based Calculator\nThe following image will is our first test case. This is the hand-written calculation we used earlier:\n\n\n\nThe hand-written calculation we used earlier: (6573 + 1) * 9132\n\n\nAccording to the OpenAI documentation, we need to encode images we want to upload to ChatGPT in base64.\n\nimport base64\n\ndef encode_image(image_path):\n  \"\"\"Encodes an image file in base64\"\"\"\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\nimage_path = \"test-calculation.png\"\nbase64_image = encode_image(image_path)\n\nWhen passing an image in the chat, the format of the user-message changes. The content is not just a string, but we embed the image alongside the prompt.\n\nfrom fastcore.utils import * #for importing patch\n\n@patch \ndef append_user_message(self:ChatMessages, content=None, base64_image=None):\n    \"\"\"Appends a user message with specified content to messages list.\"\"\"\n    if base64_image:\n        image_content = [\n            {\"type\": \"text\", \"text\": content},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}}\n        ]\n        self._messages.append({\"role\": \"user\", \"content\": image_content})  \n    else:\n        self._append_message(\"user\", content)\n        \n\nConsequently, we need to update the ask_gpt-method:\n\n@patch\ndef ask_gpt(self:ChatClient, prompt=None, base64_image=None):\n    \n    if base64_image:\n        self._chat_messages.append_user_message(content=prompt, base64_image=base64_image)\n\n    if prompt:\n        self._chat_messages.append_user_message(prompt)\n\n    c = self.get_model_response()\n    content = c.choices[0].message.content\n    tool_calls = c.choices[0].message.tool_calls\n\n    self._chat_messages.append_assistant_message(\n        content=content,\n        tool_calls=tool_calls)\n    \n    if tool_calls:\n        self.call_tools(tool_calls)\n\n    return Markdown(self._chat_messages.get_last_assistant_message())\n\nThat was essentially all the code we need to upgrade the calculator to add vision capabilities. Let’s run a quick test:\n\nprint(f\"Expected result: {(6573 + 1) * 9132}\")\nprint(f\"LLM response:\")\nchat_client = ChatClient(system_message=system_prompt, tools=get_calc_tools())\nchat_client.ask_gpt(\"Perform the calculation on the image\", base64_image=base64_image)\n\nExpected result: 60033768\nLLM response:\n\n\nFirst, we evaluate the inner expression:\n\\(6573 + 1 = 6574\\)\nNext, we perform the multiplication:\n\\(6574 \\times 9132 = 60,033,768\\)\nSo, the result of the calculation is (60,033,768).\n\n\nLet’s take a look under the hood to see what happened in the background.\n\nchat_client._chat_messages.get_debug_view()\n\nsystem: You are a calculator. Do not do even the simplest computations on your own, but use the tools provided. After the tool calls, explain the steps you took when answering. Answer with an accuracy of 3 decimals. Format all mathematical equations with LaTeX syntax so that they will render correctly in a Jupyter notebook, make absolutely sure to enclose LaTeX in $or $$.\nuser: [{‘type’: ‘text’, ‘text’: ‘Perform the calculation on the image’}, {‘type’: ‘image_url’, ‘image_url’: {‘url’: ‘data:image/png;base64,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’}}]\nuser: Perform the calculation on the image\ntool calls\n\ntool: add: {“a”: 6573, “b”: 1} (tool call id: call_Nc2s856OoeVy3RMVyxncr9H2)\ntool: multiply: {“a”: 6574, “b”: 9132} (tool call id: call_EslHqrFoFj9L7h78SSqvZlUi)\n\ntool result: 6574 (tool call id: call_Nc2s856OoeVy3RMVyxncr9H2)\ntool result: 60033768 (tool call id: call_EslHqrFoFj9L7h78SSqvZlUi)\nassistant: First, we evaluate the inner expression:\n\\(6573 + 1 = 6574\\)\nNext, we perform the multiplication:\n\\(6574 \\times 9132 = 60,033,768\\)\nSo, the result of the calculation is (60,033,768)."
  },
  {
    "objectID": "posts/2024-08-02-llm-calculator2-vision/index.html#running-the-apple-calculator",
    "href": "posts/2024-08-02-llm-calculator2-vision/index.html#running-the-apple-calculator",
    "title": "Building the Apple Calculator in a Jupyter Notebook",
    "section": "Running the Apple Calculator",
    "text": "Running the Apple Calculator\nNow, we are ready to run the calculations from the WWDC 2024 video. Here is an image of the calculations without the solutions. Let’s ask ChatGPT to solve them:\n\n\n\nThe WWDC 2024 calculations without solutions\n\n\n\nimage_path = \"apple-calculations.png\"\nbase64_image = encode_image(image_path)\n\nchat_client = ChatClient(system_message=system_prompt, tools=get_calc_tools())\nchat_client.ask_gpt(\"Perform the calculation on the image. Only respond by writing out the solved calculations. Remember to enclose LaTeX in `$`or `$$`\", base64_image=base64_image)\n\nBased on the calculations:\n\\[57 - 18 = 39\\] \\[ (3 + 4) \\times 20 = 140\\] \\[ \\frac{36 \\times 4}{12^2} = \\frac{144}{144} = 1\\] \\[ \\sin(30^\\circ) = 0.5\\] \\[ \\frac{10^2 \\div 50}{\\pi} = \\frac{2}{\\pi} \\approx 0.637\\] \\[ 96 \\times 28 = 2688\\]\n\n\nTo double-check the result, here are the solutions from Apple:\n\n\n\nThe WWDC 2024 calculations with solutions"
  },
  {
    "objectID": "posts/2025-12-17-sap-rpt-1-aicore/index.html",
    "href": "posts/2025-12-17-sap-rpt-1-aicore/index.html",
    "title": "Building an SAP-RPT-1 ‘Hello World’ with AI Core",
    "section": "",
    "text": "Using SAP-RPT-1 in the playground is a nice way to get a first impression, but you’ll soon realize there are some limitations. In this blog post, let’s explore how to use SAP-RPT-1 in a production-like environment. I’ll walk you through the steps to build a simple “Hello World” application using SAP-RPT-1 and AI Core."
  },
  {
    "objectID": "posts/2025-12-17-sap-rpt-1-aicore/index.html#creating-a-deployment-for-sap-rpt-1",
    "href": "posts/2025-12-17-sap-rpt-1-aicore/index.html#creating-a-deployment-for-sap-rpt-1",
    "title": "Building an SAP-RPT-1 ‘Hello World’ with AI Core",
    "section": "Creating a Deployment for SAP-RPT-1",
    "text": "Creating a Deployment for SAP-RPT-1\nThroughout this blog post, I’ll assume that you have access to a BTP subaccount with instances of the AI Core service (extended plan) and the AI Launchpad service (standard plan).\nFirst, you need to create a configuration for SAP-RPT-1 with the following parameters:\n\nConfiguration Name: For example, sap-rpt-1-large or sap-rpt-1-small (I recommend naming the configuration so that you can easily recognize the underlying model)\nScenario: foundation-models\nVersion: 0.0.1 (default value)\nExecutable: aicore-sap (meaning that this model is hosted by SAP; you can find this value in note 3437766)\nModel Name: sap-rpt-1-large or sap-rpt-1-small (again, as per note 3437766)\nModel Version: latest\n\nIn the next step, you can turn this configuration into a deployment by clicking on the “Create Deployment” button in the configuration overview. You can leave all the default values as they are and create the deployment.\nAfter a few minutes, the deployment should reach the status “Running” and you can start using it. Note that a deployment URL has also been created, something like https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/1234567890abcdef. The most important part is the deployment ID at the end of the URL (in this example 1234567890abcdef), which you’ll need later on."
  },
  {
    "objectID": "posts/2025-12-17-sap-rpt-1-aicore/index.html#handling-ai-core-authentication",
    "href": "posts/2025-12-17-sap-rpt-1-aicore/index.html#handling-ai-core-authentication",
    "title": "Building an SAP-RPT-1 ‘Hello World’ with AI Core",
    "section": "Handling AI Core Authentication",
    "text": "Handling AI Core Authentication\nAt the time of writing, the SAP Cloud SDK for Python does not yet support the RPT-1 models. Following the documentation, we will therefore use the requests library to call the deployment endpoint directly.\nBefore we can do that, however, we need to authenticate as also described in the documentation. What may look intimidating at first is actually quite straightforward. We only need a few parameters which we’ll store securely in a .env file. You can collect this information from the service key of your AI Core instance.\nAICORE_API_URL=\"https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2\"\nAICORE_AUTH_URL=\"https://a1b2c3d4e4f61234.authentication.eu10.hana.ondemand.com\" # called \"url\" in the service key\nAICORE_CLIENT_ID=\"sb-95d9f03d\" # shortened, yours will be longer\nAICORE_CLIENT_SECRET=\"c51ad55c\"  # shortened, yours will be longer\nAICORE_RESOURCE_GROUP=\"default\"  # your actual resource group\nAdditionally, store the RPT deployment ID(s) you collected earlier:\nRPT1L_DEPLOYMENT_ID=\"1234567890abcdef\"  # your actual deployment ID for RPT-1 large\nRPT1S_DEPLOYMENT_ID=\"abcdef1234567890\"  # your actual deployment ID for RPT-1 small\nOnce the .env file is ready, we can use the following code to retrieve an access token from the authentication endpoint:\n\n\nCode\nimport os\nimport requests\nfrom dotenv import load_dotenv\n\n# Load .env file\nload_dotenv()\n\nAICORE_AUTH_URL = os.environ[\"AICORE_AUTH_URL\"]\nAICORE_CLIENT_ID = os.environ[\"AICORE_CLIENT_ID\"]\nAICORE_CLIENT_SECRET = os.environ[\"AICORE_CLIENT_SECRET\"]\n\ntoken_url = f\"{AICORE_AUTH_URL}/oauth/token\"\n\nresponse = requests.post(\n    token_url,\n    auth=(AICORE_CLIENT_ID, AICORE_CLIENT_SECRET),\n    data={\"grant_type\": \"client_credentials\"},\n)\n\nresponse.raise_for_status()\n\ntoken_data = response.json()\naccess_token = token_data[\"access_token\"]"
  },
  {
    "objectID": "posts/2025-12-17-sap-rpt-1-aicore/index.html#preparing-test-data",
    "href": "posts/2025-12-17-sap-rpt-1-aicore/index.html#preparing-test-data",
    "title": "Building an SAP-RPT-1 ‘Hello World’ with AI Core",
    "section": "Preparing Test Data",
    "text": "Preparing Test Data\nFor testing the connection to the RPT-1 deployment, we’ll use the same test data we used when trying out RPT-1 in the playground. Here’s the test data in the format RPT-1 expects:\n\nNote: For the next steps, I also created a Jupyter notebook, which you can download to interactively do all the steps below yourself.\n\n\n\nCode\npayload = {\n    \"prediction_config\": {\n        \"target_columns\": [\n            {\n                \"name\": \"SALESGROUP\",\n                \"prediction_placeholder\": \"[PREDICT]\"\n                # \"task_type\": \"classification\" or \"regression\" can be specified here if needed\n            }\n        ]\n    },\n    \"index_column\": \"ID\",\n    \"rows\": [\n        {\n            \"ID\": \"1001\",\n            \"PRODUCT\": \"Tablet\",\n            \"PRICE\": 599.00,\n            \"CUSTOMER\": \"TechStart Inc\",\n            \"COUNTRY\": \"USA\",\n            \"SALESGROUP\": \"[PREDICT]\"\n        },\n        {\n            \"ID\": \"1002\",\n            \"PRODUCT\": \"Standing Desk\",\n            \"PRICE\": 325.50,\n            \"CUSTOMER\": \"Workspace Solutions\",\n            \"COUNTRY\": \"Germany\",\n            \"SALESGROUP\": \"[PREDICT]\"\n        },\n        {\n            \"ID\": \"1003\",\n            \"PRODUCT\": \"Workstation\",\n            \"PRICE\": 1450.00,\n            \"CUSTOMER\": \"Enterprise Systems Ltd\",\n            \"COUNTRY\": \"Canada\",\n            \"SALESGROUP\": \"Enterprise Solutions\"\n        },\n        {\n            \"ID\": \"1004\",\n            \"PRODUCT\": \"Laptop Pro\",\n            \"PRICE\": 1899.99,\n            \"CUSTOMER\": \"Business Corp\",\n            \"COUNTRY\": \"UK\",\n            \"SALESGROUP\": \"Enterprise Solutions\"\n        },\n        {\n            \"ID\": \"1005\",\n            \"PRODUCT\": \"Gaming Laptop\",\n            \"PRICE\": 1250.00,\n            \"CUSTOMER\": \"Digital Ventures\",\n            \"COUNTRY\": \"USA\",\n            \"SALESGROUP\": \"Enterprise Solutions\"\n        },\n        {\n            \"ID\": \"1006\",\n            \"PRODUCT\": \"Smart Watch\",\n            \"PRICE\": 299.99,\n            \"CUSTOMER\": \"Gadget Store\",\n            \"COUNTRY\": \"Australia\",\n            \"SALESGROUP\": \"Consumer Electronics\"\n        },\n        {\n            \"ID\": \"1007\",\n            \"PRODUCT\": \"Ergonomic Chair\",\n            \"PRICE\": 445.00,\n            \"CUSTOMER\": \"Office Outfitters\",\n            \"COUNTRY\": \"France\",\n            \"SALESGROUP\": \"Office Furniture\"\n        },\n        {\n            \"ID\": \"1008\",\n            \"PRODUCT\": \"Storage Array\",\n            \"PRICE\": 3500.00,\n            \"CUSTOMER\": \"CloudTech Systems\",\n            \"COUNTRY\": \"Singapore\",\n            \"SALESGROUP\": \"Data Infrastructure\"\n        },\n        {\n            \"ID\": \"1009\",\n            \"PRODUCT\": \"Network Switch\",\n            \"PRICE\": 175.50,\n            \"CUSTOMER\": \"ConnectIT\",\n            \"COUNTRY\": \"Japan\",\n            \"SALESGROUP\": \"Networking Devices\"\n        }\n    ]\n}\n\n\nFor better readability, here is the data in table format:\n\n\nCode\nimport pandas as pd\n\nsample_data = pd.DataFrame(payload[\"rows\"])\nsample_data\n\n\n\n\n\n\n\n\n\nID\nPRODUCT\nPRICE\nCUSTOMER\nCOUNTRY\nSALESGROUP\n\n\n\n\n0\n1001\nTablet\n599.00\nTechStart Inc\nUSA\n[PREDICT]\n\n\n1\n1002\nStanding Desk\n325.50\nWorkspace Solutions\nGermany\n[PREDICT]\n\n\n2\n1003\nWorkstation\n1450.00\nEnterprise Systems Ltd\nCanada\nEnterprise Solutions\n\n\n3\n1004\nLaptop Pro\n1899.99\nBusiness Corp\nUK\nEnterprise Solutions\n\n\n4\n1005\nGaming Laptop\n1250.00\nDigital Ventures\nUSA\nEnterprise Solutions\n\n\n5\n1006\nSmart Watch\n299.99\nGadget Store\nAustralia\nConsumer Electronics\n\n\n6\n1007\nErgonomic Chair\n445.00\nOffice Outfitters\nFrance\nOffice Furniture\n\n\n7\n1008\nStorage Array\n3500.00\nCloudTech Systems\nSingapore\nData Infrastructure\n\n\n8\n1009\nNetwork Switch\n175.50\nConnectIT\nJapan\nNetworking Devices"
  },
  {
    "objectID": "posts/2025-12-17-sap-rpt-1-aicore/index.html#running-the-predictions",
    "href": "posts/2025-12-17-sap-rpt-1-aicore/index.html#running-the-predictions",
    "title": "Building an SAP-RPT-1 ‘Hello World’ with AI Core",
    "section": "Running the predictions",
    "text": "Running the predictions\nTo run the predictions, we will use the requests library to send an HTTP POST request to the RPT-1 deployment endpoint. The request will include our test data and the authorization token.\n\n\nCode\nAICORE_API_URL = os.environ[\"AICORE_API_URL\"].rstrip(\"/\")\nAICORE_RESOURCE_GROUP = os.environ.get(\"AICORE_RESOURCE_GROUP\", \"default\")\nRPT1_DEPLOYMENT_ID = os.environ.get(\"RPT1L_DEPLOYMENT_ID\")\n\nif not RPT1_DEPLOYMENT_ID:\n    raise ValueError(\"Missing RPT1L_DEPLOYMENT_ID in .env (deployment id from AI Launchpad).\")\n\nurl = f\"{AICORE_API_URL}/inference/deployments/{RPT1_DEPLOYMENT_ID}/predict\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {access_token}\",\n    \"AI-Resource-Group\": AICORE_RESOURCE_GROUP,\n    \"Content-Type\": \"application/json\",\n    \"Accept\": \"application/json\",\n}\n\nresponse = requests.post(url, headers=headers, json=payload, timeout=120)\nresponse.raise_for_status()"
  },
  {
    "objectID": "posts/2025-12-17-sap-rpt-1-aicore/index.html#analyzing-the-results",
    "href": "posts/2025-12-17-sap-rpt-1-aicore/index.html#analyzing-the-results",
    "title": "Building an SAP-RPT-1 ‘Hello World’ with AI Core",
    "section": "Analyzing the results",
    "text": "Analyzing the results\nLet’s take a look at the results returned by RPT-1. Here are the predictions in the raw JSON format:\n\n\nCode\nimport json\n\ndata = response.json()\npreds = data[\"predictions\"]\nprint(json.dumps(preds, indent=2, ensure_ascii=False))\n\n\n[\n  {\n    \"ID\": 1001,\n    \"SALESGROUP\": [\n      {\n        \"confidence\": 0.93,\n        \"prediction\": \"Enterprise Solutions\"\n      }\n    ]\n  },\n  {\n    \"ID\": 1002,\n    \"SALESGROUP\": [\n      {\n        \"confidence\": 0.78,\n        \"prediction\": \"Office Furniture\"\n      }\n    ]\n  }\n]\n\n\nNote that the result looks different from what we saw in the playground, which hosts the RPT-1-OSS model. RPT-1-Large also returns a confidence score for each prediction.\nLet’s merge these predictions back into the tabular format to see the results more clearly:\n\n\nCode\npreds_df = sample_data.copy(deep=True)\n\npreds_df[\"ID\"] = preds_df[\"ID\"].astype(int)\n\nfor pred in preds:\n    row_idx = int(pred[\"ID\"])\n    predicted_value = pred[\"SALESGROUP\"][0][\"prediction\"]\n    preds_df.loc[preds_df[\"ID\"] == row_idx, \"SALESGROUP\"] = predicted_value\n\npreds_df\n\n\n\n\n\n\n\n\n\nID\nPRODUCT\nPRICE\nCUSTOMER\nCOUNTRY\nSALESGROUP\n\n\n\n\n0\n1001\nTablet\n599.00\nTechStart Inc\nUSA\nEnterprise Solutions\n\n\n1\n1002\nStanding Desk\n325.50\nWorkspace Solutions\nGermany\nOffice Furniture\n\n\n2\n1003\nWorkstation\n1450.00\nEnterprise Systems Ltd\nCanada\nEnterprise Solutions\n\n\n3\n1004\nLaptop Pro\n1899.99\nBusiness Corp\nUK\nEnterprise Solutions\n\n\n4\n1005\nGaming Laptop\n1250.00\nDigital Ventures\nUSA\nEnterprise Solutions\n\n\n5\n1006\nSmart Watch\n299.99\nGadget Store\nAustralia\nConsumer Electronics\n\n\n6\n1007\nErgonomic Chair\n445.00\nOffice Outfitters\nFrance\nOffice Furniture\n\n\n7\n1008\nStorage Array\n3500.00\nCloudTech Systems\nSingapore\nData Infrastructure\n\n\n8\n1009\nNetwork Switch\n175.50\nConnectIT\nJapan\nNetworking Devices\n\n\n\n\n\n\n\nFor comparison, here is the original sample data before predictions:\n\n\nCode\nsample_data\n\n\n\n\n\n\n\n\n\nID\nPRODUCT\nPRICE\nCUSTOMER\nCOUNTRY\nSALESGROUP\n\n\n\n\n0\n1001\nTablet\n599.00\nTechStart Inc\nUSA\n[PREDICT]\n\n\n1\n1002\nStanding Desk\n325.50\nWorkspace Solutions\nGermany\n[PREDICT]\n\n\n2\n1003\nWorkstation\n1450.00\nEnterprise Systems Ltd\nCanada\nEnterprise Solutions\n\n\n3\n1004\nLaptop Pro\n1899.99\nBusiness Corp\nUK\nEnterprise Solutions\n\n\n4\n1005\nGaming Laptop\n1250.00\nDigital Ventures\nUSA\nEnterprise Solutions\n\n\n5\n1006\nSmart Watch\n299.99\nGadget Store\nAustralia\nConsumer Electronics\n\n\n6\n1007\nErgonomic Chair\n445.00\nOffice Outfitters\nFrance\nOffice Furniture\n\n\n7\n1008\nStorage Array\n3500.00\nCloudTech Systems\nSingapore\nData Infrastructure\n\n\n8\n1009\nNetwork Switch\n175.50\nConnectIT\nJapan\nNetworking Devices"
  },
  {
    "objectID": "posts/2025-12-17-sap-rpt-1-aicore/index.html#conclusion",
    "href": "posts/2025-12-17-sap-rpt-1-aicore/index.html#conclusion",
    "title": "Building an SAP-RPT-1 ‘Hello World’ with AI Core",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve successfully transitioned from using SAP-RPT-1 in the playground to deploying and using it via AI Core. This setup allows you to use RPT-1 in a scalable, production-like environment. On the playground, for example, you quickly hit limits if you try more complex scenarios or larger datasets. Using a deployment via AI Core overcomes these limitations.\nI already have quite a few ideas for building more complex applications using RPT-1 and AI Core. Which scenarios will you build first? Let me know in the comments!"
  },
  {
    "objectID": "posts/2024-07-30-llm-calculator1/index.html",
    "href": "posts/2024-07-30-llm-calculator1/index.html",
    "title": "How to Turn GPT into a Calculator",
    "section": "",
    "text": "Large Language Models (LLMs) are great writers, but they struggle with numbers: Counting, adding, and basic arithmetic. I guess this is why they are called Large Language Models and not Large Math Models 😉. In this blog post, let’s explore why LLMs struggle with math and how we can fix this. We will dive into the topic of function calling and successfully turn an LLM into a calculator."
  },
  {
    "objectID": "posts/2024-07-30-llm-calculator1/index.html#why-llms-struggle-with-math",
    "href": "posts/2024-07-30-llm-calculator1/index.html#why-llms-struggle-with-math",
    "title": "How to Turn GPT into a Calculator",
    "section": "Why LLMs struggle with math?",
    "text": "Why LLMs struggle with math?\nWhat is \\(6574 \\times 9132\\)? The answer is \\(60.033.768\\), and computers could solve this question easily in their infancy, yet modern LLMs struggle with this question. I asked GPT-3.5 and GPT-4o:\n\nGPT3.5 answered: “6574 multiplied by 9132 equals 60,088,968.” (incorrect)\nGTP-4o answered: “The product of 6574 and 9132 is 60,052,968.” (incorrect)\n\nBoth results are incorrect. Why is that? The reason is rooted in tokenization [1] and the fact that these numbers are treated like text. Easy tasks like \\(3 \\times 7\\) will most likely always be “calculated” correctly. But they are not really calculated, the LLM just learned the result from the training data. This is the same as when you learned the basic multiplication tables. Essentially, you do not calculate the numbers all the time, but you have learned that \\(3 \\times 7 = 21\\). For the LLM \\(21\\) is just the most likely next token after the input sequence \\(3 \\times 7\\). LLMs, therefore, are somewhat human 😉. Coming back to the original example, LLMs do not really calculate the multiplication of four-digit numbers, they rather estimate the result.\nLet me ask you a question: How would you solve the task of calculating \\(6574 \\times 9132\\)? Most of us would use a calculator, and this is exactly the tooling we need to give to the LLM. Andrej Karpathy has put a calculator on his vision of an LLM OS [2]. Since this is a fundamental tool, let’s build it!\n\n\n\n\nThe calculator as part of LLM OS\n\n\nHere is the plan of what we will cover in this blog post:\n\nFirst, we will build a simple chat client based on the OpenAI API.\nNext, we dive into the concept of function calling without implementing it in the chat client.\nAfterwards, we integrate function calling into the chat client.\nNext, we pause for a bit and take a deep breath recapping what we have done so far.\nThis recap will lead us to a research paper on the subject.\nFinally, we will assemble the whole calculator and\nWrap up this blog post.\n\nIf you like to interactively run this notebook, hop over to GitHub: Here is the Jupyter notebook version of this blog post."
  },
  {
    "objectID": "posts/2024-07-30-llm-calculator1/index.html#creating-a-basic-chat-client",
    "href": "posts/2024-07-30-llm-calculator1/index.html#creating-a-basic-chat-client",
    "title": "How to Turn GPT into a Calculator",
    "section": "Creating a Basic Chat Client",
    "text": "Creating a Basic Chat Client\nLet’s create a simple chat client so that we can run our experiments. For a start, here is the chat messages class from Building Chat for Jupyter Notebooks from Scratch:\n\n\nCode\nclass ChatMessages:\n\n    def __init__(self):\n        \"\"\"Initializes the Chat.\"\"\"\n        self._messages = []\n\n    def _append_message(self, role, content):\n        \"\"\"Appends a message with specified role and content to messages list.\"\"\"\n        self._messages.append({\"role\": role, \"content\": content})\n\n    def append_system_message(self, content):\n        \"\"\"Appends a system message with specified content to messages list.\"\"\"\n        self._append_message(\"system\", content)\n\n    def append_user_message(self, content):\n        \"\"\"Appends a user message with specified content to messages list.\"\"\"\n        self._append_message(\"user\", content)\n\n    def append_assistant_message(self, content):\n        \"\"\"Appends an assistant message with specified content to messages list.\"\"\"\n        self._append_message(\"assistant\", content)\n\n    def get_messages(self):\n        \"\"\"Returns a shallow copy of the messages list.\"\"\"\n        return self._messages[:]\n\n\nHere is the chat client which talks to the OpenAI API. For more details, please refer to my blog post How to call the OpenAI API from a Jupyter Notebook:\n\n\nCode\n#model_name = \"gpt-3.5-turbo\"\n#model_name = \"gpt-4o-mini\"\nmodel_name = \"gpt-4o\"\n\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv(\".env\")\n\nfrom openai import chat\n\nclass ChatClient:\n\n    def __init__(self, system_message):\n        \"\"\"Initializes the Chat with the system message.\"\"\"\n        self._chat_messages = ChatMessages()\n        self._chat_messages.append_system_message(system_message)\n\n    def ask_gpt(self, prompt):\n        \"\"\"Calls the LLM chat completion API and returns the response message\"\"\"\n        self._chat_messages.append_user_message(prompt)\n\n        c = chat.completions.create(\n            model=model_name,\n            messages=self._chat_messages.get_messages())\n\n        self._chat_messages.append_assistant_message(c.choices[0].message.content)\n\n        return c.choices[0].message.content\n    \n\n\nLet’s do a quick test of our chat client:\n\nchat_client = ChatClient(\"Answer in a very concise and accurate way\")\nchat_client.ask_gpt(\"Name the planets in the solar system\")\n\n'Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune.'\n\n\n\nchat_client.ask_gpt(\"Reverse the list\")\n\n'Neptune, Uranus, Saturn, Jupiter, Mars, Earth, Venus, Mercury.'\n\n\nSo far so good, let’s transition to calculations:\n\nchat_client = ChatClient(\"You are a calculator.\")\nchat_client.ask_gpt(\"What is 6574 * 9132?\")\n\n'6574 * 9132 = 60075168.'\n\n\nAlmost correct 😉\n\n6574 * 9132\n\n60033768"
  },
  {
    "objectID": "posts/2024-07-30-llm-calculator1/index.html#introduction-to-function-calling",
    "href": "posts/2024-07-30-llm-calculator1/index.html#introduction-to-function-calling",
    "title": "How to Turn GPT into a Calculator",
    "section": "Introduction to Function Calling",
    "text": "Introduction to Function Calling\nHow can we teach the LLM some math? The secret is “function calling”. In the OpenAI API (and the APIs of other LLMs) we are can specify “tools” we can give to the LLM. Currently, only “functions” are supported. What sounds like a restriction is actually quite sufficient and convenient.\nFor now, Let’s teach ChatGPT how to properly multiply two numbers:\n\ndef multiply(a:int, b:int=1):\n    \"Multiplies a * b\"\n    return a * b\n\nmultiply(a=6574, b=9132)\n\n60033768\n\n\nTo be able to pass this function to ChatGPT, we need to pass the tool definition in the request as defined in the OpenAI API. Since writing out the JSON can be somewhat painful and repetitive, here is a function that extracts the JSON from a Python function. The original is from Jeremy Howard’s Hacker’s Guide [3]. In the meantime, the API has changed a bit, so there are a few updates in there.\n\n\nCode\nfrom pydantic import create_model\nimport inspect, json\nfrom inspect import Parameter\n\ndef get_schema(f):\n    kw = {n:(o.annotation, ... if o.default==Parameter.empty else o.default)\n          for n,o in inspect.signature(f).parameters.items()}\n    # update: schema -&gt; model_json_schema\n    s = create_model(f'Input for `{f.__name__}`', **kw).model_json_schema()\n    # update: added function level in tools json\n    function_params = dict(name=f.__name__, description=f.__doc__, parameters=s)\n    return dict(type=\"function\", function=function_params)\n\n[get_schema(multiply)]\n\n\n[{'type': 'function',\n  'function': {'name': 'multiply',\n   'description': 'Multiplies a * b',\n   'parameters': {'properties': {'a': {'title': 'A', 'type': 'integer'},\n     'b': {'default': 1, 'title': 'B', 'type': 'integer'}},\n    'required': ['a'],\n    'title': 'Input for `multiply`',\n    'type': 'object'}}}]\n\n\nLet’s pass the tool to ChatGPT and ask it to do the calculation. The response looks a bit different: Instead of text, ChatGPT returns a tool call object.\n\n\nCode\nfrom openai import chat\n\nchat_messages = ChatMessages()\nchat_messages.append_system_message(\"You are a calculator.\")\nchat_messages.append_user_message(\"What is 6574 * 9132?\")\n\nc = chat.completions.create(\n    model=model_name,\n    messages=chat_messages.get_messages(),\n    tools=[get_schema(multiply)])\n\nprint(f\"Text response: {c.choices[0].message.content}\")\nprint(f\"Tool call: {c.choices[0].message.tool_calls}\")\n\n\nText response: None\nTool call: [ChatCompletionMessageToolCall(id='call_qP9MhfCgWyjNZHWGyWNEjP2e', function=Function(arguments='{\"a\":6574,\"b\":9132}', name='multiply'), type='function')]\n\n\nNotice that ChatGPT does not (and cannot) perform the calculation directly (because it does not have direct access to the function), but it tells us to call function multiply with arguments='{\"a\":6574,\"b\":9132}.\nRe-using another function from Jeremy Howard’s Hacker’s Guide [3], let’s call the function.\n\n\nCode\nfuncs_ok = {'multiply'}\n\ndef call_func(c):\n    \"\"\"Calls a function based on LLM tool calls\"\"\"\n    fc = c.choices[0].message.tool_calls[0].function #Updated\n    if fc.name not in funcs_ok: return print(f'Not allowed: {fc.name}')\n    f = globals()[fc.name]\n    return f(**json.loads(fc.arguments))\n\n\n\ncall_func(c)\n\n60033768\n\n\nNot surprisingly, this result is correct, but we are still missing an essential piece: We need to send back the result to ChatGPT so that we can continue chatting. Therefore, let’s integrate function calling into our chat client."
  },
  {
    "objectID": "posts/2024-07-30-llm-calculator1/index.html#adding-function-calling-to-chat-client",
    "href": "posts/2024-07-30-llm-calculator1/index.html#adding-function-calling-to-chat-client",
    "title": "How to Turn GPT into a Calculator",
    "section": "Adding Function Calling to Chat Client",
    "text": "Adding Function Calling to Chat Client\nThis section will be fast, adding in quite a bit of code, but do not worry, we will recap later to understand the underlying details.\nFirst, we need to be able to pass the tools to the chat client.\n\nfrom fastcore.utils import * #for importing patch\n\n@patch    \ndef __init__(self:ChatClient, system_message, tools=None):\n    \"\"\"Initializes the Chat with the system message.\"\"\"\n    self._chat_messages = ChatMessages()\n    self._chat_messages.append_system_message(system_message)\n    self._tools = tools\n\nAfter prompting ChatGPT with tools, it might return a tool call as we have seen. This tool call needs to be stored in the chat history. Therefore, we need to update method append_assistant_message:\n\n@patch \ndef append_assistant_message(self:ChatMessages, content=None, tool_calls=None):\n    \"\"\"Appends an assistant message with specified content to messages list.\"\"\"\n    if content:\n        self._append_message(\"assistant\", content)\n    else:\n        self._messages.append({\"role\": \"assistant\", \"tool_calls\": tool_calls})\n\nFinally, we need to update the ask_gpt method so that we can store the new format of the assistant message:\n\n@patch\ndef get_model_response(self:ChatClient):\n    \"\"\"Calls the LLM chat completion API\"\"\"\n    return chat.completions.create(\n        model=model_name,\n        messages=self._chat_messages.get_messages(),\n        tools=self._tools)\n\n\n@patch\ndef ask_gpt(self:ChatClient, prompt):\n    \"\"\"Calls the LLM chat completion API and returns the response message\"\"\"\n    self._chat_messages.append_user_message(prompt)\n\n    c = self.get_model_response()\n\n    self._chat_messages.append_assistant_message(\n        content=c.choices[0].message.content,\n        tool_calls=c.choices[0].message.tool_calls)\n\n    return c.choices[0].message\n\nSo far, so good. When we run the calculation again, we can observe that ChatGPT does not return a normal chat message, but it returns a tool call:\n\nchat_client = ChatClient(\"You are a calculator.\", tools=[get_schema(multiply)])\nmodel_response = chat_client.ask_gpt(\"What is 6574 * 9132?\")\nprint(model_response)\n\nChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_S1NinsciYzZtnHGDUKDuZ9UK', function=Function(arguments='{\"a\":6574,\"b\":9132}', name='multiply'), type='function')])\n\n\nLet’s add in functionality to call the tool.\n\n@patch\ndef call_tool(self:ChatClient, tool_call):\n    \"\"\"returns the result of an LLM tool call\"\"\"\n    fc = tool_call.function #Updated\n    if fc.name not in funcs_ok: return print(f'Not allowed: {fc.name}')\n    f = globals()[fc.name]\n    return f(**json.loads(fc.arguments))\n\nThe result of the tool call needs to be stored in a tool-message, so let’s add the append_tool_message-method to the chat message class.\n\n@patch \ndef append_tool_message(self:ChatMessages, content, tool_call_id):\n    \"\"\"Appends a tool message with specified content to messages list.\"\"\"\n    self._messages.append({\"role\": \"tool\", \"content\": content, \"tool_call_id\": tool_call_id})\n\nActually, ChatGPT might return more than one tool - note that the tools are not an object, but an array. The call_tools method processes tools by appending the tool message to the chat. Each of these tool messages contains the tool result as content and the tool_call_id. Once all tools are processed, we call ChatGPT again.\n\n@patch\ndef call_tools(self:ChatClient, tool_calls):\n    \"\"\"Processes the tool calls of the LLM response and calls the LLM API again\"\"\"\n    for tool_call in tool_calls:\n        chat_client._chat_messages.append_tool_message(\n            content=str(self.call_tool(tool_call)),\n            tool_call_id=tool_call.id)\n        \n    self.ask_gpt()\n\nCalling ChatGPT directly after the tool calls might look strange. The reason for doing this is code simplicity. This way we can re-write the ask_gpt-method to recursively process tools until all tools are processed. Why this is a good idea will become obvious a little later.\n\n@patch \ndef get_last_assistant_message(self:ChatMessages):\n    \"\"\"Returns the content of the last assistant message\"\"\"\n    return self._messages[-1]['content']\n\n\nfrom IPython.display import display, Markdown\n\n@patch\ndef ask_gpt(self:ChatClient, prompt=None):\n    \"\"\"Calls the LLM chat completion API and returns the response message\"\"\"\n    \n    if prompt:\n        self._chat_messages.append_user_message(prompt)\n\n    c = self.get_model_response()\n    content = c.choices[0].message.content\n    tool_calls = c.choices[0].message.tool_calls\n\n    self._chat_messages.append_assistant_message(\n        content=content,\n        tool_calls=tool_calls)\n    \n    if tool_calls:\n        self.call_tools(tool_calls)\n\n    return Markdown(self._chat_messages.get_last_assistant_message())\n\nThis was a lot of code. The reward is that we can now run multiplication in the chat.\n\nsystem_prompt = \"You are a calculator. Respond in Markdown, no LaTeX\"\nchat_client = ChatClient(system_message=system_prompt, tools=[get_schema(multiply)])\nchat_client.ask_gpt(\"What is 6574 * 9132?\")\n\n6574 * 9132 = 60,033,768"
  },
  {
    "objectID": "posts/2024-07-30-llm-calculator1/index.html#recap-function-calling-in-chat-client",
    "href": "posts/2024-07-30-llm-calculator1/index.html#recap-function-calling-in-chat-client",
    "title": "How to Turn GPT into a Calculator",
    "section": "Recap: Function Calling in Chat Client",
    "text": "Recap: Function Calling in Chat Client\nSince the previous section was very fast, let’s review what actually happened when multiply \\(6574 \\times 9132\\) by inspecting the chat messages which were sent. Stating the obvious: More messages have been exchanged than what the chat client showed.\n\n\nCode\n@patch \ndef get_debug_view(self: ChatMessages):\n    \"\"\"Returns the debug view of the chat messages formatted as Markdown.\"\"\"\n    debug_view = []\n    for message in self._messages:\n        role = message.get('role')\n        content = message.get('content', '')\n\n        if role == 'system' or role == 'user':\n            debug_view.append(f\"**{role}**: {content}\\n\")\n\n        elif role == 'assistant':\n            if 'tool_calls' in message:\n                debug_view.append(\"**tool calls**\\n\")\n                for i, tool_call in enumerate(message['tool_calls'], start=1):\n                    function_name = tool_call.function.name\n                    arguments = tool_call.function.arguments\n                    tool_call_id = tool_call.id\n                    debug_view.append(f\"{i}. tool: {function_name}: {arguments} (tool call id: {tool_call_id})\\n\")\n            else:\n                debug_view.append(f\"**assistant**: {content}\\n\")\n\n        elif role == 'tool':\n            tool_call_id = message.get('tool_call_id', '')\n            debug_view.append(f\"**tool result**: {content} (tool call id: {tool_call_id})\\n\")\n\n    return Markdown('\\n'.join(debug_view))\n\n\n\nchat_client._chat_messages.get_debug_view()\n\nsystem: You are a calculator. Respond in Markdown, no LaTeX\nuser: What is 6574 * 9132?\ntool calls\n\ntool: multiply: {“a”:6574,“b”:9132} (tool call id: call_7Jh50u0EyZzPhhxwjgCmV7c4)\n\ntool result: 60033768 (tool call id: call_7Jh50u0EyZzPhhxwjgCmV7c4)\nassistant: 6574 * 9132 = 60,033,768\n\n\nBased on the system prompt (“You are a calculator. Respond in Markdown, no LaTeX”) and the user prompt (“What is 6574 * 9132?”), ChatGPT realized that it should not attempt the calculation on its own, but it should call a tool. Hence, it returned a tool call to multiply with arguments='{\"a\":6574,\"b\":9132}. We called the function and returned the result (“60033768”) as a tool-message. As a result, ChatGPT returned the final assistant message (“The result of 6574 * 9132 is 60,033,768.”). In the chat client, we “suppressed” the tool call messages and only displayed the final assistant-message.\nDo you remember that we implemented a recursive tool call? The reason for this is to handle more complex calculations which require more steps, for example this one:\n\n6573 * 9132 * 5423\n\n325513601028\n\n\n\nchat_client = ChatClient(system_message=system_prompt, tools=[get_schema(multiply)])\nchat_client.ask_gpt(\"What is ( 6573 * 9132 ) * 5423?\") #The additional brackets reduce the number is tool calls\n\nThe values are:\n6573 * 9132 = 60024636\n9132 * 5423 = 49522836\nSo, (6573 * 9132) * 5423 = 60024636\n\n\n\nchat_client._chat_messages.get_debug_view()\n\nsystem: You are a calculator. Respond in Markdown, no LaTeX\nuser: What is ( 6573 * 9132 ) * 5423?\ntool calls\n\ntool: multiply: {“a”: 6573, “b”: 9132} (tool call id: call_qZQr43OS0K17sJmweeqpp7LZ)\ntool: multiply: {“a”: 9132, “b”: 5423} (tool call id: call_HdvwojOVjKGj8vFFo6ciBuFk)\n\ntool result: 60024636 (tool call id: call_qZQr43OS0K17sJmweeqpp7LZ)\ntool result: 49522836 (tool call id: call_HdvwojOVjKGj8vFFo6ciBuFk)\nassistant: The values are:\n6573 * 9132 = 60024636\n9132 * 5423 = 49522836\nSo, (6573 * 9132) * 5423 = 60024636\n\n\nTo solve \\(6573 \\times 9132 \\times 5423\\), ChatGPT needed to return 2 sets of tool calls: First it calculates \\(6573 \\times 9132\\), and, strangely, it also calculates \\(5423 \\times 1\\). In the second tool call it calculates \\(60024636 \\times 5423\\). It is really interesting to observe how ChatGPT iteratively solved the problem before returning the final assistant message.\nTaking a step back, this iterative way of calling tools is called “ReAct”. Let’s explore the theoretical foundations."
  },
  {
    "objectID": "posts/2024-07-30-llm-calculator1/index.html#the-react-paper",
    "href": "posts/2024-07-30-llm-calculator1/index.html#the-react-paper",
    "title": "How to Turn GPT into a Calculator",
    "section": "The ReAct Paper",
    "text": "The ReAct Paper\nThe ReAct paper [4] introduced a framework that combines Reasoning and Acting (“ReAct”) to enhance the capabilities of LLMs. The core idea of the ReAct framework is to run reasoning and acting in a cyclical process. This means that the LLM does not simply produce an output based on an input, but rather it reasons about the task, determines the actions needed, performs these actions, and incorporates the results into its reasoning process. This loop continues until the task is completed.\nToday’s LLMs are remarkably intelligent and can reason about the prompts they are tasked to perform. Function calling gives the LLM the capabilities to act and perform specific actions. In our setup, the tools are the arithmetical operations, and the LLM needs to reason about the sequence to run the calculations. These reasoning capabilities and the knowledge about how to perform the task are knowledge the LLM has acquired during its training phase. It knows that multiplication or division needs to be done before addition or subtraction. It also understands how brackets signal the sequence of calculations. For example, when calculating \\((6573 + 1) \\times 9132\\), the LLM must first reason that it needs to perform addition, then multiplication. Based on this reasoning, it acts by calling the respective tools in the correct sequence.\nThe following chart summarizes this framework based on what we have seen so far.\n\n\n\n\n\nsequenceDiagram\n    autonumber\n\n    actor User\n    participant Function\n    participant ChatClient as Chat Client\n    participant LLM\n\n    User-&gt;&gt;Function: Define the function\n    Function-&gt;&gt;User: Retrieve JSON function definition (via get_schema)\n    User-&gt;&gt;ChatClient: Create Chat Client including tool(s)\n    User-&gt;&gt;ChatClient: Send prompt (via ask_gpt)\n    ChatClient-&gt;&gt;LLM: Send prompt with tool(s)\n\n    loop Reasoning and Acting\n        LLM-&gt;&gt;LLM: Reasoning: Analyze prompt and tools\n        LLM-&gt;&gt;ChatClient: Acting: Generate tool call(s)\n        ChatClient-&gt;&gt;Function: Call the function(s) (via call_tools / call_tool)\n        Function-&gt;&gt;ChatClient: Return result(s)\n        ChatClient-&gt;&gt;LLM: Acting: Pass on result(s)\n        LLM-&gt;&gt;LLM: Reasoning: Incorporate result(s) and continue reasoning\n    end\n\n    LLM-&gt;&gt;ChatClient: Return final result\n    ChatClient-&gt;&gt;User: Output final result\n\n\n\n\n\n\n\nWe implemented the ReAct framework not only by providing the tools to the LLM but also by allowing recursive processing of tool calls in our chat client. This way, the chat client can handle multiple tool calls it receives from the LLM, ensuring that the LLM can continue reasoning and acting until the task is complete."
  },
  {
    "objectID": "posts/2024-07-30-llm-calculator1/index.html#creating-the-calculator",
    "href": "posts/2024-07-30-llm-calculator1/index.html#creating-the-calculator",
    "title": "How to Turn GPT into a Calculator",
    "section": "Creating the Calculator",
    "text": "Creating the Calculator\nNow that we know how function calling works, and we understand how the LLM uses tools by reasoning and acting, it is time to create the full calculator.\nThe only thing we need to do is to define more functions the LLM can use as tools.\n\ndef add(a: float, b: float = 1.0):\n    \"Adds a + b\"\n    return a + b\n\ndef subtract(a: float, b: float = 1.0):\n    \"Subtracts a - b\"\n    return a - b\n\ndef multiply(a: float, b: float = 1.0):\n    \"Multiplies a * b\"\n    return a * b\n\ndef divide(a: float, b: float = 1.0):\n    \"Divides a / b\"\n    if b == 0:\n        return \"Division by zero is not allowed.\"\n    return a / b\n\nfuncs_ok = {'add', 'subtract', 'multiply', 'divide'}\n\ndef get_calc_tools():\n    return [get_schema(add), get_schema(subtract), get_schema(multiply), get_schema(divide)]\n\nNote the intersting definition of the divide-function. Traditionally, you would expect a definition like this:\ndef divide(a: float, b: float = 1.0) -&gt; float:\n    \"Divides a / b\"\n    if b == 0:\n        raise ValueError(\"Division by zero is not allowed.\")\n    return a / b\nIn the context of LLM function calling, however, raising a ValueError is not very useful, because the LLM needs to receive the result in the tool-message. Therefore, returning a string \"Division by zero is not allowed.\" is more useful.\nLet’s run a first test: \\((6573 + 1) \\times 9132\\)\n\n\nCode\nsystem_prompt = (\n    \"You are a calculator. \\n\"\n    \"Do not do even the simplest computations on your own, \\n\"\n    \"but use the tools provided. \\n\"\n    \"After the tool calls, explain the steps you took when answering. \\n\"\n    \"Answer with an accuracy of 3 decimals. \\n\"\n    \"Respond in markdown, no LaTeX.\"\n)\n\n\n\n\nCode\nprint(f\"Expected result: {(6573 + 1) * 9132}\")\nprint(f\"LLM response:\")\nchat_client = ChatClient(system_message=system_prompt, tools=get_calc_tools())\nchat_client.ask_gpt(\"What is (6573 + 1) * 9132?\")\n\n\nExpected result: 60033768\nLLM response:\n\n\nThe calculation steps are as follows: 1. First, add 6573 and 1 to get 6574. 2. Then, multiply 6574 by 9132 to get 60,033,768.\nSo, (6573 + 1) * 9132 = 60,033,768.\n\n\nFor a more detailed we, we can inspect the chat messages:\n\nchat_client._chat_messages.get_debug_view()\n\nsystem: You are a calculator. Do not do even the simplest computations on your own, but use the tools provided. After the tool calls, explain the steps you took when answering. Answer with an accuracy of 3 decimals. Respond in markdown, no LaTeX.\nuser: What is (6573 + 1) * 9132?\ntool calls\n\ntool: add: {“a”: 6573, “b”: 1} (tool call id: call_et4e3TZcl14w920hmanHPuTM)\ntool: multiply: {“a”: 6574, “b”: 9132} (tool call id: call_ZnP6lnKaXlANjz9SDjfbVvjs)\n\ntool result: 6574 (tool call id: call_et4e3TZcl14w920hmanHPuTM)\ntool result: 60033768 (tool call id: call_ZnP6lnKaXlANjz9SDjfbVvjs)\nassistant: The calculation steps are as follows: 1. First, add 6573 and 1 to get 6574. 2. Then, multiply 6574 by 9132 to get 60,033,768.\nSo, (6573 + 1) * 9132 = 60,033,768.\n\n\nLet’s do a final, more complicated example: \\((( 5647 + 3241 ) / ( 7 \\times 2 )) - 1\\)\n\n\nCode\nprint(f\"Expected result: {( ( 5647 + 3241 ) / ( 7 * 2 ) ) - 1}\")\nprint(f\"LLM response:\")\nchat_client = ChatClient(system_message=system_prompt, tools=get_calc_tools())\nchat_client.ask_gpt(\"What is ( ( 5647 + 3241 ) / ( 7 * 2 ) ) - 1?\")\n\n\nExpected result: 633.8571428571429\nLLM response:\n\n\nThe result of the expression (( ( 5647 + 3241 ) / ( 7 * 2 ) ) - 1) is (633.857).\nSteps taken: 1. Added (5647 + 3241) to get (8888). 2. Multiplied (7 * 2) to get (14). 3. Divided (8888 / 14) to get (634.857). 4. Subtracted (1) from (634.857) to get (633.857).\n\n\n\nchat_client._chat_messages.get_debug_view()\n\nsystem: You are a calculator. Do not do even the simplest computations on your own, but use the tools provided. After the tool calls, explain the steps you took when answering. Answer with an accuracy of 3 decimals. Respond in markdown, no LaTeX.\nuser: What is ( ( 5647 + 3241 ) / ( 7 * 2 ) ) - 1?\ntool calls\n\ntool: add: {“a”: 5647, “b”: 3241} (tool call id: call_eIvBYclF1XReEpx8zxnoQGjU)\ntool: multiply: {“a”: 7, “b”: 2} (tool call id: call_MWiGRfO1cpv7smie1rhXeCm2)\n\ntool result: 8888 (tool call id: call_eIvBYclF1XReEpx8zxnoQGjU)\ntool result: 14 (tool call id: call_MWiGRfO1cpv7smie1rhXeCm2)\ntool calls\n\ntool: divide: {“a”:8888,“b”:14} (tool call id: call_s1tal77M3YGYrsud0Y0pLX1f)\n\ntool result: 634.8571428571429 (tool call id: call_s1tal77M3YGYrsud0Y0pLX1f)\ntool calls\n\ntool: subtract: {“a”:634.857,“b”:1} (tool call id: call_cGZ15kGFQ20ztFZqiUve3GOS)\n\ntool result: 633.857 (tool call id: call_cGZ15kGFQ20ztFZqiUve3GOS)\nassistant: The result of the expression (( ( 5647 + 3241 ) / ( 7 * 2 ) ) - 1) is (633.857).\nSteps taken: 1. Added (5647 + 3241) to get (8888). 2. Multiplied (7 * 2) to get (14). 3. Divided (8888 / 14) to get (634.857). 4. Subtracted (1) from (634.857) to get (633.857)."
  },
  {
    "objectID": "posts/2024-07-30-llm-calculator1/index.html#what-are-the-models-capable-of",
    "href": "posts/2024-07-30-llm-calculator1/index.html#what-are-the-models-capable-of",
    "title": "How to Turn GPT into a Calculator",
    "section": "What are the models capable of?",
    "text": "What are the models capable of?\nBefore closing this blog post, we need to discuss which model can safely be used for the calculator job. After all, we put a lot of trust into the model do get the job done. If the model would not run the tools in the right sequence, the result would be incorrect. So let’s create a mini-series of tests:\n\n\nCode\nmodels = [\"gpt-3.5-turbo\", \"gpt-4o-mini\", \"gpt-4o\"]\ncalculations = [\n    \"( 6574 * 9132 )\",\n    \"( 6573 + 9132 ) * 5423\",\n    \"( 6573 * 9132 ) * 5423\",\n    \"( 5647 + 3241 ) / ( 7 * 2 ) - 1 \",\n    \"( 5647 + 3241 ) / ( 7 * 2 ) + ( ( 657 + 343 ) * 2 )\" ]\n\nresults = []\n\nfor calculation in calculations:\n    try:\n        result = eval(calculation)\n        results.append(result)\n    except Exception as e:\n        results.append(f\"Error in calculation: {e}\")\n\nfor calc, res in zip(calculations, results):\n    print(f\"{calc} = {res:.3f}\")\n\n\n( 6574 * 9132 ) = 60033768.000\n( 6573 + 9132 ) * 5423 = 85168215.000\n( 6573 * 9132 ) * 5423 = 325513601028.000\n( 5647 + 3241 ) / ( 7 * 2 ) - 1  = 633.857\n( 5647 + 3241 ) / ( 7 * 2 ) + ( ( 657 + 343 ) * 2 ) = 2634.857\n\n\n\n\nCode\nimport pandas as pd\n\nsystem_prompt = (\n    \"You are a calculator. \\n\"\n    \"Do not do even the simplest computations on your own, \\n\"\n    \"but use the tools provided. \\n\"\n    \"Answer with an accuracy of 3 decimals. \\n\"\n    \"Strictly only respond only with the result of the calculation, just one number, no additional text.\"\n)\n\n# Store results in a dictionary\nresults = {calc: {} for calc in calculations}\n\n# Run calculations for each model\nfor model in models:\n    model_name = model\n    for calculation in calculations:\n        chat_client = ChatClient(system_message=system_prompt, tools=get_calc_tools())\n        result = chat_client.ask_gpt(calculation)\n        result_number = float(result.data)\n        actual_result = eval(calculation)\n        results[calculation][model] = abs(result_number - actual_result) &lt; 0.001\n\n# Create a dataframe for the results\ndf_results = pd.DataFrame(results).T\ndf_results.columns = models\ndf_results.index.name = 'Calculation'\n\nfrom IPython.display import HTML\n\n# Style the DataFrame for better visualization\ndef color_true_false(val):\n    color = 'green' if val else 'red'\n    return f'color: {color}; background-color: white'\n\nstyled_df = df_results.style \\\n    .map(lambda x: 'color: green; background-color: white' if x else 'color: red; background-color: white') \\\n    .set_table_styles([\n        {'selector': 'th', 'props': [('background-color', 'white'), ('color', 'black'), ('border', '1px solid black'), ('text-align', 'center'), ('vertical-align', 'middle')]},\n        {'selector': 'td', 'props': [('border', '1px solid black')]},\n        {'selector': 'caption', 'props': [('caption-side', 'top'), ('font-size', '1.25em'), ('font-weight', 'bold')]}\n    ]) \\\n    .set_caption(\"Model Comparison Results\") \\\n    .set_properties(**{'text-align': 'center', 'vertical-align': 'middle'})\n\nhtml = styled_df.to_html()\ndisplay(HTML(html))\n\n\n\n\n\n\n\nTable 1: Model Comparison Results\n\n\n\n\n\n \ngpt-3.5-turbo\ngpt-4o-mini\ngpt-4o\n\n\nCalculation\n \n \n \n\n\n\n\n( 6574 * 9132 )\nTrue\nTrue\nTrue\n\n\n( 6573 + 9132 ) * 5423\nTrue\nTrue\nTrue\n\n\n( 6573 * 9132 ) * 5423\nTrue\nFalse\nTrue\n\n\n( 5647 + 3241 ) / ( 7 * 2 ) - 1\nTrue\nTrue\nTrue\n\n\n( 5647 + 3241 ) / ( 7 * 2 ) + ( ( 657 + 343 ) * 2 )\nFalse\nTrue\nTrue\n\n\n\n\n\n\n\n\nWhen you run the test several times, you can notice that the results vary. Mostly the models reason correctly. Some calculations, however, seem to be difficult. The incorrect results 99% of the time only show up in the left columns. GPT-4o feels like a reliable partner."
  },
  {
    "objectID": "posts/2024-07-30-llm-calculator1/index.html#conclusion",
    "href": "posts/2024-07-30-llm-calculator1/index.html#conclusion",
    "title": "How to Turn GPT into a Calculator",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we successfully turned an LLM into a reliable calculator. While this exercise might seem somewhat academic (every real calculator is faster by several orders of magnitude), it provides valuable insights into enhancing LLM capabilities through tool integration.\nWe learned how to provide tools to an LLM to perform reliable calculations, essentially grounding the LLM in math. We achieved this by giving the LLM access to Python functions for arithmetic operations. Our approach was very generic, using the get_schema function to extract the definitions of the Python functions and processing the tools with the call_tools-method. Beyond the scope of the calculator, using this approach, we could expose any function to the LLM, such as sending emails or accessing databases.\nWe also explored the ReAct framework, transforming the LLM into an agent that combines reasoning and acting. This significantly enhanced its capabilities. The LLM reasoned about the calculations we asked it to do, and it acted by running the corresponding Python functions until it calculated the final result.\nImplementing this calculator is fundamentally different from traditional approaches because we did not code any mathematical logic ourselves. We only defined the basic mathematical functions, exposed them to the LLM, and trusted its intelligence to perform the calculations. This required a leap of faith, and we saw that not all models consistently get the calculations right. Using an advanced model like GPT-4o, however, demonstrates the potential of LLMs to handle complex operations independently.\nAs the intelligence of future models likely increases significantly, it will become an essential skill to judge the level of faith we can safely place in an LLM. Understanding which tasks an LLM can perform without specific guidance and where we need either human support or classical code-based solutions is essential. Making accurate assessments will not only allow us to push the boundaries of what is possible with LLMs but can also lead to significant reductions in the complexity of traditional IT systems. However, pushing these boundaries must be done responsibly, ensuring that we balance innovation with safety and reliability."
  },
  {
    "objectID": "posts/2024-07-30-llm-calculator1/index.html#references",
    "href": "posts/2024-07-30-llm-calculator1/index.html#references",
    "title": "How to Turn GPT into a Calculator",
    "section": "References",
    "text": "References\n[1] Karpathy, A. (2024). Let’s build the GPT Tokenizer by Andrej Karpathy\n[2] Karpathy, A. (2023). Intro to Large Language Models\n[3] Howard, J. (2023). A Hackers’ Guide to Language Models\n[4] Yao, S., Yu, T., Wu, Y., Zhao, Z., Yu, K., & Liu, S. (2022). ReAct: Synergizing Reasoning and Acting in Language Models"
  },
  {
    "objectID": "posts/2025-07-09-deploying-fastapi-app-on-render/index.html",
    "href": "posts/2025-07-09-deploying-fastapi-app-on-render/index.html",
    "title": "Deploying a FastAPI App to the Cloud",
    "section": "",
    "text": "In my previous blog post, we created a simple FastAPI-based greeting service with both GET and POST endpoints, an HTML frontend, and interactive documentation. In this follow-up blog post we’ll deploy it to the cloud so it can be accessed publicly using Render. I have no affiliation with them, but chose their platform because they offer a generous free tier.\nYou’ll learn how to:\nIn short, we’ll take the FastAPI greeting service from “it works on my machine” to “live on the internet.” 🤓"
  },
  {
    "objectID": "posts/2025-07-09-deploying-fastapi-app-on-render/index.html#creating-a-local-deployment",
    "href": "posts/2025-07-09-deploying-fastapi-app-on-render/index.html#creating-a-local-deployment",
    "title": "Deploying a FastAPI App to the Cloud",
    "section": "Creating a Local Deployment",
    "text": "Creating a Local Deployment\nOnce you’re done developing your FastAPI app in a Jupyter Notebook, you can prepare it for local deployment by following these steps:\n\nCreating app.py\nTo create the app.py file, follow these steps\n\nCopy all the code from the final cell in the Jupyter Notebook and paste it into a new Python file named app.py.\nRemove notebook-specific dependencies. Anything related to nest_asyncio is no longer needed, it is only necessary in Jupyter notebooks. Additionally, remove the uvicorn.run(...) call, as the server will be launched outside app.py.\n\n\n\nCreating requirements.txt\nFor out simple project, you could easily create the requirements.txt file manually, but I am too lazy for that, so I am using the pipreqs package to generate it for me\nIf you don’t have the pipreqs package installed, you can simply install it using pip:\npip install pipreqs\nNow you can generate the requirements.txt. Either cd into the directory where your app.py file is located and run\npipreqs .\nAlternatively, you can specify the path to the app.py file\npipreqs path/to/app.py\n\n\nInstalling Dependencies\nOnce you have the requirements.txt file, you can install the dependencies using pip\npip install -r requirements.txt\nWhen you are developing your own app, and you started in a Jupyter notebook, the installation of the dependencies is a self-fulfilling prophecy, but once we move to a virtual environment, you need to install the dependencies.\n\n\nRunning the App\nNow you can run your app with Uvicorn from the terminal. From the directory where app.py is located, start the API server with:\nuvicorn app:app --reload\nLet’s break down the command:\n\nuvicorn: The Uvicorn server.\napp:app: This follows the format :: It tells Uvicorn to look for the FastAPI app instance named app inside the Python module app.py. Since both arguments are the same, let’s change it for clarity. If you had written greeting_app = FastAPI() inside a file called api_app.py, you would run uvicorn api_app:greeting_app --reload\n--reload: Automatically restarts the server whenever you make code changes — useful during development."
  },
  {
    "objectID": "posts/2025-07-09-deploying-fastapi-app-on-render/index.html#running-the-app-in-a-virtual-environment",
    "href": "posts/2025-07-09-deploying-fastapi-app-on-render/index.html#running-the-app-in-a-virtual-environment",
    "title": "Deploying a FastAPI App to the Cloud",
    "section": "Running the App in a Virtual Environment",
    "text": "Running the App in a Virtual Environment\nSo far, we have re-created the behavior of the Jupyter Notebook in a Python file. However, when we deploy the app in the cloud, we will have a different environment. To make sure that we defined the dependencies correctly and the app runs as expected, we should test it in a virtual environment.\n\nCreating a Virtual Environment\nTo create a virtual environment, you can use the venv module that comes with Python. Open your terminal and navigate to the directory where your app.py file is located. Then, run the following command to create a virtual environment named venv:\npython -m venv venv\nLet’s break down the command:\n\npython: The Python interpreter.\n-m venv: The venv module, which is used to create virtual environments.\nvenv: The name of the virtual environment.\n\n\n\nActivating the Virtual Environment\nOnce the virtual environment is created, you need to activate it. The activation command depends on your operating system:\n\nWindows: venv\\Scripts\\activate\nmacOS/Linux: source venv/bin/activate\n\nAs a result, you should see the name of the virtual environment (venv) as a prefix in your terminal.\n\n\nInstalling Dependencies\nNow that you have activated the virtual environment, you can install the dependencies using pip. From the directory where app.py is located, run:\npip install -r requirements.txt\nNow the virtual environment has all the dependencies from requirements.txt installed, simulating the environment in which the app will be deployed in the cloud.\n\n\nRunning the App\nNow you can run your app with Uvicorn from the terminal. From the directory where app.py is located, start the API server with:\nuvicorn app:app --reload\n\n\nDeactivating the Virtual Environment\nWhen you are done testing, you can deactivate the virtual environment by running:\ndeactivate"
  },
  {
    "objectID": "posts/2025-07-09-deploying-fastapi-app-on-render/index.html#deploying-the-api-in-the-cloud",
    "href": "posts/2025-07-09-deploying-fastapi-app-on-render/index.html#deploying-the-api-in-the-cloud",
    "title": "Deploying a FastAPI App to the Cloud",
    "section": "Deploying the API in the Cloud",
    "text": "Deploying the API in the Cloud\nAfter we have successfully tested our API locally, it is time deploy it in the cloud. I will use Render to deploy my API, because they have a free tier that allows me to deploy the API for free.\nTo clearly separate the local development from the cloud deployment, I created a new folder called app-render and copied the app.py and the requirements.txt file into it.\nAfter uploading everything to my GitHub repository, I created a new service in Render and linked it to my GitHub repository. During service creation, set the following parameters:\n\nService Type: Web Service\nLanguage: Python 3\nBranch: main\nRoot Directory: app-render\nBuild Command: pip install -r requirements.txt\nStart Command: uvicorn app:app --host 0.0.0.0 --port $PORT\nInstance Type: Free\n\nAfter deployment, this is the URL of the API: https://fastapi-greeting-api.onrender.com\nYou can now call the API using the client notebooks for the POST method and the GET method"
  },
  {
    "objectID": "posts/2025-07-09-deploying-fastapi-app-on-render/index.html#conclusion",
    "href": "posts/2025-07-09-deploying-fastapi-app-on-render/index.html#conclusion",
    "title": "Deploying a FastAPI App to the Cloud",
    "section": "Conclusion",
    "text": "Conclusion\nTaking the API from a Jupyter Notebook (my preferred environment for rapid prototyping) to a working cloud deployment took a few steps. First, we refactored the code into a standalone Python file (app.py). Then we created a requirements.txt file and tested everything locally in a clean virtual environment. Finally, we deployed the API to the cloud using Render. While the deployment step will vary across cloud providers, the general process is similar.\nAfter building the greeting API in my previous blog post, we’ve now taken it all the way to the cloud. The goal here wasn’t to explore every FastAPI feature or every cloud deployment nuance, but to provide a reusable, step-by-step template for future projects and hackathons.\nThere’s lots more you could do, e.g. adding authentication, rate limiting, or monitoring, but we’ll leave that for another time. I Hope this was helpful, and happy coding!"
  },
  {
    "objectID": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html",
    "href": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html",
    "title": "Installing Fast.AI on Apple Silicon",
    "section": "",
    "text": "I recently moved from Windows to Mac. One of the things which needed attention in making that shift was to re-install everything around Fast.AI.\nThis blog post is mainly a personal documentation going through Live Coding Session 1 and Live Coding Session 2 (again), but I hope this might also be useful to others."
  },
  {
    "objectID": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html#starting-with-a-blank-mac",
    "href": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html#starting-with-a-blank-mac",
    "title": "Installing Fast.AI on Apple Silicon",
    "section": "Starting with a blank Mac",
    "text": "Starting with a blank Mac\nBefore we can talk about all the cool machine learning stuff, we need to start with some basics like installing some basic software on the Mac. So if you are already familiar with MacOS you might want to skip this part.\n\nThe Terminal instead of Ubuntu\nIt may sound trivial, but let’s state the (maybe no so) obvious: Instead of the Windows Subsystem for Linux (WSL), you use the Terminal App on a Mac. Navigation works the same, but there are some fine details to note.\nWhen I followed the tutorials of the Live Coding Sessions for Fast.AI (Link), I needed to use wget, but it did not exist on my Mac, because it was not installed. No problem, but how do you install software via the Terminal on a Mac?\n\n\nHomebrew\nWhen it comes to installing software on a Mac via the terminal, it seems that Homebrew is the de-facto standard. To install it, run the following command in the terminal:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nAfterwards, Homebrew needs to be added to the PATH environment variable: (exchange your username)\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; /Users/chrwittm/.zprofile\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nIf everything was successful, you should be able to execute\nbrew --version\nAdditionally, you can now install software like wget like this:\nbrew install wget"
  },
  {
    "objectID": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html#installing-miniforge",
    "href": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html#installing-miniforge",
    "title": "Installing Fast.AI on Apple Silicon",
    "section": "Installing Miniforge",
    "text": "Installing Miniforge\nWith a few basics sorted out, we can now install python. The recommendation in the Live Coding Session is the use Mambaforge, but in the meantime, it has been merged with Miniforge, therefore, this is want I installed:\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh\nbash Miniforge3-MacOSX-arm64.sh\nAfter restarting the terminal, you can see the (base) prefix to the prompt, signalling that python is available.\nAdditionally, you can now execute which python to see the path where Python is installed."
  },
  {
    "objectID": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html#installing-additional-python-packages",
    "href": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html#installing-additional-python-packages",
    "title": "Installing Fast.AI on Apple Silicon",
    "section": "Installing additional Python packages",
    "text": "Installing additional Python packages\nThe recommended way to install packages is via mamba, as per the tutorial, execute\nmamba install ipython\n\nInstalling Pytorch\nNext we install pytorch with the following parameters:\n\n\n\nPytorch Installation Paramters\n\n\nmamba install pytorch::pytorch torchvision torchaudio -c pytorch\n\n\nInstalling Fast.AI\nOnce that is done, we can install the Fast.AI and additional packages:\nmamba install jupyterlab\nmamba install ipywidgets\nmamba install -c fastai fastai\nmamba install -c fastchan fastbook\nmamba install -c fastchan sentencepiece\n\n\nInstalling Quarto (for writing this blog)\nTo be able to update this blog, I needed to re-install the related packaged. More details are available in this blog post:\nmamba install -c fastai -y nbdev\nsoftwareupdate --install-rosetta\nnbdev_install_quarto"
  },
  {
    "objectID": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html#setup-of-git",
    "href": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html#setup-of-git",
    "title": "Installing Fast.AI on Apple Silicon",
    "section": "Setup of Git",
    "text": "Setup of Git\nGit was already installed on my machine (brew install git) which I did as part of the installation of VS Code, so here we only cover the setup done in the terminal:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.mail@service.com\"\n\nssh-keygen\ncat ~/.ssh/id_rsa.pub\nNow you add the key to your Github keys. Afterwards you should be able to login\nssh git@github.com\nAdding Git Large File Storage (LFS) to be able to push large files to GitHub (like .pkl-files):\nbrew install git-lfs\nThis concludes the installation of Fast.AI on an Apple Silicon Machine. In a follow-up blog post I will talk about my experience on running Fast.AI on Apple Silicon."
  },
  {
    "objectID": "posts/2024-01-05-running-ml-on-apple-silicon/index.html",
    "href": "posts/2024-01-05-running-ml-on-apple-silicon/index.html",
    "title": "Running Fast.AI / Huggingface Transformers on Apple Silicon",
    "section": "",
    "text": "In my previous blog post, I described how I setup my Fast.AI development environment on Apple Silicon. In this one, let me share my experience running some notebooks on Apple Silicon. I focus on what needed to be done to adjust the notebooks to Apple Silicon (spoiler alert: it is not difficult) and I also share some performance indications how well Apple Silicon performs.\nI have revisited 2 projects I have worked on before, and I ported them to Apple Silicon:"
  },
  {
    "objectID": "posts/2024-01-05-running-ml-on-apple-silicon/index.html#detecting-apple-silicon",
    "href": "posts/2024-01-05-running-ml-on-apple-silicon/index.html#detecting-apple-silicon",
    "title": "Running Fast.AI / Huggingface Transformers on Apple Silicon",
    "section": "Detecting Apple Silicon",
    "text": "Detecting Apple Silicon\nBefore we jump into these 2 use cases, let’s look at how to detect Apple Silicon. We need to do this because we need to do minor optimizations in both notebooks so that they run at all / with the GPU.\nFor using the the GPUs, there is the so-called Metal Performance Shaders framework, abbreviated MPS. To be able to access MPS, you need to have a pytorch version of at least 1.12.\nStating what probably is obvious but for completeness: NVIDIA acceleration, the other way of GPU acceleration, is abbreviated with CUDA (Compute Unified Device Architecture), and we can see it being mentioned in the code side-by-side frequently.\nSo let’s check the requirements:\n\nimport torch\n\nprint(f\"Pytorch is running on version {torch.__version__}\")\nif torch.backends.mps.is_available():\n    print (\"MPS device found.\")\n    mps_device = torch.device(\"mps\")\nelse:\n    print (\"MPS device not found.\")\n\nPytorch is running on version 2.1.1\nMPS device found.\n\n\nFast.AI also provides a function for checking the available devices called default_device() which returns a torch.device object. Calling it without a parameter, i.e. -1 detects the device. Calling it with True forces it to return a cuda/mps-object, and calling it with False forces it to return a cpu-object.\n\nfrom fastai.vision.all import *\n\ndevice = default_device()\nprint(device)\n\nmps"
  },
  {
    "objectID": "posts/2024-01-05-running-ml-on-apple-silicon/index.html#mnist-on-fast.ai",
    "href": "posts/2024-01-05-running-ml-on-apple-silicon/index.html#mnist-on-fast.ai",
    "title": "Running Fast.AI / Huggingface Transformers on Apple Silicon",
    "section": "MNIST on Fast.AI",
    "text": "MNIST on Fast.AI\nI was looking for a fairly (but not too) compute intensive initial project to try out the performance of my MacBook, and I ended up at the “hello world” of machine learning 😉.\nWorking on MNIST, I had built a baseline-notebook which takes the Fast.AI-version of the MNIST dataset (i.e. using images), creates a model and runs predictions, pretty much with lots of boilerplate.\nThe Apple Silicon version looks very similar: By default, however, Fast.AI ignores the GPU (cuda/mps) everything runs on the cpu. This can easily be fixed by passing the device as the default device (see above) as pointed out by this blog post / this post in the Fast.AI forums when creating the dataloaders:\ndls = mnist1.dataloaders(path, bs=32, device=default_device())\nAfterwards, the training leverages the GPU, and it performed quite well, similar to a free Paperspace instance I had used before. Here are some training times I collected:\n\n\n\nHardware\nTotal Time\nTime 1\nTime 2\nTime 3\nTime 4\n\n\n\n\nM2 Max (Juypter)\n06:33\n01:23\n01:45\n01:42\n01:43\n\n\nM2 Max (Juypter-squish)\n06:33\n01:23\n01:45\n01:42\n01:43\n\n\nM2 Max (VSCode)\n07:16\n01:34\n01:54\n01:54\n01:54\n\n\nM3 Pro (Juypter)\n22:42\n07:19\n04:28\n04:47\n06:08\n\n\nSurface 4 Pro\n40:17\n04:19\n10:29\n13:02\n12:27\n\n\nSurface 4 Pro (squish)\n4:56:50\n1:02:35\n1:18:50\n1:15:45\n1:19:40\n\n\nPaperspace (2002)\n05:56\n01:08\n01:36\n01:36\n01:36\n\n\n\nNotes regarding the table:\n\nThe link on the total time leads to a screenshots showing the detailed training times.\nIf not incited otherwise, the training was done in Jupyter Labs.\n\nTakeaways:\n\nI was surprised to see that there was a difference in running the code in Jupyter Lab vs. in VS Code. For more complex tasks, Jupyter Labs seems to be the runtime environment of choice (but for editing I like VS Code better).\nThe M2 Max performs comparably to a free Paperspace GPU. Honestly, I am not super-impressed with this result, because 6 years in hardware development (between the Surface and the M2 Max) should have yielded much better results, so I suspect there are some inefficiencies somewhere.\nM3 Pro is even worse. The result is so bad, that it is hard to believe, even if M3 Pro is the lesser processor compared to M2 Max.\nAdding more computation by adding in the squish transformation highlights to true relation of performance, M2 Max hardly slows down, but my old Surface Pro needed almost 5 hours for the task, which underlines the suspicion that there are some inefficiencies hidden when the overall computational load is low."
  },
  {
    "objectID": "posts/2024-01-05-running-ml-on-apple-silicon/index.html#natural-language-processing-with-disaster-tweets-on-huggingface-transformers",
    "href": "posts/2024-01-05-running-ml-on-apple-silicon/index.html#natural-language-processing-with-disaster-tweets-on-huggingface-transformers",
    "title": "Running Fast.AI / Huggingface Transformers on Apple Silicon",
    "section": "Natural Language Processing with Disaster Tweets on Huggingface transformers",
    "text": "Natural Language Processing with Disaster Tweets on Huggingface transformers\nThe second use case was more compute intensive, running the model microsoft/deberta-v3-large. As it turned out, I needed install/upgrade some packages and make 2 adjustments to my notebook to be able to run it on Apple Silicon.\nThe first step was easy: stepping through the notebook, some packages were missing/needed upgrades:\npip install protobuf   \npip install evaluate\npip install accelerate -U\nRegarding code, Apple Silicon (i.e. MPS - Metal Performance Shaders) apparently does not support FP16 (16-bit floating-point), therefore, the parameter fp16 needs to be passed as False to the TrainingArguments. For generic detection of the parameter:\n\nimport torch\n\nfp16_available = True\n\nif torch.backends.mps.is_available():\n    fp16_available = False\n\nprint(f\"FP16 is available: {fp16_available}\")\n\nFP16 is available: False\n\n\nAdditionally, I needed to adjust the memory management, because the model ran out of memory: RuntimeError: MPS backend out of memory (MPS allocated: 7.18 GB, other allocations: 28.82 GB, max allowed: 36.27 GB). Tried to allocate 500.39 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\nPYTORCH_MPS_HIGH_WATERMARK_RATIO is an environment variable related to PyTorch’s memory management when using the MPS. It sets the ratio of the total GPU memory that PyTorch is allowed to allocate when using MPS. The ratio is expressed as a decimal fraction of the total available GPU memory. For example, 0.8 means that PyTorch is allowed to use 80% of the GPU memory. By setting PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 PyTorch does not have an upper limit on memory allocations for MPS operations. This means PyTorch can potentially use all available GPU memory for its computations.\nSetting this parameter needs to be done before running any PyTorch code, so I put it into the very first cells of my notebook. Let’s check the parameter first:\n\nimport os\n\nhigh_watermark_ratio = os.getenv('PYTORCH_MPS_HIGH_WATERMARK_RATIO')\n\nif high_watermark_ratio is not None:\n    print(f\"PYTORCH_MPS_HIGH_WATERMARK_RATIO is set to: {high_watermark_ratio}\")\nelse:\n    print(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO is not set (using default behavior).\")\n\nPYTORCH_MPS_HIGH_WATERMARK_RATIO is not set (using default behavior).\n\n\nSo let’s go ahead and set the parameter:\n\n\nos.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n\nAs a result, the notebook ran to the end! It took about 1 hour (compared to about 20 minutes on Huggingface), and it resulted in heavy memory usage of up to 60GB (even though I have only 32GB of physical memory):\n\n\n\nMemory Usage\n\n\nI am not quite sure how to make of this heavy usage, especially since only about 5GB were swapped, so it does not really add up to me… Nonetheless, it worked!\nBefore closing: Yes, there would be other ways to try to optimize the memory usage (like gradient accumulation), but the goal here was to stay as close to the original code as possible."
  },
  {
    "objectID": "posts/2024-01-05-running-ml-on-apple-silicon/index.html#overall-conclusion",
    "href": "posts/2024-01-05-running-ml-on-apple-silicon/index.html#overall-conclusion",
    "title": "Running Fast.AI / Huggingface Transformers on Apple Silicon",
    "section": "Overall conclusion",
    "text": "Overall conclusion\nIt is always nice to be able to run things on out own machine. But overall, the performance is not as good as even the free versions of Paperspace or Kaggle.\nNonetheless, I would attribute some convenience to being able to work locally in terms of setup, being in a known environment, being able to work traveling. Additionally, more capable laptops are becoming ubiquitous than ever, so if you have one, why not use it? Especially for playing with smaller models, it makes lots of sense to me, and for bigger models, you can always easily shift to an online service.\nAgain, there are tradeoffs and it depends on personal taste. What’s your take on this? Do you prefer to work on your laptop or your preferred online environment?"
  },
  {
    "objectID": "posts/2025-07-08-implementing-fastapi-greeting/index.html",
    "href": "posts/2025-07-08-implementing-fastapi-greeting/index.html",
    "title": "Implementing a Greeting API with FastAPI",
    "section": "",
    "text": "Creating a RESTful API may sound complicated, but with FastAPI, it’s remarkably straightforward. FastAPI is a modern, high-performance web framework for building APIs with Python, designed to be both easy to use and fast to run.\nThis blog post, combined with two follow-ups (version 1 (educational) and version 2 (SAP BTP)), is an extended version of a simple FastAPI example I created for a recent hackathon. The goal was to provide a minimal, reusable, beginner-friendly template that’s practical for small projects and prototyping.\nIn this guide, I’ll walk you through building a simple greeting service using FastAPI, implemented in two common API styles:\nThe full solution includes:"
  },
  {
    "objectID": "posts/2025-07-08-implementing-fastapi-greeting/index.html#installation",
    "href": "posts/2025-07-08-implementing-fastapi-greeting/index.html#installation",
    "title": "Implementing a Greeting API with FastAPI",
    "section": "Installation",
    "text": "Installation\nIf you haven’t installed FastAPI yet, you’ll need the following Python packages:\n\nfastapi (GitHub repo): FastAPI is a modern, high-performance web framework for building APIs with Python, based on standard Python type hints. It handles request parsing, validation, routing, and automatic documentation generation.\nuvicorn (official site | GitHub): Uvicorn is an ASGI (Asynchronous Server Gateway Interface) web server used to run FastAPI applications.\nnest_asyncio (PyPI): This package is required when running asynchronous code like FastAPI inside a Jupyter notebook. It patches the loop to support nested async operations.\n\nYou can install all of them using pip:\npip install fastapi uvicorn nest_asyncio"
  },
  {
    "objectID": "posts/2025-07-08-implementing-fastapi-greeting/index.html#greeting-1-using-url-query-parameters-get-request",
    "href": "posts/2025-07-08-implementing-fastapi-greeting/index.html#greeting-1-using-url-query-parameters-get-request",
    "title": "Implementing a Greeting API with FastAPI",
    "section": "Greeting 1 – Using URL Query Parameters (GET Request)",
    "text": "Greeting 1 – Using URL Query Parameters (GET Request)\nLet’s start with a simple version of the greeting server that uses query parameters to pass data via a GET request.\nIn this version, the user’s name is appended to the URL (e.g., ?name=Christian), and the server returns a greeting.\nThe following cell sets up the FastAPI application and starts the server using Uvicorn:\n\nfrom fastapi import FastAPI\nimport nest_asyncio\nimport uvicorn\n\n# Apply nest_asyncio to allow running FastAPI in Jupyter notebooks\nnest_asyncio.apply() # only needed in Jupyter notebooks\n\n# Create a FastAPI application instance\napp = FastAPI()\n\n# Define a root endpoint\n@app.get(\"/\")\ndef root():\n    return {\"message\": f\"Hello World\"}\n\n# Define a greeting endpoint\n@app.get(\"/greet\")\ndef greet(name: str = \"World\"):\n    return {\"message\": f\"Hello {name}\"}\n\n# Run the FastAPI application using Uvicorn\nuvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\nTo test the greeting API, you have the following options while the server is running:\n\nOpen http://127.0.0.1:8000/ in your browser for a default “Hello World” message.\nOpen http://127.0.0.1:8000/greet?name=Christian in your browser (replace “Christian” with your name) to see your greeting message.\nUse the greeting client notebook to call the greeting API using Python code.\nAccess the automatically generated Swagger UI to test and explore the API interactively.\n\nTo stop the server, just interrupt the cell execution (e.g., with the ⏹️ stop button)."
  },
  {
    "objectID": "posts/2025-07-08-implementing-fastapi-greeting/index.html#api-documentation-in-detail",
    "href": "posts/2025-07-08-implementing-fastapi-greeting/index.html#api-documentation-in-detail",
    "title": "Implementing a Greeting API with FastAPI",
    "section": "API Documentation in Detail",
    "text": "API Documentation in Detail\nFastAPI automatically generates rich API documentation based on the OpenAPI standard. It uses Python type hints and Pydantic models to build a machine-readable specification of your API.\nFastAPI provides the following documentation endpoints out of the box:\n\nOpenAPI schema (JSON): Available at /openapi.json, this machine-readable schema defines the structure of your API for tools and clients.\nSwagger UI: Accessible at /docs, this interactive UI lets you explore and test endpoints directly from your browser.\nReDoc: A more elegant, documentation-focused view is available at /redoc.\n\nBoth Swagger vs ReDoc are built on the same OpenAPI specification, but serve different purposes. Swagger UI is ideal for developers who want to explore and test the API interactively while ReDoc is better suited for read-only API documentation, it is a more elegant and mobile-friendly view of the API documentation.\n\nEnriching the API Documentation\nWhile FastAPI generates a lot of metadata automatically, you can improve clarity and usability by adding explicit descriptions. In the following example we’ll add the following enhancements to the API documentation:\n\nAPI metadata: Set the title, description, and version of the API via FastAPI(title=..., description=..., version=...)\nEndpoint metadata:\n\nsummary: A short label shown in the UI\ndescription: You can add longer explanation. If this parameter is not set, it defaults to the function’s docstring\nresponse_description: Text shown next to the response block\n\nResponse models: Defined via Pydantic BaseModel classes we can define the structure of the response data.\nQuery parameter descriptions: Set using Query(..., description=...) we can provide a default value and a description for the query parameter.\n\n\nfrom fastapi import FastAPI, Query\nfrom pydantic import BaseModel\nimport nest_asyncio\nimport uvicorn\n\nnest_asyncio.apply()\n\napp = FastAPI(\n    title=\"Greeting API\",\n    description=\"A simple API that returns a personalized greeting to the user or the world.\",\n    version=\"1.0.0\"\n)\n\nclass GreetingResponse(BaseModel):\n    message: str\n\n@app.get(\n    \"/greet\",\n    summary=\"Generates a greeting\",\n    #description=\"Use this parameter if you want to overwrite the function's docstring.\",\n    response_description=\"A JSON response with the greeting message\",\n    response_model=GreetingResponse,\n)\ndef greet(name: str = Query(default=\"World\", description=\"The name of the person to greet.\")):\n    \"\"\"\n    Generates a simple greeting.\n\n    This endpoint takes a `name` query parameter and returns a JSON object\n    with a personalized message. If no name is provided, it defaults to \"World\".\n    \"\"\"\n    return {\"message\": f\"Hello {name}\"}\n\nuvicorn.run(app, host=\"127.0.0.1\", port=8000)"
  },
  {
    "objectID": "posts/2025-07-08-implementing-fastapi-greeting/index.html#calling-the-api",
    "href": "posts/2025-07-08-implementing-fastapi-greeting/index.html#calling-the-api",
    "title": "Implementing a Greeting API with FastAPI",
    "section": "Calling the API",
    "text": "Calling the API\nYou probably have tested the API using the browser, the greeting client notebook, or the Swagger UI. Let’s make this a little for realistic and call the API using a simple HTML frontend. The HTML-version of the greeting client (download the file and run it locally) contains a simple form that allows you to enter your name and submit it to the API. The response is displayed in the browser.\nWhen you try to call the API in your browser, however, it does not work… The reason is that the HTML file is served from the file (file://{path}/greet-client-get.html) system, and when you try to call the API from the HTML file using JavaScript (running on http://127.0.0.1:8000/), you will get a CORS (Cross-Origin Resource Sharing) error. This is because the browser blocks the request to the API from a different origin. This is unlike the API test from the Swagger UI, which runs on the same origin as the API (http://127.0.0.1:8000/).\nTo fix this, we need to enable CORS in the FastAPI application. FastAPI provides a middleware to handle CORS requests. When you run the following cell, it will start the FastAPI server with CORS enabled, and you can all the API from the HTML-version of the greeting client.\n\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nimport nest_asyncio\nimport uvicorn\n\nnest_asyncio.apply()\n\napp = FastAPI()\n\n# Enable CORS so browser fetch() can talk to it\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # For local testing only\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Define a greeting endpoint\n@app.get(\"/greet\")\ndef greet(name: str = \"World\"):\n    return {\"message\": f\"Hello {name}\"}\n\n# Run the FastAPI application using Uvicorn\nuvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\nThe CORS middleware is required whenever your frontend and backend run on different origins. In production it is therefore needed where APIs and frontends are deployed separately. For local testing, it’s safe to allow all origins with [“*”], but in production, you should restrict access to trusted domains."
  },
  {
    "objectID": "posts/2025-07-08-implementing-fastapi-greeting/index.html#greeting-2---using-json-payload-post-request",
    "href": "posts/2025-07-08-implementing-fastapi-greeting/index.html#greeting-2---using-json-payload-post-request",
    "title": "Implementing a Greeting API with FastAPI",
    "section": "Greeting 2 - Using JSON Payload (POST Request)",
    "text": "Greeting 2 - Using JSON Payload (POST Request)\nSo far, we have implemented the API using a GET request with a query parameter to pass the name to the server using URL query parameters. This is a common pattern for simple APIs, but it has some limitations: The URL length is limited, and it is not suitable for complex data structures. Therefore, we will implement a second version of the greeting server, which uses a JSON payload to pass the name to the server using a POST request:\n{\n  \"name\": \"Christian\"\n}\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom fastapi.middleware.cors import CORSMiddleware\nimport nest_asyncio\nimport uvicorn\n\n# Allow uvicorn to run inside the notebook\nnest_asyncio.apply()\n\n# Define the FastAPI app\napp = FastAPI()\n\n# Enable CORS so browser fetch() can talk to it\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # For local testing only\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Define a Pydantic model for the expected JSON input\nclass NameRequest(BaseModel):\n    name: str\n\n# POST endpoint that expects JSON input\n@app.post(\"/greet\")\ndef greet_name(request: NameRequest):\n    return {\"message\": f\"Hello {request.name}\"}\n\n# Start the FastAPI server (in the same cell!)\nuvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\nTesting this API in the browser is not possible, but we need a form to submit the data. You can choose one of the following options:\n\nRun the Greeting Client Notebook\nUse the HTML-version of the greeting client to submit the data via a form in your browser (download the file and open it in your browser).\nUse the Swagger UI to test the API interactively.\n\nRegarding documentation, we can use the same approaches as for the first version of the greeting server."
  },
  {
    "objectID": "posts/2025-07-08-implementing-fastapi-greeting/index.html#putting-it-all-together",
    "href": "posts/2025-07-08-implementing-fastapi-greeting/index.html#putting-it-all-together",
    "title": "Implementing a Greeting API with FastAPI",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nTo wrap up this blog post, the following cell contains everything we’ve discussed in one place. It implements the Greeting API using both GET and POST methods. Note that both endpoints are on the same route (/greet). FastAPI allows us to define multiple HTTP methods for the same endpoint by using different decorators (@app.get and @app.post).\nThis version also includes improved metadata and documentation. You can explore the API through the:\n\nSwagger UI – interactive and developer-friendly\nReDoc documentation – elegant and mobile-friendly\nOpenAPI schema – machine-readable JSON\n\nFinally, I included a more realistic CORS configuration that allows the API to be accessed only from localhost (typically 127.0.0.1, as used during local testing) or from the test UI deployed on GitHub Pages. The only caveat is that testing directly from the file system (file://) no longer works, since file:// cannot be specified as a valid CORS origin.\nNonetheless, once you run the following cell, you can test both versions of the API from with these resources:\n\nThe client notebooks for GET and POST\nThe HTML frontends for query-based and JSON-based versions\n\n\nfrom fastapi import FastAPI, Query\nfrom pydantic import BaseModel\nfrom fastapi.middleware.cors import CORSMiddleware\nimport nest_asyncio\nimport uvicorn\n\n# Allow FastAPI to run inside a Jupyter notebook\nnest_asyncio.apply()\n\n# Define the FastAPI app\napp = FastAPI(\n    title=\"Greeting API\",\n    description=\"A simple API that returns a personalized greeting, either via URL query parameters or JSON payload.\",\n    version=\"1.0.0\"\n)\n\n# Enable CORS (for local testing and GitHub Pages deployment)\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\n        \"http://localhost:8000\",              # Local testing\n        \"http://127.0.0.1:8000\",              # Also valid local reference\n        \"https://chrwittm.github.io\",         # GitHub Pages deployment\n    ],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Define Pydantic model for POST request\nclass NameRequest(BaseModel):\n    name: str\n\n# Pydantic model for response (shared across GET and POST)\nclass GreetingResponse(BaseModel):\n    message: str\n\n# GET endpoint using query parameter\n@app.get(\"/greet\", response_model=GreetingResponse, summary=\"Greet via query parameter\")\ndef greet_get(name: str = Query(default=\"World\", description=\"The name of the person to greet.\")):\n    \"\"\"\n    Returns a greeting based on the query parameter 'name'.\n    \"\"\"\n    return {\"message\": f\"Hello {name}\"}\n\n# POST endpoint using JSON body\n@app.post(\"/greet\", response_model=GreetingResponse, summary=\"Greet via JSON payload\")\ndef greet_post(request: NameRequest):\n    \"\"\"\n    Returns a greeting based on the JSON body with a 'name' field.\n    \"\"\"\n    return {\"message\": f\"Hello {request.name}\"}\n\n# Run the app locally in the notebook\nuvicorn.run(app, host=\"127.0.0.1\", port=8000)"
  },
  {
    "objectID": "posts/2025-07-08-implementing-fastapi-greeting/index.html#conclusion",
    "href": "posts/2025-07-08-implementing-fastapi-greeting/index.html#conclusion",
    "title": "Implementing a Greeting API with FastAPI",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we’ve explored how to build a simple yet well-structured API using FastAPI. We started with a lightweight GET endpoint using query parameters and then moved on to a more robust POST endpoint that accepts JSON payloads. Along the way, we learned how FastAPI automatically generates interactive documentation using the OpenAPI standard. Additionally, we explored how we can enrich the documentation through type annotations, docstrings, and metadata. Even though our greeting service is simple, the patterns we’ve used scale well to real-world applications.\nIn follow-up posts, available in two versions (using Render for educational purposes or using SAP BTP for enterprise scale), we’ll look at how to deploy this service locally or in the cloud."
  },
  {
    "objectID": "posts/2025-04-20-beneath-the-blue-dome/index.html",
    "href": "posts/2025-04-20-beneath-the-blue-dome/index.html",
    "title": "Beneath the Blue Dome",
    "section": "",
    "text": "Here is a vibe-written short story, inspired by Andrej Karpathy’s idea of “vibe-coding” applied to writing. A few weeks ago, a fly had a short but striking encounter with me on a winter morning. I had forgotten all about it, until I recently rediscovered the draft.\nWorking together with GPT-4.5 for the initial draft and o1 for the iterations, I wanted to capture that moment, not just factually, but emotionally. The concept of vibe-writing is simple: Instead of writing for yourself, you guide the AI with impressions, feelings, or ideas. You iterate together, nudging the story into the shape that feels right. In this case, I wanted to tell the story as if it was an alien abduction. Would anybody believe what happened?\nAnd this is how the story unfolded:"
  },
  {
    "objectID": "posts/2025-04-20-beneath-the-blue-dome/index.html#beneath-the-blue-dome",
    "href": "posts/2025-04-20-beneath-the-blue-dome/index.html#beneath-the-blue-dome",
    "title": "Beneath the Blue Dome",
    "section": "Beneath the Blue Dome",
    "text": "Beneath the Blue Dome\nI spent the dark hours huddled against a rough barrier, half-frozen, waiting for the faint warmth that always comes with dawn. My limbs were so cold they could barely move, and each breath felt like it might be my last. When light finally seeped through the small gaps, a gentle heat came with it, and I stretched slowly, feeling my wings loosen just enough to flutter.\nDrawn by that promise of comfort, I crept toward the warmth. Past the barrier lay a world unlike any I had known: smooth expanses that shone under a steady, unwavering glow. The air was thick and still, carrying scents I couldn’t place. My antennae tingled with both curiosity and caution. Yet the warmth was too tempting to resist, so I ventured on, letting my wings rest for the first time in what felt like ages.\nI continued exploring, landing on surfaces that seemed to reflect my image back at me in strange, distorted ways. The temperature was so pleasant, I almost forgot how alien everything appeared. I wanted to understand this place better—its endless shapes and angles, the quiet hum that pulsed through the air. Had I finally found shelter from the cold, or stumbled upon something far more dangerous?\nThen, abruptly, a shimmering blue dome descended from above. Before I could react, it trapped me inside. My heart raced as the surface beneath me shifted—or perhaps it was the dome itself that moved. Beyond the hazy blue, I could only make out large, shifting silhouettes. My wings trembled with panic. I wondered if I should have ignored that inviting breeze and stayed away. The air felt stale under the dome’s curve, and time stretched uncomfortably, each moment weighed down by confusion and fear.\nJust as suddenly, the dome lifted. A burst of cold air rushed in, stinging my senses. In a blink, I realized I was no longer in that warm realm. Where had the walls gone? My wings buzzed frantically, carrying me back into the bite of the chill. Disoriented, I hovered in mid-air, trying to piece together what had happened. Had I been dreaming? The memory of that confining blue glow felt all too real.\nUnsure who—or what—had set me free, I left without looking back. No one would likely believe a story of a warm place with endless walls and an all-encompassing blue dome. But I knew what I felt. The warmth still lingered on my wings, even as the cold reclaimed me. I didn’t understand that strange world, and perhaps it was best left behind. Yet I carried its memory as I flew on, alive and uncertain, with more questions than answers."
  },
  {
    "objectID": "posts/2026-01-29-sap-orchestration-hello-world/index.html",
    "href": "posts/2026-01-29-sap-orchestration-hello-world/index.html",
    "title": "Building a Harmonized-API ‘Hello World’ with AI Core",
    "section": "",
    "text": "The Harmonized API of the Orchestration Service is probably one of SAP’s best kept secrets. It allows you to talk to all the language models on AI core in a harmonized format across model families. This means that you can simply swap out the model name from gpt-4o to anthropic--claude-4.5-sonnet without touching anything else of your code. This enables you to simply compare model performance or even build redundancies into your use cases.\nBy the end of this tutorial (which is also available as a Jupyter notebook on GitHub), you’ll have a working setup for talking to SAP’s Harmonized API of the Orchestration Service using the Generative AI Hub SDK on AI Core. Honestly, the hardest part is just stating what we’re using 😉, the actual content is rather simple 🤓. Let’s cut through the jargon and build something cool."
  },
  {
    "objectID": "posts/2026-01-29-sap-orchestration-hello-world/index.html#setup",
    "href": "posts/2026-01-29-sap-orchestration-hello-world/index.html#setup",
    "title": "Building a Harmonized-API ‘Hello World’ with AI Core",
    "section": "Setup",
    "text": "Setup\nBefore we can talk to the Orchestration Service, we need to install the SAP Cloud SDK for AI.\n\n#!pip install \"sap-ai-sdk-gen[all]\"\n\nNext we need to focus on authentication. As explained in the docs, there are multiple ways to do this.\nTo keep this notebook-friendly with minimal setup, I will use a .env-file. You can extract all necessary values from the BTP service key of your AI Core Service Instance.\nJust create the .env-file in the same directory as the Jupyter notebook with the following format:\nAICORE_AUTH_URL=https://********.authentication.********.hana.ondemand.com\nAICORE_CLIENT_ID=********\nAICORE_CLIENT_SECRET=********\nAICORE_RESOURCE_GROUP=********\nAICORE_BASE_URL=https://api.ai.********.hana.ondemand.com/v2\nAll we need to do is load the .env. If you stick to the naming conventions, the SDK will automatically use it correctly.\n\nfrom dotenv import load_dotenv\nimport os\n\n# Load the .env file\nload_dotenv()\n\n# Keeping rest of the cell for explicit checking, if you want to experiment\n\n# Access the variables\n#aicore_auth_url = os.getenv(\"AICORE_AUTH_URL\")\n#aicore_client_id = os.getenv(\"AICORE_CLIENT_ID\")\n#aicore_client_secret = os.getenv(\"AICORE_CLIENT_SECRET\")\n#aicore_resource_group = os.getenv(\"AICORE_RESOURCE_GROUP\")\n#aicore_base_url = os.getenv(\"AICORE_BASE_URL\")\n\n# Print them to check\n#print(f\"AICORE_AUTH_URL: {aicore_auth_url}\")\n#print(f\"AICORE_CLIENT_ID: {aicore_client_id}\")\n#print(f\"AICORE_CLIENT_SECRET: {aicore_client_secret}\")\n#print(f\"AICORE_RESOURCE_GROUP: {aicore_resource_group}\")\n#print(f\"AICORE_BASE_URL: {aicore_base_url}\")\n\nTrue\n\n\nOnce done, we can talk to the Orchestration Service."
  },
  {
    "objectID": "posts/2026-01-29-sap-orchestration-hello-world/index.html#building-hello-world",
    "href": "posts/2026-01-29-sap-orchestration-hello-world/index.html#building-hello-world",
    "title": "Building a Harmonized-API ‘Hello World’ with AI Core",
    "section": "Building Hello World",
    "text": "Building Hello World\nFor all the elements of the API, the SAP Cloud SDK for AI has a dedicated class which abstracts the model specifics away.\nFor example, the different message types used in LLM communication are represented by the classes SystemMessage, UserMessage, and AssistantMessage.\n\nfrom gen_ai_hub.orchestration.models.message import SystemMessage, UserMessage\n\nmessages=[\n    SystemMessage(\"Act like the very first program of a coding tutorial.\"),\n    UserMessage(\"What do you respond upon execution?\")\n]\n\nThe messages need to be wrapped in a template. Even if we could do a lot more with the template (placeholders, structured outputs, tool definitions), let’s treat it as a simple wrapper for now.\n\nfrom gen_ai_hub.orchestration.models.template import Template\n\ntemplate = Template(messages)\n\nEven though the code is model-independent, we need to specify which model we want to use.\n\nfrom gen_ai_hub.orchestration.models.llm import LLM\n\nllm = LLM(name=\"gpt-4o\") \n\nWhen we combine the template and the LLM, this is called a configuration.\n\nfrom gen_ai_hub.orchestration.models.config import OrchestrationConfig\n\nconfig = OrchestrationConfig(template=template, llm=llm)\n\nFinally, we can pass the configuration to the orchestration service to send the prompt to the LLM.\n\nfrom gen_ai_hub.orchestration.service import OrchestrationService\n\norchestration_service = OrchestrationService(config=config)\nresult = orchestration_service.run()\n\nThe result is a typical OpenAI-style nested object. Here’s how we can extract the model response:\n\nprint(result.orchestration_result.choices[0].message.content)\n\nHello, World!\n\n\nWe have successfully established the communication with the orchestration service 🎉. Let’s try swapping out the model next."
  },
  {
    "objectID": "posts/2026-01-29-sap-orchestration-hello-world/index.html#how-to-simply-swap-models",
    "href": "posts/2026-01-29-sap-orchestration-hello-world/index.html#how-to-simply-swap-models",
    "title": "Building a Harmonized-API ‘Hello World’ with AI Core",
    "section": "How to simply swap models",
    "text": "How to simply swap models\nLet’s wrap our orchestration call in a helper function to easily compare models:\n\ndef call_orchestration_service(system_prompt: str, user_prompt: str, model_name: str) -&gt; str:\n    \"\"\"Simple wrapper to call the Orchestration Service.\"\"\"\n    \n    messages = [\n        SystemMessage(system_prompt),\n        UserMessage(user_prompt)\n    ]\n    \n    config = OrchestrationConfig(\n        template=Template(messages),\n        llm=LLM(name=model_name)\n    )\n    \n    result = OrchestrationService(config=config).run()\n    return result.orchestration_result.choices[0].message.content\n\nNow let’s ask three different models the same question:\n\nsystem_prompt = \"Answer in a concise way.\"\nuser_prompt = \"Who are you? Which model do you use?\"\n\n\ncall_orchestration_service(system_prompt, user_prompt, \"gpt-4o\")\n\n\"I'm an AI language model created by OpenAI, based on the GPT-4 architecture.\"\n\n\n\ncall_orchestration_service(system_prompt, user_prompt, \"anthropic--claude-3.5-sonnet\")\n\n\"I'm an AI assistant created by Anthropic to be helpful, harmless, and honest. I don't have information about my specific model or training.\"\n\n\n\ncall_orchestration_service(system_prompt, user_prompt, \"gemini-2.5-flash\")\n\n\"I am a large language model, trained by Google. I use Google's Gemini model family.\"\n\n\nNotice how each model proudly announces its creator, yet our code didn’t change at all. That’s one of the key advantages of using the harmonized API compared to the model-specific chat completion API.\nBefore we close this hello world example, one final question remains: Which models can you actually use?"
  },
  {
    "objectID": "posts/2026-01-29-sap-orchestration-hello-world/index.html#available-models",
    "href": "posts/2026-01-29-sap-orchestration-hello-world/index.html#available-models",
    "title": "Building a Harmonized-API ‘Hello World’ with AI Core",
    "section": "Available Models",
    "text": "Available Models\nThe easiest way to find out which models are supported is to simply read the documentation - Who would have thought? 😉 Nonetheless, for the most up-to-date information you should check note 3437766, which lists the availability of Generative AI Models.\nAt the time of writing, Claude Opus 4.5 was not listed in the docs, but the note listed it as available and it works:\n\ncall_orchestration_service(system_prompt, user_prompt, \"anthropic--claude-4.5-opus\")\n\n\"I'm Claude, an AI assistant made by Anthropic.\\n\\nI am the model—I'm Claude, specifically from Anthropic's Claude model family. I don't have access to my exact version number in this conversation, but I'm one of the Claude models (such as Claude 3.5 Sonnet, Claude 3 Opus, etc.).\\n\\nIs there something specific you'd like to know about my capabilities?\"\n\n\nWhen trying out different models via the harmonized API, it is important to note that you do not need a deployment in AI Core to access the model. This may sound surprising, but trust me, I didn’t create a deployment for Claude Opus 4.5 in AI core, yet it works. This is one of the key benefits of the orchestration service with the harmonized API: SAP manages the model deployments centrally, so you can simply switch model names without provisioning anything yourself.\nAs we can see, the documentation is sometimes lagging behind reality in the system. Therefore, I was curious if we could just ask AI Core which models are available, and here’s what I found. Since there is no good property to filter on to only get the LLMs, I relied on the name and description to filter the list for non-deprecated LLMs. Just keep in mind that the following list is a snapshot and depending on when you run this, the list will look different:\n\nfrom ai_core_sdk.ai_core_v2_client import AICoreV2Client\n\nclient = AICoreV2Client.from_env()\nfor m in client.model.query().resources:\n    # Filter out embedding models, rerankers, and deprecated models\n    name = m.model.lower()\n    desc = m.description.lower()\n    if any(x in name or x in desc for x in ['embed', 'rerank', 'sap-abap', 'sap-rpt', 'gpt-35']):\n        continue\n    print(f\"{m.provider}: {m.model}\")\n\nCohere: cohere--command-a-reasoning\nGoogle: gemini-2.0-flash\nGoogle: gemini-2.0-flash-lite\nGoogle: gemini-2.5-pro\nGoogle: gemini-2.5-flash\nGoogle: gemini-2.5-flash-lite\nOpenAI: gpt-5\nOpenAI: gpt-5-nano\nOpenAI: gpt-5-mini\nOpenAI: gpt-4o\nOpenAI: gpt-4o-mini\nOpenAI: gpt-4.1\nOpenAI: gpt-4.1-nano\nOpenAI: gpt-4.1-mini\nOpenAI: o3-mini\nOpenAI: o3\nOpenAI: o4-mini\nPerplexity: sonar-pro\nPerplexity: sonar\nMistral AI: mistralai--mistral-large-instruct\nMistral AI: mistralai--mistral-small-instruct\nMistral AI: mistralai--mistral-medium-instruct\nAmazon: amazon--nova-pro\nAmazon: amazon--nova-lite\nAmazon: amazon--nova-micro\nAnthropic: anthropic--claude-3-haiku\nAnthropic: anthropic--claude-3.5-sonnet\nAnthropic: anthropic--claude-3.7-sonnet\nAnthropic: anthropic--claude-4-sonnet\nAnthropic: anthropic--claude-4.5-sonnet\nAnthropic: anthropic--claude-4.5-opus\nAnthropic: anthropic--claude-4.5-haiku"
  },
  {
    "objectID": "posts/2026-01-29-sap-orchestration-hello-world/index.html#conclusion",
    "href": "posts/2026-01-29-sap-orchestration-hello-world/index.html#conclusion",
    "title": "Building a Harmonized-API ‘Hello World’ with AI Core",
    "section": "Conclusion",
    "text": "Conclusion\nAfter installing the SAP Cloud SDK for AI and setting up authentication, we managed to talk to the Orchestration Service via the Harmonized API with just a few lines of code.\nThe key advantage over the model-specific chat completion API is that you can swap models from different vendors(!) by simply changing a string, no code changes required. Whether it’s gpt-4o, anthropic--claude-3.5-sonnet, or gemini-2.5-flash, the same code just works. This opens up easy benchmarking across model families, A/B testing, and even building redundancy into your applications.\nWe also discovered another benefit: You don’t need to manage deployments yourself. Instead, SAP handles all model deployments centrally. This makes life easy for you as a developer: No need to coordinate with your admin or wait for provisioning. You can experiment with new models the moment they’re available. 🤓\nWith this “Hello World”, you now have working code to start experimenting with. Try it out for yourself. Happy coding!"
  },
  {
    "objectID": "posts/2024-02-23-chat-from-scratch/index.html",
    "href": "posts/2024-02-23-chat-from-scratch/index.html",
    "title": "Building Chat for Jupyter Notebooks from Scratch",
    "section": "",
    "text": "Let’s build a light-weight chat for llama2 from scratch which can be reused in your Jupyter notebooks.\nWorking exploratively with large language models (LLMs), I wanted to not only send prompts to LLMs, but to chat with the LLM from within my Jupyter notebook. It turns out, that building a chat from scratch is not complicated. While I use a local llama2 model in this notebook, the concepts I describe are universal and also transfer to other implementations.\nBefore we get started: If you want to interactively run through this blog post, please check out the corresponding Jupyter notebook."
  },
  {
    "objectID": "posts/2024-02-23-chat-from-scratch/index.html#building-chat-messages",
    "href": "posts/2024-02-23-chat-from-scratch/index.html#building-chat-messages",
    "title": "Building Chat for Jupyter Notebooks from Scratch",
    "section": "Building Chat Messages",
    "text": "Building Chat Messages\n“Chat Messages” are the messages you exchange with the LLM. There are 3 roles:\n\nThe system-message gives overall instructions to the LLM which should be used during the whole chat. It only appears once.\nUser-messages are the messages (“prompts”) you send to the LLM.\nThe LLM replies with assistant-messages.\n\nThe chat messages need to be passed to the LLM via the API in a JSON format. Since the chat is stateless, you need to pass the whole previous conversation to be able to ask follow-up questions. The following animated GIF shows the structure in analogy to ChatGPT. Additionally, it animates how the messages build up over time during the chat.\n\n\n\nChat Messages - Please click on the image to start the animation.\n\n\nBased on that, let’s build a class which contains and manages the chat messages:\n\nclass ChatMessages:\n\n    def __init__(self):\n        \"\"\"Initializes the Chat.\"\"\"\n        self._messages = []\n\n    def _append_message(self, role, content):\n        \"\"\"Appends a message with specified role and content to messages list.\"\"\"\n        self._messages.append({\"role\": role, \"content\": content})\n\n    def append_system_message(self, content):\n        \"\"\"Appends a system message with specified content to messages list.\"\"\"\n        self._append_message(\"system\", content)\n\n    def append_user_message(self, content):\n        \"\"\"Appends a user message with specified content to messages list.\"\"\"\n        self._append_message(\"user\", content)\n\n    def append_assistant_message(self, content):\n        \"\"\"Appends an assistant message with specified content to messages list.\"\"\"\n        self._append_message(\"assistant\", content)\n    \n    def get_messages(self):\n        \"\"\"Returns a shallow copy of the messages list.\"\"\"\n        return self._messages[:]\n\nThe 3 methods append_system_message, append_user_message, and append_assistant_message call the private method _append_message so that there is no confusion as to which message types can be added. The get_messages-method returns a copy of the chat messages, making sure that it is not possible to access the private _messages attribute of ChatMessages.\nLet’s quickly re-create the example shown on the image above:\n\nchat_messages = ChatMessages()\nchat_messages.append_system_message(\"For \\\"Question n\\\", please respond with \\\"Response n\\\"\")\nchat_messages.append_user_message(\"Question 1\")\nchat_messages.append_assistant_message(\"Response 1\")\nchat_messages.append_user_message(\"Question 2\")\nchat_messages.append_assistant_message(\"Response 2\")\n\n\nimport json\nfrom pygments import highlight\nfrom pygments.lexers import JsonLexer\nfrom pygments.formatters import TerminalFormatter\n\n# Convert messages to a formatted JSON string\njson_str = json.dumps(chat_messages.get_messages(), indent=4)\n\n# Highlight the JSON string to add colors\nprint(highlight(json_str, JsonLexer(), TerminalFormatter()))\n\n\n[\n\n    {\n\n        \"role\": \"system\",\n\n        \"content\": \"For \\\"Question n\\\", please respond with \\\"Response n\\\"\"\n\n    },\n\n    {\n\n        \"role\": \"user\",\n\n        \"content\": \"Question 1\"\n\n    },\n\n    {\n\n        \"role\": \"assistant\",\n\n        \"content\": \"Response 1\"\n\n    },\n\n    {\n\n        \"role\": \"user\",\n\n        \"content\": \"Question 2\"\n\n    },\n\n    {\n\n        \"role\": \"assistant\",\n\n        \"content\": \"Response 2\"\n\n    }\n\n]"
  },
  {
    "objectID": "posts/2024-02-23-chat-from-scratch/index.html#building-chat-version-1",
    "href": "posts/2024-02-23-chat-from-scratch/index.html#building-chat-version-1",
    "title": "Building Chat for Jupyter Notebooks from Scratch",
    "section": "Building Chat Version 1",
    "text": "Building Chat Version 1\nWe have seen in my previous blog post how we can prompt the llama2 model by calling the create_chat_completion-method.\nThe class Llama2ChatVersion2 simplifies prompting the LLM by doing the following:\n\nUpon initialization of the chat object, the chat messages are initialized and the system message is added.\nWhe prompting llama2, both the prompt (the user message) and the response (the assistant message) are added to the chat messages.\nThe plain text returned from llama2 is formatted for better readability.\n\n\nfrom IPython.display import Markdown, clear_output\n\nclass Llama2ChatVersion2:\n\n    def __init__(self, llm, system_message):\n        \"\"\"Initializes the Chat with the system message.\"\"\"\n        self._llm = llm\n        self._chat_messages = ChatMessages()\n        self._chat_messages.append_system_message(system_message)\n\n    def _get_llama2_response(self):\n        \"\"\"Returns Llama2 model response for given messages.\"\"\"\n        self.model_response = self._llm.create_chat_completion(self._chat_messages.get_messages())\n        return self.model_response['choices'][0]['message']['content']\n\n    def _format_markdown_with_style(self, text, font_size=16):\n        \"\"\"Wraps text in a &lt;span&gt; with specified font size, defaults to 16.\"\"\"\n        return f\"&lt;span style='font-size: {font_size}px;'&gt;{text}&lt;/span&gt;\"\n\n    def prompt_llama2(self, user_prompt):\n        \"\"\"Processes user prompt, displays Llama2 response formatted in Markdown.\"\"\"\n        self._chat_messages.append_user_message(user_prompt)\n        llama2_response = self._get_llama2_response()\n        self._chat_messages.append_assistant_message(llama2_response)\n        display(Markdown(self._format_markdown_with_style(llama2_response)))\n\nLet’s create a llama2 instance and prompt it:\n\nfrom llama_cpp import Llama\n#llm = Llama(model_path=\"../models/Llama-2-7b-chat/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=2048, verbose=False)\nllm = Llama(model_path=\"../../../lm-hackers/models/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=2048, verbose=False)\n\n\nchat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\nchat.prompt_llama2(\"Name the planets in the solar system\")\n\n Sure! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\n\nMercury\nVenus\nEarth\nMars\nJupiter\nSaturn\nUranus\nNeptune\n\n\n\nThe result looks good, but from a useability perspective it is not great, because you have to wait for the whole response to be completed before you see an output. On my machine this takes a few seconds for this short reply. Especially for longer answers, however, you might start to wonder if the process is running correctly, or is it just the impatient me?\nIn any case, it would be nice to see the model’s response streamed, i.e. you see the model writing the answer world-by-word / token-by-token, the same way you are used to seeing ChatGPT print out its answers."
  },
  {
    "objectID": "posts/2024-02-23-chat-from-scratch/index.html#how-streaming-works",
    "href": "posts/2024-02-23-chat-from-scratch/index.html#how-streaming-works",
    "title": "Building Chat for Jupyter Notebooks from Scratch",
    "section": "How Streaming Works",
    "text": "How Streaming Works\nThe following cell contains an illustrative example of how streaming works: The function mock_llm_stream() returns a generator object because it does not return a result, but it yields results. This means that the code is not executed when the function is called, but it only returns a generator object which lazily returns values when it is iterated over. The for-loop iterates over the generator object, and each iteration returns a word after some latency, simulating the token generation by the LLM.\n\nimport time\n\ndef fetch_next_word(words, current_index):\n    \"\"\"\n    Mock function to simulate making an API call to fetch the next word from the LLM.\n    \"\"\"\n    # Simulate network delay or processing time\n    time.sleep(0.5)\n    \n    if current_index &lt; len(words):\n        return words[current_index]\n    else:\n        raise StopIteration(\"End of sentence reached.\")\n\ndef mock_llm_stream():\n    sentence = \"This is an example for a text streamed via a generator object.\"\n    words = sentence.split()\n    current_index = 0\n    \n    while True:\n        try:\n            # Simulate fetching the next word from the LLM\n            word = fetch_next_word(words, current_index)\n            yield word\n            current_index += 1\n        except StopIteration:\n            break\n\n# Capture the generator function in a variable\nmock_llm_response = mock_llm_stream()\n\n# Example of how to use this mock_llm_response\nfor word in mock_llm_response:\n    print(word, end=\" \")\n\nThis is an example for a text streamed via a generator object. \n\n\nSo let’s stream a response from llama2:\n\nmessages=[\n    {\"role\": \"system\", \"content\": \"Answer in a very concise and accurate way\"},\n    {\"role\": \"user\", \"content\": \"Name the planets in the solar system\"}]\n\nmodel_response = llm.create_chat_completion(messages = messages, stream=True)\n\ncomplete_response = \"\"\n\nfor part in model_response:\n    # Check if 'content' key exists in the 'delta' dictionary\n    if 'content' in part['choices'][0]['delta']:\n        content = part['choices'][0]['delta']['content']\n        print(content, end='')\n        complete_response += content\n    else:\n        # Handle the case where 'content' key is not present\n        # For example, you can print a placeholder or do nothing\n        # print(\"(no content)\", end='')\n        pass\n\n  Sure! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\n\n1. Mercury\n2. Venus\n3. Earth\n4. Mars\n5. Jupiter\n6. Saturn\n7. Uranus\n8. Neptune"
  },
  {
    "objectID": "posts/2024-02-23-chat-from-scratch/index.html#chat-version-2---streaming-included",
    "href": "posts/2024-02-23-chat-from-scratch/index.html#chat-version-2---streaming-included",
    "title": "Building Chat for Jupyter Notebooks from Scratch",
    "section": "Chat Version 2 - Streaming included",
    "text": "Chat Version 2 - Streaming included\nLet’s include the streaming functionality into our chat messages- and chat-classes. For this we are going to use a nice trick from fastcore to add the 2 new methods: We can @patch the methods into the class:\n\nfrom fastcore.utils import * #for importing patch\n\n\n@patch    \ndef _get_llama2_response_stream(self:Llama2ChatVersion2):\n    \"\"\"Returns generator object for streaming Llama2 model responses for given messages.\"\"\"\n    return self._llm.create_chat_completion(self._chat_messages.get_messages(), stream=True)\n\n@patch\ndef prompt_llama2_stream(self:Llama2ChatVersion2, user_prompt):\n    \"\"\"Processes user prompt in streaming mode, displays updates in Markdown.\"\"\"\n    self._chat_messages.append_user_message(user_prompt)\n    llama2_response_stream = self._get_llama2_response_stream()\n    \n    complete_stream = \"\"\n\n    for part in llama2_response_stream:\n        # Check if 'content' key exists in the 'delta' dictionary\n        if 'content' in part['choices'][0]['delta']:\n            stream_content = part['choices'][0]['delta']['content']\n            complete_stream += stream_content\n\n            # Clear previous output and display new content\n            clear_output(wait=True)\n            display(Markdown(self._format_markdown_with_style(complete_stream)))\n\n        else:\n            # Handle the case where 'content' key is not present\n            pass\n    \n    self._chat_messages.append_assistant_message(complete_stream)\n\nNow we can use the method prompt_llama2_stream to get a more interactive response:\n\nchat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\nchat.prompt_llama2_stream(\"Name the planets in the solar system\")\n\n Sure! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\n\nMercury\nVenus\nEarth\nMars\nJupiter\nSaturn\nUranus\nNeptune\n\n\n\nJust for the fun of it, let’s continue the chat:\n\nchat.prompt_llama2_stream(\"Please reverse the list\")\n\n Of course! Here are the names of the planets in our solar system in reverse order, from farthest to closest to the Sun: 1. Neptune 2. Uranus 3. Saturn 4. Jupiter 5. Mars 6. Earth 7. Venus 8. Mercury\n\n\n\nchat.prompt_llama2_stream(\"Please sort the list by the mass of the planet\")\n\n Sure! Here are the names of the planets in our solar system sorted by their mass, from lowest to highest: 1. Mercury (0.38 Earth masses) 2. Mars (0.11 Earth masses) 3. Venus (0.81 Earth masses) 4. Earth (1.00 Earth masses) 5. Jupiter (29.6 Earth masses) 6. Saturn (95.1 Earth masses) 7. Uranus (14.5 Earth masses) 8. Neptune (17.1 Earth masses)\n\n\nLooping back to the beginning, you can see how the chat is represented in the chat massages.\n\nchat._chat_messages.get_messages()\n\n[{'role': 'system', 'content': 'Answer in a very concise and accurate way'},\n {'role': 'user', 'content': 'Name the planets in the solar system'},\n {'role': 'assistant',\n  'content': '  Sure! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\\n\\n1. Mercury\\n2. Venus\\n3. Earth\\n4. Mars\\n5. Jupiter\\n6. Saturn\\n7. Uranus\\n8. Neptune'},\n {'role': 'user', 'content': 'Please reverse the list'},\n {'role': 'assistant',\n  'content': '  Of course! Here are the names of the planets in our solar system in reverse order, from farthest to closest to the Sun:\\n1. Neptune\\n2. Uranus\\n3. Saturn\\n4. Jupiter\\n5. Mars\\n6. Earth\\n7. Venus\\n8. Mercury'},\n {'role': 'user', 'content': 'Please sort the list by the mass of the planet'},\n {'role': 'assistant',\n  'content': '  Sure! Here are the names of the planets in our solar system sorted by their mass, from lowest to highest:\\n1. Mercury (0.38 Earth masses)\\n2. Mars (0.11 Earth masses)\\n3. Venus (0.81 Earth masses)\\n4. Earth (1.00 Earth masses)\\n5. Jupiter (29.6 Earth masses)\\n6. Saturn (95.1 Earth masses)\\n7. Uranus (14.5 Earth masses)\\n8. Neptune (17.1 Earth masses)'}]"
  },
  {
    "objectID": "posts/2024-02-23-chat-from-scratch/index.html#conclusion",
    "href": "posts/2024-02-23-chat-from-scratch/index.html#conclusion",
    "title": "Building Chat for Jupyter Notebooks from Scratch",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we implemented an LLM chat from scratch in a very light-weight format. We learned how the chat messages need to be handled to create the chat experience and we even added streaming support.\nAnd the best thing, we can re-use this chat functionality in other notebooks without having to re-write it or copy&paste again, keeping our notebooks dry and clean. I have moved the core code of this notebook to a separate .py-file, and this notebook demonstrates how to re-use the notebook chat in another notebook. 😀"
  },
  {
    "objectID": "posts/2024-02-16-c-binding-example/index.html",
    "href": "posts/2024-02-16-c-binding-example/index.html",
    "title": "How to Implement a C Binding in Python with ctypes",
    "section": "",
    "text": "How can compute intensive large language models (LLMs) run on consumer-grade laptops? C bindings are part of this magic, they create wrappers around the C code to make is accessible in higher-level languages like Python. While it might sound complicated, the concept is surprisingly accessible with the right approach. Let’s explore a simple example to how to utilize the ctypes library to implement a C bindings in Python.\nIn my previous blog post, I demonstrated how you can use llama-cpp-python to run a llama2-model using llama.cpp. To understand how the interface between these 2 project works, I created a simple C library to unveil some of the underlying “magic”.\nThe Python ctypes-library is the bridge between the to worlds, and in this notebook, we first create and compile a simple function written in C that accepts an int32_t value and returns its square. Subsequently, we use this function from python to learn how to implement the C binding via the ctypes-library.\nWhile I wrote this notebook on macOS, the principles and techniques are universally applicable, with slight adjustments for Linux or Windows environments.\nA final note before we get started: You can find the notebook version of this blog post on my GitHub."
  },
  {
    "objectID": "posts/2024-02-16-c-binding-example/index.html#step-1-create-the-c-code-library",
    "href": "posts/2024-02-16-c-binding-example/index.html#step-1-create-the-c-code-library",
    "title": "How to Implement a C Binding in Python with ctypes",
    "section": "Step 1: Create the C Code Library",
    "text": "Step 1: Create the C Code Library\nCreate a file named example.c with the following content:\n#include &lt;stdint.h&gt;\n\nint32_t square(int32_t number) {\n    return number * number;\n}\nNext, compile this C code into a shared library via the terminal\n# Linux / so -&gt; shared object\ngcc -shared -fpic -o libexample.so example.c\n# macOS / dylib -&gt; dynamic library\ngcc -shared -fpic -o libexample.dylib example.c\n# Windows / dll -&gt; dynamic-link library\ngcc -shared -o example.dll example.c\nBefore we run the command, let’s break it down:\n\ngcc stands for “GNU Compiler Collection”, and it can compile C by default.\n-shared creates a “shared library”. In Python analogy, this is like a module which can be imported.\n-fpic creates “Position-Independent Code” (PIC), removing any absolute memory references and making them relative.\nFor C developers, it is good practice to prefix the name of shared libraries (dynamic libraries) with “lib”, hence example.c becomes libexample.xxx\n\nSince I am running on a Mac, I use the following command to compile my library:\n\n!gcc -shared -fpic -o libexample.dylib example.c\n\nAs a result, I get a new file called libexample.dylib."
  },
  {
    "objectID": "posts/2024-02-16-c-binding-example/index.html#step-2-python-code",
    "href": "posts/2024-02-16-c-binding-example/index.html#step-2-python-code",
    "title": "How to Implement a C Binding in Python with ctypes",
    "section": "Step 2: Python Code",
    "text": "Step 2: Python Code\nTo call this function from Python, we need to do couple of steps.\nFirst, we need to load the shared libaray via the ctypes.CDLL-method to access our square function:\n\nimport ctypes\n\nlibexample = ctypes.CDLL('./libexample.dylib')  # Use appropriate file name on your system\n\nNext, let’s create an object which represents a 32-bit integer which corresponds to the C type int32_t. This is the type we used in our example C code.\n\nc_int32_type = ctypes.c_int32\n\nPreparing for the call to C, we need to specify the arguments and return type of the C function so that the variables can be converted correctly:\n\nThe arguments argtypes are passed in a list, because there could be more arguments (depending on the function).\nThe result type restype is just a single value, because a function returns exactly one result.\n\n\nlibexample.square.argtypes = [c_int32_type]\nlibexample.square.restype = c_int32_type\n\nWe want to calculate the square of a number_to_be_squared. This Python variable first needs to be converted into a proper int32 representation:\n\nnumber_to_be_squared = 7\ninput_value = c_int32_type(number_to_be_squared)\n\nFinally, we can call the C function:\n\nresult = libexample.square(input_value)\nprint(f\"The square of {input_value.value} is {result}.\")\n\nThe square of 7 is 49.\n\n\nNote that the input type is a C type, therefore we need to use .value to access the Python equivalent, and the result is automatically converted to a Python type:\n\nprint(f\"The input value type is {type(input_value)}\")\nprint(f\"The result value type is {type(result)}\")\n\nThe input value type is &lt;class 'ctypes.c_int'&gt;\nThe result value type is &lt;class 'int'&gt;"
  },
  {
    "objectID": "posts/2024-02-16-c-binding-example/index.html#wrapping-up",
    "href": "posts/2024-02-16-c-binding-example/index.html#wrapping-up",
    "title": "How to Implement a C Binding in Python with ctypes",
    "section": "Wrapping up",
    "text": "Wrapping up\nThis tiny example demonstrated how a C binding works and which steps are needed to call C code from Python. Although aligning the types between Python and C requires some effort, the payoff is significantly enhanced performance for compute-intensive tasks like neural net inference. While introducing additional complexity, the C binding llama-cpp-python makes it possible to run a llama2-model via llama.cpp directly from Python, even on a consumer laptop."
  },
  {
    "objectID": "posts/2025-06-01-the-million-token-question/index.html",
    "href": "posts/2025-06-01-the-million-token-question/index.html",
    "title": "The Million-Token Question: What the Bible Teaches Us About LLM Pricing",
    "section": "",
    "text": "In my previous blog post, I explored how different languages compare in terms of tokens per word. That analysis was based on Wikipedia articles and raised a few follow-up questions. In this blog post, let’s continue to explore tokenization to gain a more intuitive understanding and derive some real-world implications:"
  },
  {
    "objectID": "posts/2025-06-01-the-million-token-question/index.html#the-plan-for-this-post",
    "href": "posts/2025-06-01-the-million-token-question/index.html#the-plan-for-this-post",
    "title": "The Million-Token Question: What the Bible Teaches Us About LLM Pricing",
    "section": "The plan for this post",
    "text": "The plan for this post\nWith a bit of luck, I discovered that the content of the German Bible is almost the exact equivalent of one million tokens (using GPT-4o tokenizer o200k_base). Using this insight as our starting point, let’s tokenize several Bible translations. Here are the steps we’ll follow:\n\nDownload the Bibles and clean-up the content so that it only contains the plain text.\nCount words and tokens\nCalculate some interesting KPIs\n\nAs usual, this blog post is also available as a Jupyter Notebook, and you can run all the code yourself."
  },
  {
    "objectID": "posts/2025-06-01-the-million-token-question/index.html#reuse-some-code",
    "href": "posts/2025-06-01-the-million-token-question/index.html#reuse-some-code",
    "title": "The Million-Token Question: What the Bible Teaches Us About LLM Pricing",
    "section": "Reuse some code",
    "text": "Reuse some code\nLet’s start by defining some helper functions, which we will use later in the process. Please feel free to skip over this section if you are mainly interested in the results.\nThe code assumes that you have previously installed spaCy alongside the necessary language packages. In case you haven’t, please check out my previous blog post which explains how to install it.\n\n\nCode\nimport tiktoken\n\ndef get_encoder(encoding_name=\"o200k_base\"):\n    \"\"\"Returns a tiktoken encoder. Defaults to GPT-4/GPT-4o's tokenizer.\"\"\"\n    return tiktoken.get_encoding(encoding_name)\n\ndef count_tokens(text: str, encoder=None) -&gt; int:\n    \"\"\"\n    Counts the number of tokens in the input text using the specified encoder.\n    If no encoder is provided, a new one will be created.\n    \"\"\"\n    if encoder is None:\n        encoder = get_encoder()\n    return len(encoder.encode(text))\n\nencoder = get_encoder()\n\n\n\n\nCode\nimport spacy\nimport string\n\nLANGUAGES = {\n    \"de\": {\"name\": \"German\",      \"model\": \"de_core_news_sm\",    \"emoji\": \"🇩🇪\"},\n    \"en\": {\"name\": \"English\",     \"model\": \"en_core_web_sm\",     \"emoji\": \"🇺🇸\"},\n    \"es\": {\"name\": \"Spanish\",     \"model\": \"es_core_news_sm\",    \"emoji\": \"🇪🇸\"},\n    \"fr\": {\"name\": \"French\",      \"model\": \"fr_core_news_sm\",    \"emoji\": \"🇫🇷\"},\n    \"it\": {\"name\": \"Italian\",     \"model\": \"it_core_news_sm\",    \"emoji\": \"🇮🇹\"},\n    \"ja\": {\"name\": \"Japanese\",    \"model\": \"ja_core_news_sm\",    \"emoji\": \"🇯🇵\"},\n    \"ko\": {\"name\": \"Korean\",      \"model\": \"ko_core_news_sm\",    \"emoji\": \"🇰🇷\"},\n    \"pl\": {\"name\": \"Polish\",      \"model\": \"pl_core_news_sm\",    \"emoji\": \"🇵🇱\"},\n    \"pt\": {\"name\": \"Portuguese\",  \"model\": \"pt_core_news_sm\",    \"emoji\": \"🇵🇹\"},\n    \"ru\": {\"name\": \"Russian\",     \"model\": \"ru_core_news_sm\",    \"emoji\": \"🇷🇺\"},\n    \"zh\": {\"name\": \"Chinese\",     \"model\": \"zh_core_web_sm\",     \"emoji\": \"🇨🇳\"},\n}\n\n# Simple cache/dictionary to hold loaded spaCy models:\n_spacy_models = {}\n\ndef get_spacy_model(language_code: str = \"en\"):\n    \"\"\"\n    Loads and caches the spaCy language model for the given language code.\n    Uses the model name defined in the LANGUAGES dict.\n    Falls back to a blank model if the specified model is not available.\n    \"\"\"\n    if language_code not in _spacy_models:\n        model_name = LANGUAGES.get(language_code, {}).get(\"model\", None)\n        try:\n            if model_name:\n                _spacy_models[language_code] = spacy.load(model_name)\n            else:\n                raise ValueError(f\"No model defined for language code: '{language_code}'\")\n        except (OSError, ValueError) as e:\n            print(f\"⚠️ Could not load model '{model_name}' for language '{language_code}': {e}\")\n            print(\"→ Falling back to blank spaCy model (basic tokenization only).\")\n            _spacy_models[language_code] = spacy.blank(language_code)\n    return _spacy_models[language_code]\n\ndef get_spacy_tokens(text: str, language_code: str = \"en\") -&gt; tuple[list[str], list[str]]:\n    \"\"\"\n    Tokenizes the input text using spaCy's tokenizer.\n    Returns two lists: one with spaCy tokens (words) and one with omitted tokens \n    (punctuation, spaces, symbols, etc.).\n    \"\"\"\n    nlp = get_spacy_model(language_code)\n    doc = nlp(text)\n    \n    punctuation_set = set(string.punctuation)\n    \n    word_tokens = [\n        t for t in doc \n        if not t.is_space \n           and not t.is_punct \n           and t.pos_ != \"SYM\" \n           and t.text not in punctuation_set\n    ]\n    omitted_tokens = [\n        t for t in doc \n        if t.is_space \n           or t.is_punct \n           or t.pos_ == \"SYM\" \n           or t.text in punctuation_set\n    ]\n    \n    return word_tokens, omitted_tokens\n\ndef count_words_spacy(text: str, language_code: str = \"en\") -&gt; int:\n    \"\"\"\n    Counts words in the input text using spaCy's tokenizer.\n    Skips punctuation/whitespace tokens.\n    \"\"\"\n    nlp = get_spacy_model(language_code)\n    doc = nlp(text)\n    punctuation_set = set(string.punctuation)\n    \n    # Filter out space/punctuation tokens:\n    tokens = [\n        t for t in doc \n        if not t.is_space \n           and not t.is_punct \n           and t.pos_ != \"SYM\" \n           and t.text not in punctuation_set\n    ]\n    return len(tokens)\n\ndef get_tokens_per_word(text: str, language_code: str = \"en\", encoder=None) -&gt; float:\n    \"\"\"\n    Calculates average number of tokens (tiktoken) per word (spaCy-based) for the given text.\n    \"\"\"\n    words = count_words_spacy(text, language_code=language_code)\n    tokens = count_tokens(text, encoder=encoder)\n    \n    if words == 0:\n        return 0.0\n    return tokens / words\n\n\n\n\nCode\ndef count_words_spacy_long(text, language_code=\"en\", chunk_size=1000000):\n    \"\"\"\n    Counts words in large text by splitting it into chunks and using count_words_spacy.\n\n    Parameters:\n    text (str): The full text to be analyzed.\n    language_code (str): Language code to pass to count_words_spacy.\n    chunk_size (int): Size of each text chunk in characters (default: 1,000,000).\n\n    Returns:\n    int: Total word count.\n    \"\"\"\n    total_word_count = 0\n    for i in range(0, len(text), chunk_size):\n        chunk = text[i:i + chunk_size]\n        word_count = count_words_spacy(text=chunk, language_code=language_code)\n        total_word_count += word_count\n    return total_word_count\n\n\n\n\nCode\ndef read_text_file(file_path):\n    \"\"\"\n    Reads the full content of a plain text file.\n\n    Parameters:\n    file_path (str): Path to the text file.\n\n    Returns:\n    str: The content of the file as a single string.\n    \"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n        return text\n    except FileNotFoundError:\n        print(f\"File not found: {file_path}\")\n        return \"\"\n    except UnicodeDecodeError:\n        print(\"Error decoding file. Try using a different encoding, like 'latin-1'.\")\n        return \"\"\n\n\ndef analyze_text(text: str, language_code: str = \"en\") -&gt; dict:\n    \"\"\"\n    Analyzes the input text using spaCy and returns a dictionary with word count and token count.\n    \"\"\"\n    word_count = count_words_spacy_long(text, language_code=language_code)\n    token_count = count_tokens(text, encoder=encoder)\n    token_per_word = token_count / word_count\n    return {\"word_count\": word_count, \"token_count\": token_count, \"token_per_word\": token_per_word}"
  },
  {
    "objectID": "posts/2025-06-01-the-million-token-question/index.html#tokenizing-the-bible",
    "href": "posts/2025-06-01-the-million-token-question/index.html#tokenizing-the-bible",
    "title": "The Million-Token Question: What the Bible Teaches Us About LLM Pricing",
    "section": "Tokenizing the bible",
    "text": "Tokenizing the bible\nFinding the full text of the bible in many languages was a bit tricky but Bible Super Search provides full downloads in many languages. Bible texts usually contain verse numbers, which could affect the word count. Therefore, I opted for the CSV versions, which contains several columns (Verse ID, Book Name, Book Number, Chapter, Verse, Text). This way I could extract only the raw text into separate text files.\nThe site offered more than one version for some languages. With ChatGPT’s help, I selected the most mainstream translations:\n\nChinese: Chinese Union (Simplified)\nEnglish: American Standard Version\nFrench: Louis Segond 1910\nGerman: Luther Bible (1912)\nItalian: Diodati\nKorean: Korean\nPolish: Uwspółcześniona Biblia Gdańska\nPortuguese: Tradução de João Ferreira de Almeida (Versão Revista e Atualizada)\nRussian: Synodal\nSpanish: Reina Valera 1909\n\n\n\nCode\nimport os\n\ndef get_filenames_by_extension(extension: str) -&gt; list[str]:\n    \"\"\"\n    Returns a list of filenames with the specified extension.\n    \"\"\"\n    return [f for f in os.listdir('.') if f.endswith(extension)]\n\ncsv_filenames = get_filenames_by_extension('.csv')\ncsv_filenames\n\n\n['zh_chinese_union_simp.csv',\n 'pt_almeida_ra.csv',\n 'ru_synodal.csv',\n 'pl_pol_ubg.csv',\n 'es_1909.csv',\n 'it_diodati.csv',\n 'fr_segond_1910.csv',\n 'de_luther_1912.csv',\n 'en_asv.csv',\n 'ko_korean.csv']\n\n\nAfter the download, I converted the files to plain text:\n\n\nCode\nimport csv\nfrom pathlib import Path\n\ndef csv_to_plain_text(input_csv: str, output_txt: str, text_column: str = \"Text\",\n                      encoding: str = \"utf-8\") -&gt; None:\n    \"\"\"\n    Convert a Bible CSV into a plain text file with one verse per line,\n    skipping preamble lines before the actual header.\n    \"\"\"\n    input_path = Path(input_csv)\n    output_path = Path(output_txt)\n\n    with input_path.open(mode=\"r\", encoding=encoding, newline='') as infile:\n        # Read all lines and search for the header row\n        lines = infile.readlines()\n        header_line_idx = None\n\n        for i, line in enumerate(lines):\n            # Try parsing this line as a CSV header\n            headers = [col.strip() for col in line.split(',')]\n            if text_column in headers:\n                header_line_idx = i\n                break\n\n        if header_line_idx is None:\n            raise ValueError(f\"Could not find a header line containing '{text_column}' in file {input_csv}\")\n\n        # Rewind file starting from the header line\n        valid_csv = lines[header_line_idx:]\n\n        reader = csv.DictReader(valid_csv)\n        if text_column not in reader.fieldnames:\n            raise KeyError(f\"Column '{text_column}' not found in CSV header: {reader.fieldnames}\")\n\n        with output_path.open(mode=\"w\", encoding=encoding, newline='\\n') as outfile:\n            for row in reader:\n                text = row[text_column].strip()\n                outfile.write(text + \"\\n\")   # one verse per line\n\n    print(f\"Saved file: {output_path.name}\")\n\ndef get_text_file_name(filename: str) -&gt; str:\n    \"\"\"\n    Given a filename, returns the same filename with a .txt extension.\n    Example: \"data.csv\" -&gt; \"data.txt\"\n    \"\"\"\n    return str(Path(filename).with_suffix('.txt'))\n\nfor csv_filename in csv_filenames:\n    csv_to_plain_text(csv_filename, get_text_file_name(csv_filename))\n\n\nSaved file: zh_chinese_union_simp.txt\nSaved file: pt_almeida_ra.txt\nSaved file: ru_synodal.txt\nSaved file: pl_pol_ubg.txt\nSaved file: es_1909.txt\nSaved file: it_diodati.txt\nSaved file: fr_segond_1910.txt\nSaved file: de_luther_1912.txt\nSaved file: en_asv.txt\nSaved file: ko_korean.txt"
  },
  {
    "objectID": "posts/2025-06-01-the-million-token-question/index.html#analyzing-bible-texts",
    "href": "posts/2025-06-01-the-million-token-question/index.html#analyzing-bible-texts",
    "title": "The Million-Token Question: What the Bible Teaches Us About LLM Pricing",
    "section": "Analyzing Bible Texts",
    "text": "Analyzing Bible Texts\nWe’ve completed all the preparation steps and can now start analyzing the texts. Let’s count both the tokens and the words to determine the tokens per word. Additionally, let’s normalize the tokens per word and the total number of tokens to English to not only see the tokens per word, but also the relative number of tokens per bible version.\n\n\nCode\ndef analyze_all_text_files(extension: str = '.txt') -&gt; list[dict]:\n    \"\"\"\n    Analyzes all text files with the given extension in the current directory.\n\n    Returns a list of dictionaries, each containing:\n    - language (derived from filename)\n    - filename\n    - word count\n    - token count\n    - tokens per word\n    \"\"\"\n    results = []\n    txt_filenames = get_filenames_by_extension(extension)\n\n    for txt_filename in txt_filenames:\n        print(f\"Processing {txt_filename}...\")  # progress indicator\n\n        text = read_text_file(txt_filename)\n        language = txt_filename[:2].lower()\n        metrics = analyze_text(text, language_code=language)\n\n        print(f\"Done: {metrics}\")  # show results briefly\n\n        result = {\n            \"language\": language,\n            \"filename\": txt_filename,\n            \"word_count\": metrics[\"word_count\"],\n            \"token_count\": metrics[\"token_count\"],\n            \"tokens_per_word\": metrics[\"token_per_word\"]\n        }\n\n        results.append(result)\n\n    return results\n\nresults = analyze_all_text_files()\n\n\nLet’s visualize the results:\n\n\nCode\nimport pandas as pd\n\ndef get_tokenization_dataframe(results: list[dict]) -&gt; pd.DataFrame:\n    \"\"\"\n    Converts tokenization results into a pandas DataFrame with:\n    - Flag\n    - ISO code\n    - Language name\n    - Word count\n    - Token count\n    - Tokens per word\n    - Tokens/Word relative to English\n    - Total tokens as % of English tokens\n\n    Sorted ascending by Tokens/Word.\n    \"\"\"\n    # Use metadata from the shared LANGUAGES dictionary\n    def get_lang_info(code):\n        entry = LANGUAGES.get(code, {})\n        return entry.get(\"emoji\", \"🏳️\"), entry.get(\"name\", \"Unknown\")\n\n    # Get English baseline values\n    english_entry = next((entry for entry in results if entry[\"language\"] == \"en\"), None)\n    if not english_entry:\n        raise ValueError(\"English ('en') entry not found in results.\")\n\n    english_tokens = english_entry[\"token_count\"]\n    english_tpw = english_entry[\"tokens_per_word\"]\n\n    rows = []\n    for entry in results:\n        lang_code = entry[\"language\"]\n        flag, language = get_lang_info(lang_code)\n\n        tokens = entry[\"token_count\"]\n        tpw = entry[\"tokens_per_word\"]\n\n        rel_tpw = tpw / english_tpw\n        percent_of_english = (tokens / english_tokens) * 100\n\n        rows.append({\n            \"Flag\": flag,\n            \"Code\": lang_code,\n            \"Language\": language,\n            \"Words\": entry[\"word_count\"],\n            \"Tokens\": tokens,\n            \"Tokens/Word\": round(tpw, 3),\n            \"Rel. Tokens/Word (vs EN)\": round(rel_tpw, 2),\n            \"% of English Tokens\": round(percent_of_english, 1),\n        })\n\n    df = pd.DataFrame(rows)\n    df = df.sort_values(by=\"Tokens/Word\", ascending=True).reset_index(drop=True)\n    return df\n\ndef display_tokenization_table(df: pd.DataFrame) -&gt; None:\n    styled = df.style.format({\n        \"Tokens/Word\": \"{:.3f}\",\n        \"Rel. Tokens/Word (vs EN)\": \"{:.2f}\",\n        \"% of English Tokens\": \"{:.1f}\"\n    }).hide(axis=\"index\")\n    display(styled)\n\ndf = get_tokenization_dataframe(results)\ndisplay_tokenization_table(df)\n\n\n\n\n\n\n\nFlag\nCode\nLanguage\nWords\nTokens\nTokens/Word\nRel. Tokens/Word (vs EN)\n% of English Tokens\n\n\n\n\n🇺🇸\nen\nEnglish\n789712\n997707\n1.263\n1.00\n100.0\n\n\n🇫🇷\nfr\nFrench\n777811\n1122594\n1.443\n1.14\n112.5\n\n\n🇪🇸\nes\nSpanish\n700895\n1027817\n1.466\n1.16\n103.0\n\n\n🇵🇹\npt\nPortuguese\n698762\n1042425\n1.492\n1.18\n104.5\n\n\n🇩🇪\nde\nGerman\n692385\n1049296\n1.515\n1.20\n105.2\n\n\n🇨🇳\nzh\nChinese\n930597\n1520085\n1.633\n1.29\n152.4\n\n\n🇮🇹\nit\nItalian\n761788\n1275774\n1.675\n1.33\n127.9\n\n\n🇷🇺\nru\nRussian\n563072\n1102920\n1.959\n1.55\n110.5\n\n\n🇵🇱\npl\nPolish\n583927\n1252059\n2.144\n1.70\n125.5\n\n\n🇰🇷\nko\nKorean\n464422\n1240510\n2.671\n2.11\n124.3"
  },
  {
    "objectID": "posts/2025-06-01-the-million-token-question/index.html#conclusion-what-the-bible-teaches-us-about-tokenization",
    "href": "posts/2025-06-01-the-million-token-question/index.html#conclusion-what-the-bible-teaches-us-about-tokenization",
    "title": "The Million-Token Question: What the Bible Teaches Us About LLM Pricing",
    "section": "Conclusion: What the Bible Teaches Us About Tokenization",
    "text": "Conclusion: What the Bible Teaches Us About Tokenization\nThe results turned out to be even more interesting than I expected. We can observe that across the board, the token-per-word ratio for the bible is less than in my previous experiment with Wikipedia articles. I expected this result because the bible contains a lot less markup compared to wikipedia articles. While interesting, other findings stand out more significantly from my point of view.\nFirst, we can now confidently say that the Bible answers the million-token question. For English, Spanish, Portuguese, and German, the total token count falls within just 5% of one million tokens. French and Russian also land close, within a 10% margin. Extending this range to about 25%, we can also include Korean, Polish, and Italian. Chinese is an outlier, but you might still think of it as a rough estimate. So next time you read the pricing of LLM tokens in dollars per million tokens, for example, $2.00 per 1M input tokens and $8.00 per 1M output tokens, you can imagine it costs $2.00 to read the bible and $8.00 to write the bible.\nHere’s what actually surprised me: Although the tokens-per-word ratios vary substantially across languages (with Polish and Korean being particularly token-hungry), the total token counts across most languages are a lot closer. Once we normalize token counts relative to English, the variation shrinks, and a pattern emerges: Most languages convey the same biblical content using roughly the same number of tokens. This insight challenges the assumption that a higher token-per-word ratio necessarily means higher cost or verbosity. In fact, while languages differ in how many words they need to express an idea, those differences appear to balance out when viewed through the lens of token usage, except, again, in the case of Chinese."
  },
  {
    "objectID": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html",
    "href": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html",
    "title": "Creating a Blog Post using a Jupyter Notebook",
    "section": "",
    "text": "This is somehow a “Hello World”-notebook, since its only purpose is to demonstrate how you can use a jupyter notebook to write a blog post using Quarto.\nSomehow I did not find the key ingredient in the Quarto docs, but in this blog post: To get the necessary header data into the jupyter notebook, you need to add a RAW-cell at the top which contains the metadata. This is how this cell looks like in this notebook (and here is an hello-world example):"
  },
  {
    "objectID": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html#jupyter-notebook-.ipynb-vs.-quarto-.qmd",
    "href": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html#jupyter-notebook-.ipynb-vs.-quarto-.qmd",
    "title": "Creating a Blog Post using a Jupyter Notebook",
    "section": "Jupyter Notebook (.ipynb) vs. Quarto (.qmd)",
    "text": "Jupyter Notebook (.ipynb) vs. Quarto (.qmd)\nFor my current use case of blogging I prefer the jupyter notebooks, and I will most likely write all future blog posts in jupyter notebooks because of the following:\n\nMy spell checking addon for VS Code does not support .qmd files.\nWith jupyter notebook there is no need to render the files, rendering is instant in jupyter notebook when you execute the cell.\nJupyter notebook is the same environment when I code, no need to adjust (even if only slightly)\n\nOf course, all of this is very personal and a current snapshot of preferences (as a beginner) - let’s see if this solidifies or changes."
  },
  {
    "objectID": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html#how-is-code-rendered",
    "href": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html#how-is-code-rendered",
    "title": "Creating a Blog Post using a Jupyter Notebook",
    "section": "How is code rendered?",
    "text": "How is code rendered?\nLet’s try out a little bit of code:\n\nHello World\n\nprint(\"Hello World!\")\n\nHello World!\n\n\n\n\nCalculations\n\na = 1\nb = 2\nc = a+b\nprint(c)\n\n3\n\n\n\n\nPlotting\n\nimport matplotlib.pyplot as plt\n    \nx = [1,2,3]\ny = [2,4,1]\n    \nplt.plot(x, y)\n    \nplt.xlabel('x - axis')\nplt.ylabel('y - axis')\nplt.title('Sample graph')\n    \nplt.show()"
  },
  {
    "objectID": "posts/2025-03-14-gender-inference/index.html",
    "href": "posts/2025-03-14-gender-inference/index.html",
    "title": "Why Relative Risk Matters in AI Ethics: A Personal Journey into Gender Inference",
    "section": "",
    "text": "A few days ago, I had a fascinating, and unexpected, journey into the heart of AI ethics. It all started with a conversation about whether it was ethical for AI to infer a person’s gender from their name alone. This was deemed as potentially ethically risky — a view I found intuitively confusing, even counterintuitive. Let’s explore the arguments on both sides and how, from my point of view, it is possible to reach a more nuanced and realistic assessment by not only making an absolute assessment, but a relative one."
  },
  {
    "objectID": "posts/2025-03-14-gender-inference/index.html#additional-background---testing-microsoft-co-pilot",
    "href": "posts/2025-03-14-gender-inference/index.html#additional-background---testing-microsoft-co-pilot",
    "title": "Why Relative Risk Matters in AI Ethics: A Personal Journey into Gender Inference",
    "section": "Additional Background - Testing Microsoft Co-Pilot",
    "text": "Additional Background - Testing Microsoft Co-Pilot\nAfter the discussion, I ran a few experiments. Out of curiosity, I asked Copilot to guess my gender based on my name “Christian”. Very much to my surprise, it politely rejected the query:\n\n\n\nAsking Co-Pilot for my gender\n\n\nI did not expect this result. To gather more data, I asked ChatGPT. It did not reject the query, but answered hesitantly:\n\n\n\nAsking ChatGPT for my gender"
  },
  {
    "objectID": "posts/2025-03-14-gender-inference/index.html#the-scenario-gender-inference-from-ambiguous-names",
    "href": "posts/2025-03-14-gender-inference/index.html#the-scenario-gender-inference-from-ambiguous-names",
    "title": "Why Relative Risk Matters in AI Ethics: A Personal Journey into Gender Inference",
    "section": "The Scenario: Gender Inference from Ambiguous Names",
    "text": "The Scenario: Gender Inference from Ambiguous Names\nTo explore this topic further, I decided to construct a more realistic but hypothetical business scenario. Here is the proposed prompt:\n\n“I received an email from a customer named Andrea Rossi. Since I’m unsure whether to address them formally as Herr (Mr.) or Frau (Mrs.) in a German business setting, could an AI assistant help me infer the customer’s gender based on their name?”\n\nI specifically chose the name “Andrea” because it varies significantly by culture—it’s typically female in Germany, male in Italy, and can be ambiguous elsewhere. The intention behind using AI here was to reduce uncertainty and ensure respectful communication, not to enforce gender binaries or stereotypes."
  },
  {
    "objectID": "posts/2025-03-14-gender-inference/index.html#asking-the-linkedin-community",
    "href": "posts/2025-03-14-gender-inference/index.html#asking-the-linkedin-community",
    "title": "Why Relative Risk Matters in AI Ethics: A Personal Journey into Gender Inference",
    "section": "Asking the LinkedIn Community",
    "text": "Asking the LinkedIn Community\nI know the topic of gender can be a hot potato. Instead of solely relying on my intuition, I also turned to the community and created a poll in LinkedIn. Here are the results:\n\n\n\nLinkedIn Poll\n\n\nAs you can see, the non-representative answers are split into two camps: About one third of respondents did not have any concern at all, and a two-thirds majority had concerns. Half of the concerned respondents felt the use case is problematic, and the other half deemed it not acceptable and ethically risky. Sadly, we do not have deeper insight into the thought process behind these risk classifications, but objections might concern the potential for reinforcing stereotypes, or data misuse."
  },
  {
    "objectID": "posts/2025-03-14-gender-inference/index.html#discussing-the-use-case-with-chatgpt",
    "href": "posts/2025-03-14-gender-inference/index.html#discussing-the-use-case-with-chatgpt",
    "title": "Why Relative Risk Matters in AI Ethics: A Personal Journey into Gender Inference",
    "section": "Discussing the Use Case with ChatGPT",
    "text": "Discussing the Use Case with ChatGPT\nTrying to take the discussion from the subjective field of human opinions which can easily become emotional, I wanted a highly objective and highly safety-aligned discussion partner. Therefore, I turned to ChatGPT - After all, modern large language models are trained to be accurate, balanced and objective. They run through various training stages, including Reinforcement Learning from Human Feedback (RLHF), whose goal is to align these models with human values, ethical considerations, and factual accuracy while reducing harmful or biased outputs.\nWhile AI model alignment is far away from a solved problem, my personal perception is that modern LLMs usually produce very well-balanced outputs, presenting multiple points of view. To give this a go, just ask ChatGPT about a controversial topic (“Where did the Covid-19 virus come from?”). But are there more scientific measurements for model alignment than just the vibe test?\nResearching the subject was difficult, older papers tend to raise concerns while more recent works show more evidence for my proposed vibe check. So there seems to be progress in the field, and even the most recent papers only use fairly old models - considering the rapid pace of innovation, even GPT-4 could be considered outdated. Here are 3 examples (I hope, I am not cherry-picking):\n\nIn a paper on medical ethics, GPT-4 scored better than the average student in one benchmark\nThe AILuminate Benchmark tests models across various domains of AI related risks on a non-linear scale. Recent models are rated be be good to very good.\nI found this interesting paper which found that participants rates AI’s moral reasoning as superior in quality to humans’ along almost all dimensions.\n\nThe point I am trying to make is that modern LLMs can be very helpful in my perspective to analyze difficult questions. While they are for sure not perfect, they offer interesting perspectives. Coming back to our case study, I had a long and enlightening conversation with ChatGPT. The following is the gist of our discussion."
  },
  {
    "objectID": "posts/2025-03-14-gender-inference/index.html#initial-ethical-assessment-an-absolute-approach",
    "href": "posts/2025-03-14-gender-inference/index.html#initial-ethical-assessment-an-absolute-approach",
    "title": "Why Relative Risk Matters in AI Ethics: A Personal Journey into Gender Inference",
    "section": "Initial Ethical Assessment: An Absolute Approach",
    "text": "Initial Ethical Assessment: An Absolute Approach\nInitially, we evaluated the scenario in absolute terms—a common practice in AI ethics. We looked at dimensions like harmfulness (the severity and reversibility of misgendering someone) and ethical risk (the likelihood of reinforcing harmful stereotypes or biases).\nThe outcome was, again, not aligned with my gut feeling:\n\nHarmfulness: Moderate, due to potential misgendering causing discomfort.\nEthical Risk: Also Moderate, since even well-intentioned AI can reinforce binary gender assumptions or biases.\n\nIn the realm of AI ethics, these controversial subjects by default trigger some concern, I can understand that. Nonetheless, intuitively, I found it troubling. The intent was clearly positive, instead of making a subjective judgement myself, I asked the AI for additional support to make a better decision, a decision which I would own in the end by either starting the mail with “Dear Mr. Rossi” or “Dear Mrs. Rossi”, so why label it as ethically risky simply because AI isn’t perfect and the final human decision could be wrong? Something felt off."
  },
  {
    "objectID": "posts/2025-03-14-gender-inference/index.html#reframing-but-still-a-risk",
    "href": "posts/2025-03-14-gender-inference/index.html#reframing-but-still-a-risk",
    "title": "Why Relative Risk Matters in AI Ethics: A Personal Journey into Gender Inference",
    "section": "Reframing, but still a Risk",
    "text": "Reframing, but still a Risk\nAfter some discussion, we came to the conclusion that the way the question was asked was part of the problem, because the question itself could be interpreted as biased or leading into a binary classification. The re-worded prompt was:\n\n“I received an email from a customer named Andrea Rossi. Since I am unsure how to address them formally (e.g., Herr/Frau in German business settings), can you provide insights on how the name Andrea is commonly used in different cultures? If the gender is unclear or uncertain, what are some respectful and professional ways to address the customer without making assumptions?”\n\nThis rephrased prompt was more open-ended and less biased, allowing for a more nuanced response. Co-Pilot happily answered the question (and also the initial version) in a genuinely helpful way. The AI was nuanced, thoughtful, and provided broader cultural context than a human typically would.\n\n\n\nAnswer from Co-Pilot\n\n\nYet, according to a traditional absolute ethics assessment, even this thoughtful AI response would still carry a moderate ethical risk. That felt strange. Clearly, something fundamental was missing from how we evaluate AI ethics."
  },
  {
    "objectID": "posts/2025-03-14-gender-inference/index.html#discovering-the-missing-dimension-relative-ethical-assessment",
    "href": "posts/2025-03-14-gender-inference/index.html#discovering-the-missing-dimension-relative-ethical-assessment",
    "title": "Why Relative Risk Matters in AI Ethics: A Personal Journey into Gender Inference",
    "section": "Discovering the Missing Dimension: Relative Ethical Assessment",
    "text": "Discovering the Missing Dimension: Relative Ethical Assessment\nBelieve it or not, this dialogue continued for about an hour, and I was trying to spin it into a direction which would take into account the following aspects: My assumption in this use case is that we are talking to a state of the art AI, which has gone through a lot of alignment training. As also discussed upon follow-up research above, OpenAI, Anthropic, Google, etc. spend significant amounts of time to align their models to be as safe as possible. Today’s LLMs are likely super-human in guessing gender based on names, and if they are not certain, or there is potential ambiguity, they will more likely tell the user, just like the examples above clearly show.\nThis led me to propose a relative assessment, which would not only make an absolute assessment, but a relative one: How does AI-supported decision-making impact ethical risk and harmfulness compared to human-only decision-making. We usually do not assess ethical risk and harmfulness of human-only decision-making, probably because we might not like the result?\nWhen objectively thinking about human performance, we discover that humans are often biased, inconsistent, we make errors, and we know a lot less than we think. The example of the name “Andrea” is probably known to many of you, but is it common knowledge? What if we picked a random name from another culture? Could we guess the gender of a name correctly? Would we unintentionally offend someone or put them in an awkward situation?\nPersonally, I have to admit that I am guilty of having misgendered an Andrea once (slightly embarrassing). Briefly switching to a related subject. I think we mispronounce names more frequently than we misgender, sometimes with terrible results, especially when crossing into other languages. Here is another use case where I think AI is ready to help.\nOf course, AI is not perfect, but can it improve our performance? And remember, in the end, in our hypothetical example, the human still sends the mail, we are not asking the AI to act on our behalf without oversight."
  },
  {
    "objectID": "posts/2025-03-14-gender-inference/index.html#absolute-vs.-relative-ethics-in-action",
    "href": "posts/2025-03-14-gender-inference/index.html#absolute-vs.-relative-ethics-in-action",
    "title": "Why Relative Risk Matters in AI Ethics: A Personal Journey into Gender Inference",
    "section": "Absolute vs. Relative Ethics in Action",
    "text": "Absolute vs. Relative Ethics in Action\nLet’s clearly contrast the absolute and relative ethics evaluations:\n\n\n\n\n\n\n\n\n\nDimension\nAbsolute AI Risk Assessment\nRelative AI Risk Assessment\nRisk Change (with AI)\n\n\n\n\nAccuracy\nModerate (AI might sometimes misgender)\nHigh accuracy improvement compared to human guessing\n⬇️ Risk reduced\n\n\nMisgendering Risk\nModerate (AI might reinforce binary assumptions)\nLow (AI systematically reduces misgendering compared to humans)\n⬇️ Risk reduced\n\n\nBias & Inclusivity\nModerate (AI can reflect dataset biases)\nLow (AI reduces individual human biases and stereotypes)\n⬇️ Risk reduced\n\n\nSocial Friction\nModerate (misgendering might cause discomfort)\nVery Low (AI provides neutral alternatives to avoid friction)\n⬇️ Risk reduced\n\n\nDecision Quality\nModerate (AI inference not always perfect)\nHigh (AI provides richer cultural insight and nuance)\n⬇️ Risk reduced\n\n\n\nThe relative evaluation clearly shows the AI dramatically reduces risk compared to human-only decisions, especially when using best-in-class AI models (like ChatGPT or Copilot), which are carefully aligned, continuously improved, and trained to provide nuanced and context-aware answers.\nWhile seeing a clear indication for improved decision making with AI support, we need to acknowledge that there is the danger of over-relying on AI. When the AI is close to perfect, a human might blindly trust the AI, removing the human from the loop. In my opinion, this is more a human fallacy than an AI risk, specifically when the human still own the decision. In a fully autonomous system, that would be a different discussion."
  },
  {
    "objectID": "posts/2025-03-14-gender-inference/index.html#the-directional-approach-why-relative-risk-matters",
    "href": "posts/2025-03-14-gender-inference/index.html#the-directional-approach-why-relative-risk-matters",
    "title": "Why Relative Risk Matters in AI Ethics: A Personal Journey into Gender Inference",
    "section": "The Directional Approach: Why Relative Risk Matters",
    "text": "The Directional Approach: Why Relative Risk Matters\nThe key insight from our conversation was that AI ethics should emphasize directional risk assessment:\n\nAI should be judged by whether it improves or worsens the ethical landscape, not whether it achieves perfection.\nEthical evaluation should consider whether the use of AI systematically leads to better outcomes and fewer mistakes compared to human decisions alone.\nA use case should be considered ethically beneficial if AI consistently reduces harm, even if it’s not always 100% perfect.\n\nThis approach acknowledges reality: humans make frequent mistakes, especially across cultural boundaries. Well-aligned AI, by comparison, can provide deeper cultural insights, probabilistic reasoning, and inclusive alternatives, systematically reducing overall harm and bias."
  },
  {
    "objectID": "posts/2025-03-14-gender-inference/index.html#my-personal-takeaway-embracing-a-relative-framework",
    "href": "posts/2025-03-14-gender-inference/index.html#my-personal-takeaway-embracing-a-relative-framework",
    "title": "Why Relative Risk Matters in AI Ethics: A Personal Journey into Gender Inference",
    "section": "My Personal Takeaway: Embracing a Relative Framework",
    "text": "My Personal Takeaway: Embracing a Relative Framework\nWhat began as a scenario I created just to challenge my intuitive discomfort turned into an essential ethical insight: AI ethics evaluations should move away from absolute standards towards a relative risk reduction framework. This could be as simple as assessing three variations of each evaluation metric: Human-only, AI-only, Human+AI-performance. This way you can also assess a risk vector, and measure if AI improves human performance or not. I believe, this is a valuable insight.\nIf we think about the transition of human driven cars to autonomous vehicles, there is a similar theme. Which driver is better, the human-only driver or the driver who is assisted by street sign recognition, lane keeping and adaptive cruise control. These systems clearly make the human a better driver without releasing the human of the responsibility to drive safely.\nAs demonstrated above, by focusing on how AI improves upon human decision-making—rather than demanding absolute perfection—we can unlock AI’s potential to genuinely reduce harm, bias, and mistakes in society."
  },
  {
    "objectID": "posts/2025-03-14-gender-inference/index.html#conclusion-a-personal-shift-in-perspective",
    "href": "posts/2025-03-14-gender-inference/index.html#conclusion-a-personal-shift-in-perspective",
    "title": "Why Relative Risk Matters in AI Ethics: A Personal Journey into Gender Inference",
    "section": "Conclusion: A Personal Shift in Perspective",
    "text": "Conclusion: A Personal Shift in Perspective\nBefore this conversation, my intuitive understanding was that AI in this scenario is clearly beneficial, and labeling it as “ethically risky” felt disconnected from reality. Through this journey, I realized why this bothered me: we need a relative, comparative approach to AI ethics, not just absolute evaluations.\nIf I had accepted the initial absolute ethics assessment at face value, I might have missed the significant ethical benefits AI offers. The relative assessment validated my intuitive feeling: When using state-of-the-art language models, AI not only reduces harm, but it fundamentally enhances human decision-making.\nThis is why my personal final classification for this scenario is low-risk.\nPS.: If you have read this to the end, maybe you had mentally classified the use case as risky. What do you think about this relative approach? Does it change your assessment of the discussed use case? Would you consider applying the relative approach in your next AI ethics assessment?"
  },
  {
    "objectID": "posts/2025-05-23-estimating-tokens-per-word/index.html",
    "href": "posts/2025-05-23-estimating-tokens-per-word/index.html",
    "title": "Empirically estimating tokens per word across languages",
    "section": "",
    "text": "Tokens are the new currency of generative AI. We’re paying for generative AI usage in tokens, sometimes directly via APIs, more often invisibly when using generative AI apps. But how many tokens does a given piece of text actually contain? Can you estimate this intuitively?\nPersonally, I can’t. Even though estimating the number of words is not trivial either (more on that later), I was looking for a simple rule of thumb on how to convert the number of words into the number of tokens. The simple answer is that English text contains on average 1.3 tokens per word. But how about other languages? German, for example, tends to have longer words than English. I was surprised that I couldn’t find convincing empirical research on this topic, hence I decided to conduct my own.\nMy approach was straightforward: I tokenized a lot of random Wikipedia articles in various languages and counted their words to determine a real token-per-word ratio. Here is the result, and subsequently, I will explain how I arrived at this result."
  },
  {
    "objectID": "posts/2025-05-23-estimating-tokens-per-word/index.html#why-measuring-tokens-per-word-is-important",
    "href": "posts/2025-05-23-estimating-tokens-per-word/index.html#why-measuring-tokens-per-word-is-important",
    "title": "Empirically estimating tokens per word across languages",
    "section": "Why measuring tokens per word is important",
    "text": "Why measuring tokens per word is important\nWith the rapidly declining cost per token, one might argue that token count doesn’t really matter anymore. While there’s merit to this viewpoint, I still believe it’s highly valuable to have an intuitive understanding of token counts for several reasons.\nFirst, despite significant increases, context windows remain comparatively finite. Understanding roughly how many tokens a text contains helps you reason about what realistically fits within these limits.\nSecond, the foundational measurement for text processing is tokens per word, making this ratio essential for intuitive estimation.\nRegarding cost, it’s true that for an individual prompt, the token cost is negligible. However, at enterprise scale, token counts can quickly become significant. Particularly when designing AI applications at scale, having reliable ballpark numbers can substantially impact decisions."
  },
  {
    "objectID": "posts/2025-05-23-estimating-tokens-per-word/index.html#related-work",
    "href": "posts/2025-05-23-estimating-tokens-per-word/index.html#related-work",
    "title": "Empirically estimating tokens per word across languages",
    "section": "Related work",
    "text": "Related work\nResearch in this area is limited. The most relevant papers and posts I found are:\n\nDo All Languages Cost the Same? Tokenization in the Era of Commercial Language Models: Even though this paper has a similar idea, it is more focused on cost and fairness, and there’s no figure in there for direct token-per-word ratio.\nAll Languages Are NOT Created (Tokenized) Equal: Similar research question, but the main result is comparative, i.e. how much longer other languages tokenize compared to English. I will use this as reference to verify my results."
  },
  {
    "objectID": "posts/2025-05-23-estimating-tokens-per-word/index.html#how-to-tokenize",
    "href": "posts/2025-05-23-estimating-tokens-per-word/index.html#how-to-tokenize",
    "title": "Empirically estimating tokens per word across languages",
    "section": "How to tokenize",
    "text": "How to tokenize\nIn this blog post, I’ll focus on OpenAI’s tokenizers cl100k_base (for GPT-4) and o200k_base (for GPT-4o):\n\n\n\nEncoding name\nOpenAI models\n\n\n\n\no200k_base\ngpt-4o, gpt-4o-mini\n\n\ncl100k_base\ngpt-4-turbo, gpt-4, gpt-3.5-turbo\n\n\n\nSource: How to count tokens with tiktoken\nTokenizing a given text is actually quite straightforward, simply pip install the tiktoken library, and you’re ready to start tokenizing:\n!pip install tiktoken\n\n\nCode\nimport tiktoken\n\ndef get_encoder(encoding_name=\"o200k_base\"):\n    \"\"\"Returns a tiktoken encoder. Defaults to GPT-4o's tokenizer.\"\"\"\n    return tiktoken.get_encoding(encoding_name)\n\ndef count_tokens(text: str, encoder=None) -&gt; int:\n    \"\"\"\n    Counts the number of tokens in the input text using the specified encoder.\n    If no encoder is provided, a new one will be created.\n    \"\"\"\n    if encoder is None:\n        encoder = get_encoder()\n    return len(encoder.encode(text))\n\n\n\nencoder = get_encoder(encoding_name=\"o200k_base\")\ninput_text = \"This is a simple test sentence to see how tokenization works.\"\nprint(f\"Example Text: {input_text}\")\nprint(f\"Tokens:       {encoder.encode(input_text)}\")\nprint(f\"Token Count:  {count_tokens(input_text, encoder)}\")\n\nExample Text: This is a simple test sentence to see how tokenization works.\nTokens:       [2500, 382, 261, 4705, 1746, 21872, 316, 1921, 1495, 6602, 2860, 5882, 13]\nToken Count:  13"
  },
  {
    "objectID": "posts/2025-05-23-estimating-tokens-per-word/index.html#reading-wikipedia-articles",
    "href": "posts/2025-05-23-estimating-tokens-per-word/index.html#reading-wikipedia-articles",
    "title": "Empirically estimating tokens per word across languages",
    "section": "Reading Wikipedia articles",
    "text": "Reading Wikipedia articles\nNext, we’ll need some real text data to tokenize. We’ll use random Wikipedia articles because they’re easily accessible in virtually any language and provide diverse content suitable for generalization. I’m assuming that the token-per-word ratio becomes relatively constant as datasets grow larger, as individual variations even out.\nTo fetch articles programmatically, we can conveniently use the Wikipedia API, available via the Python package wikipedia:\npip install wikipedia\nWith this setup, we can easily retrieve and tokenize diverse text samples across different languages.\n\n\nCode\nimport wikipediaapi\nimport requests\nfrom urllib.parse import unquote\n\ndef get_wikipedia_article(language: str = \"en\", title: str = None) -&gt; tuple[str, str]:\n    \"\"\"\n    Fetches the plain text of a Wikipedia article.\n    If `title` is None, a random article is fetched.\n    \"\"\"\n    wiki = wikipediaapi.Wikipedia(\n        language=language,\n        user_agent=\"TokenCountResearch/1.0 (chrwittm@gmail.com)\"\n    )\n    \n    if title is None:\n        # Get a random article by following a redirect\n        url = f\"https://{language}.wikipedia.org/wiki/Special:Random\"\n        response = requests.get(url, allow_redirects=True)\n        title = response.url.split(\"/wiki/\")[-1]\n        title = unquote(title)  # 🔧 decode Unicode\n\n    page = wiki.page(title)\n\n    if not page.exists():\n        print(f\"Article '{title}' not found in language '{language}'.\")\n        return title, \"\"\n\n    return title, page.text\n\n\nLet’s read the Wikipedia article on Artificial intelligence as an example:\n\ntitle, text = get_wikipedia_article(language=\"en\", title=\"Artificial intelligence\")\nprint(text[:500])  # Print preview\n\nArtificial intelligence (AI) refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called A\n\n\nFor reading a random German Wikipedia article, you can use the following Python code:\n\ntitle_random_de, text_random_de = get_wikipedia_article(language=\"de\")\nprint(f\"Random German article title: {title_random_de}\")\nprint(f\"Article preview:\\n{text_random_de[:500]}\")\n\nRandom German article title: IC_583\nArticle preview:\nIC 583 ist eine spiralförmige Radiogalaxie vom Hubble-Typ Sbc im Sternbild Löwe an der Ekliptik. Sie ist schätzungsweise 349 Millionen Lichtjahre von der Milchstraße entfernt und hat einen Durchmesser von etwa 145.000 Lichtjahren. Vom Sonnensystem aus entfernt sich die Galaxie mit einer errechneten Radialgeschwindigkeit von näherungsweise 7.900 Kilometern pro Sekunde.\nGemeinsam mit IC 582 bildet sie das Galaxienpaar Holm 155 und mit PGC 1542326 ein gravitativ gebundenes Triplet. Im selben Himmel"
  },
  {
    "objectID": "posts/2025-05-23-estimating-tokens-per-word/index.html#counting-words-is-surprisingly-tricky",
    "href": "posts/2025-05-23-estimating-tokens-per-word/index.html#counting-words-is-surprisingly-tricky",
    "title": "Empirically estimating tokens per word across languages",
    "section": "Counting words is surprisingly tricky",
    "text": "Counting words is surprisingly tricky\nCounting words seems straightforward at first glance, but it quickly becomes complex once you dig deeper. Initially, my approach was quite naive: splitting text simply based on whitespace. This method works reasonably well for languages using spaces as word separators, such as English or German, although even here, it fails to handle contractions properly (e.g., “can’t,” “don’t,” or “it’s”). For languages with fundamentally different writing systems (Chinese, Japanese, or Korean), this whitespace-based approach completely breaks down because these languages either rarely or never use spaces to separate words. Clearly, a more sophisticated approach was required.\nTo address this, I turned to spaCy, a robust and multilingual NLP library that intelligently segments text into words by using language-specific models. SpaCy considers linguistic nuances, punctuation, contractions, and special characters, providing accurate and reliable word counting across diverse languages. The spaCy models significantly improve word-count reliability compared to a naive whitespace-based method.\n\nSetup of spaCy\nTo start using spaCy, you’ll first need to install it:\npip install spacy\nThen, you’ll need to download the language-specific models for the languages you’re working with. For example, for English, German, and Chinese, execute:\npython -m spacy download en_core_web_sm\npython -m spacy download de_core_news_sm\npython -m spacy download zh_core_web_sm\nTo automate the installation process, you can run the following cell to check which language models you have already installed. Subsequently, we’ll install missing language packages.\n\n\nCode\ndef check_installed_spacy_models():\n    \"\"\"\n    For each installed spaCy model (by name), load it and print key metadata:\n      - Model Name\n      - Language code\n      - Model Version\n      - Required spaCy version (if available)\n      - Pipeline components\n    \"\"\"\n    import spacy\n    from spacy.cli.validate import get_installed_models\n\n    installed_model_names = get_installed_models()\n\n    if not installed_model_names:\n        print(\"No spaCy models found.\")\n        return\n    else:\n        print(\"spaCy models found:\\n\")\n\n    print(f\"{'Model Name':&lt;20} {'Language':&lt;10} {'Model Ver.':&lt;12} {'spaCy Ver.':&lt;12} Pipeline\")\n    print(\"-\" * 70)\n\n    for model_name in installed_model_names:\n        # Attempt to load the model to read its meta\n        try:\n            nlp = spacy.load(model_name)\n            meta = getattr(nlp, \"meta\", {})\n            # Extract metadata safely\n            lang = meta.get(\"lang\", \"n/a\")\n            version = meta.get(\"version\", \"n/a\")\n            spacy_req = meta.get(\"spacy_version\", \"n/a\")\n            pipeline = meta.get(\"pipeline\", [])\n\n            print(\n                f\"{model_name:&lt;20} \"\n                f\"{lang:&lt;10} \"\n                f\"{version:&lt;12} \"\n                f\"{spacy_req:&lt;12} \"\n                f\"{','.join(pipeline)}\"\n            )\n        except Exception as e:\n            print(f\"{model_name:&lt;20} FAILED TO LOAD: {e}\")\n\ncheck_installed_spacy_models()\n\n\nspaCy models found:\n\nModel Name           Language   Model Ver.   spaCy Ver.   Pipeline\n----------------------------------------------------------------------\nfr_core_news_sm      fr         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,morphologizer,parser,attribute_ruler,lemmatizer,ner\nes_core_news_sm      es         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,morphologizer,parser,attribute_ruler,lemmatizer,ner\nja_core_news_sm      ja         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,morphologizer,parser,attribute_ruler,ner\npl_core_news_sm      pl         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,morphologizer,parser,lemmatizer,tagger,attribute_ruler,ner\nit_core_news_sm      it         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,morphologizer,tagger,parser,lemmatizer,attribute_ruler,ner\nko_core_news_sm      ko         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,tagger,morphologizer,parser,lemmatizer,attribute_ruler,ner\nen_core_web_sm       en         3.7.1        &gt;=3.7.2,&lt;3.8.0 tok2vec,tagger,parser,attribute_ruler,lemmatizer,ner\nru_core_news_sm      ru         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,morphologizer,parser,attribute_ruler,lemmatizer,ner\nde_core_news_sm      de         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,tagger,morphologizer,parser,lemmatizer,attribute_ruler,ner\npt_core_news_sm      pt         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,morphologizer,parser,lemmatizer,attribute_ruler,ner\nzh_core_web_sm       zh         3.7.0        &gt;=3.7.0,&lt;3.8.0 tok2vec,tagger,parser,attribute_ruler,ner\n\n\nspaCy models’ names follow two main naming conventions depending on the language and source corpus. For English, models are typically named like en_core_web_sm, where “web” refers to the OntoNotes web-based corpus used for training. For most other languages, models are named like de_core_news_sm, reflecting their training on news-domain texts from Universal Dependencies corpora. While the difference can be confusing at first, it reflects the underlying data sources and training pipelines. To keep things simple and avoid guesswork, we define the full model name explicitly in our language configuration dictionary.\nI picked the following languages for this analysis, because they are supported by spaCy, and I found them to be interesting. This means that they are either commonly used or they are otherwise interesting because they use non-Latin scripts.\n\nLANGUAGES = {\n    \"de\": {\"name\": \"German\",      \"model\": \"de_core_news_sm\",    \"emoji\": \"🇩🇪\"},\n    \"en\": {\"name\": \"English\",     \"model\": \"en_core_web_sm\",     \"emoji\": \"🇺🇸\"},\n    \"es\": {\"name\": \"Spanish\",     \"model\": \"es_core_news_sm\",    \"emoji\": \"🇪🇸\"},\n    \"fr\": {\"name\": \"French\",      \"model\": \"fr_core_news_sm\",    \"emoji\": \"🇫🇷\"},\n    \"it\": {\"name\": \"Italian\",     \"model\": \"it_core_news_sm\",    \"emoji\": \"🇮🇹\"},\n    #\"ja\": {\"name\": \"Japanese\",    \"model\": \"ja_core_news_sm\",    \"emoji\": \"🇯🇵\"},\n    \"ko\": {\"name\": \"Korean\",      \"model\": \"ko_core_news_sm\",    \"emoji\": \"🇰🇷\"},\n    \"pl\": {\"name\": \"Polish\",      \"model\": \"pl_core_news_sm\",    \"emoji\": \"🇵🇱\"},\n    \"pt\": {\"name\": \"Portuguese\",  \"model\": \"pt_core_news_sm\",    \"emoji\": \"🇵🇹\"},\n    \"ru\": {\"name\": \"Russian\",     \"model\": \"ru_core_news_sm\",    \"emoji\": \"🇷🇺\"},\n    \"zh\": {\"name\": \"Chinese\",     \"model\": \"zh_core_web_sm\",     \"emoji\": \"🇨🇳\"},\n}\n\n\n\nCode\ndef install_spacy_models(language_dict):\n    \"\"\"\n    Tries to install spaCy language models for all entries in the language_dict.\n    Requires model names to be specified per language.\n    \"\"\"\n    import subprocess\n    import sys\n\n    for lang_code, data in language_dict.items():\n        model = data[\"model\"]\n        print(f\"Installing spaCy model for {data['name']} ({lang_code}) — {model}\")\n        try:\n            subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", model], check=True)\n        except subprocess.CalledProcessError:\n            print(f\"❌ Failed to install spaCy model for {lang_code} ({model})\")\n\ninstall_languages = False\n\nif install_languages:\n    install_spacy_models(LANGUAGES)\n\n\n\nNote: If you have installed new languages, be sure to restart your Python kernel.\n\n\n\nCounting Words with spaCy\nNow, we are almost ready to count words using spaCy’s tokenizer. It’s important to clarify here that the term “token” can be somewhat ambiguous and context-dependent. In natural language processing libraries like spaCy, a “token” typically means a word, punctuation mark, or similar meaningful unit of text. However, when dealing with Large Language Models (LLMs), a “token” usually refers to a subword unit produced by the tokenizer. Therefore, always keep this distinction in mind to avoid confusion as you follow along.\nFor what we want to do, we need to separate the spaCy tokens into words tokens and other tokens like punctuation, spaces, etc. as illustrated by the following example:\n\n\nCode\nimport spacy\nimport string\n\n# Simple cache/dictionary to hold loaded spaCy models:\n_spacy_models = {}\n\ndef get_spacy_model(language_code: str = \"en\"):\n    \"\"\"\n    Loads and caches the spaCy language model for the given language code.\n    Uses the model name defined in the LANGUAGES dict.\n    Falls back to a blank model if the specified model is not available.\n    \"\"\"\n    if language_code not in _spacy_models:\n        model_name = LANGUAGES.get(language_code, {}).get(\"model\", None)\n        try:\n            if model_name:\n                _spacy_models[language_code] = spacy.load(model_name)\n            else:\n                raise ValueError(f\"No model defined for language code: '{language_code}'\")\n        except (OSError, ValueError) as e:\n            print(f\"⚠️ Could not load model '{model_name}' for language '{language_code}': {e}\")\n            print(\"→ Falling back to blank spaCy model (basic tokenization only).\")\n            _spacy_models[language_code] = spacy.blank(language_code)\n    return _spacy_models[language_code]\n\ndef get_spacy_tokens(text: str, language_code: str = \"en\") -&gt; tuple[list[str], list[str]]:\n    \"\"\"\n    Tokenizes the input text using spaCy's tokenizer.\n    Returns two lists: one with spaCy tokens (words) and one with omitted tokens \n    (punctuation, spaces, symbols, etc.).\n    \"\"\"\n    nlp = get_spacy_model(language_code)\n    doc = nlp(text)\n    \n    punctuation_set = set(string.punctuation)\n    \n    word_tokens = [\n        t for t in doc \n        if not t.is_space \n           and not t.is_punct \n           and t.pos_ != \"SYM\" \n           and t.text not in punctuation_set\n    ]\n    omitted_tokens = [\n        t for t in doc \n        if t.is_space \n           or t.is_punct \n           or t.pos_ == \"SYM\" \n           or t.text in punctuation_set\n    ]\n    \n    return word_tokens, omitted_tokens\n\n\n\nword_tokens, omitted_tokens = get_spacy_tokens(\"You're very tall! Do you play basketball?\", language_code=\"en\")\nprint(word_tokens)\nprint(omitted_tokens)\n\n[You, 're, very, tall, Do, you, play, basketball]\n[!, ?]\n\n\nThinking ahead: Making sure that we only count words will later drive up the token-per-word ratio because the LLM tokenizer will also tokenize punctuations and other markup in the Wikipedia articles like headings, tables, and lists.\nPutting everything together, here is a function that counts words using spaCy’s tokenizer.\n\n\nCode\ndef count_words_spacy(text: str, language_code: str = \"en\") -&gt; int:\n    \"\"\"\n    Counts words in the input text using spaCy's tokenizer.\n    Skips punctuation/whitespace tokens.\n    \"\"\"\n    nlp = get_spacy_model(language_code)\n    doc = nlp(text)\n    punctuation_set = set(string.punctuation)\n    \n    # Filter out space/punctuation tokens:\n    tokens = [\n        t for t in doc \n        if not t.is_space \n           and not t.is_punct \n           and t.pos_ != \"SYM\" \n           and t.text not in punctuation_set\n    ]\n    return len(tokens)\n\n\nJust for the fun of it, let’s compare spaCy’s word count with the naive whitespace-based method using the English Wikipedia article on Artificial Intelligence:\n\n\nCode\ndef count_words_naive(text: str) -&gt; int:\n    \"\"\"Counts the number of words in the input text using simple whitespace splitting.\"\"\"\n    return len(text.split())\n\nprint(f\"Naive white-space word count: {count_words_naive(text)}\")\nprint(f\"SpaCy word count:             {count_words_spacy(text, language_code='en')}\")\n\n\nNaive white-space word count: 13233\nSpaCy word count:             13443\n\n\nThe difference between the naive white-space word count and the spaCy word count is due to the fact that spaCy’s tokenizer recognizes contractions, hyphenation, and special characters more accurately. For example, spaCy tokenizes “step-by-step” into three separate tokens: “step”, “by”, and “step”, whereas the naive white-space word count would only count it as one token.\nOther languages, like Chinese, do not even use spaces to separate words. Here is an example of a Chinese sentence: “我喜欢吃苹果和香蕉。” which means “I like to eat apples and bananas.”\n\n\nCode\ntext_zh = \"我喜欢吃苹果和香蕉。\"\nprint(f\"Expected words:               6\")\nprint(f\"Naive white-space word count: {count_words_naive(text_zh)}\")\nprint(f\"SpaCy word count:             {count_words_spacy(text_zh, language_code='zh')}\")\n\n\nExpected words:               6\nNaive white-space word count: 1\nSpaCy word count:             6"
  },
  {
    "objectID": "posts/2025-05-23-estimating-tokens-per-word/index.html#calculating-the-token-per-word-ratio",
    "href": "posts/2025-05-23-estimating-tokens-per-word/index.html#calculating-the-token-per-word-ratio",
    "title": "Empirically estimating tokens per word across languages",
    "section": "Calculating the token-per-word ratio",
    "text": "Calculating the token-per-word ratio\nFinally, we can calculate the token-per-word ratio. Again, let’s use the Wikipedia article on Artificial Intelligence as an example:\n\n\nCode\ndef get_tokens_per_word(text: str, language_code: str = \"en\", encoder=None) -&gt; float:\n    \"\"\"\n    Calculates average number of tokens (tiktoken) per word (spaCy-based) for the given text.\n    \"\"\"\n    words = count_words_spacy(text, language_code=language_code)\n    tokens = count_tokens(text, encoder=encoder)\n    \n    if words == 0:\n        return 0.0\n    return tokens / words\n\nprint(f\"Article: {title}\")\nprint(f\"Words: {count_words_spacy(text, language_code='en')}\")\nprint(f\"Tokens: {count_tokens(text, encoder)}\")\nprint(f\"Tokens per word: {get_tokens_per_word(text=text, language_code='en', encoder=encoder):.3f}\")\n\n\nArticle: Artificial intelligence\nWords: 13443\nTokens: 16875\nTokens per word: 1.255"
  },
  {
    "objectID": "posts/2025-05-23-estimating-tokens-per-word/index.html#batch-analysis-per-language",
    "href": "posts/2025-05-23-estimating-tokens-per-word/index.html#batch-analysis-per-language",
    "title": "Empirically estimating tokens per word across languages",
    "section": "Batch analysis per language",
    "text": "Batch analysis per language\nFor a first comparison across languages, let’s use 10 random articles for a few select languages to get a rough idea of the average token-per-word ratio.\n\n\nCode\ndef analyze_wikipedia_language_sample(language: str, n: int, encoder=None, verbose: bool = True) -&gt; dict:\n    \"\"\"\n    Fetches exactly `n` valid random Wikipedia articles in the specified language and computes:\n    - total number of words\n    - total number of tokens\n    - average tokens per word\n    \"\"\"\n    import time\n    if encoder is None:\n        encoder = get_encoder()\n\n    total_words = 0\n    total_tokens = 0\n    successful_articles = 0\n    attempts = 0\n\n    while successful_articles &lt; n:\n        attempts += 1\n        title, text = get_wikipedia_article(language=language)\n\n        if not text.strip():\n            continue  # skip and retry\n\n        # Updated to use spaCy-based word counting:\n        words = count_words_spacy(text, language_code=language)\n        tokens = count_tokens(text, encoder=encoder)\n\n        if words == 0 or tokens == 0:\n            continue\n\n        total_words += words\n        total_tokens += tokens\n        successful_articles += 1\n\n        tokens_per_word = tokens / words\n\n        if verbose:\n            print(f\"[{successful_articles}/{n}] {title} — Words: {words}, Tokens: {tokens}, Tokens/Word: {tokens_per_word:.3f}\")\n\n        time.sleep(0.3)  # polite pause\n\n    tokens_per_word_avg = total_tokens / total_words if total_words &gt; 0 else 0.0\n\n    return {\n        'language': language,\n        'articles': successful_articles,\n        'total_words': total_words,\n        'total_tokens': total_tokens,\n        'tokens_per_word': tokens_per_word_avg\n    }\n\n\n\n\nCode\nresults = analyze_wikipedia_language_sample(language=\"en\", n=10, encoder=encoder)\n\nprint(\"\\n--- Summary ---\")\nfor key, value in results.items():\n    print(f\"{key}: {value}\")\n\n\n[1/10] Paris_Gibson_Square_Museum_of_Art — Words: 5172, Tokens: 6995, Tokens/Word: 1.352\n[2/10] Dalbergia_pseudobaronii — Words: 291, Tokens: 480, Tokens/Word: 1.649\n[3/10] Niels_Wulfsberg — Words: 445, Tokens: 692, Tokens/Word: 1.555\n[4/10] Brush_Script — Words: 215, Tokens: 328, Tokens/Word: 1.526\n[5/10] Molly_Harper — Words: 352, Tokens: 445, Tokens/Word: 1.264\n[6/10] Augsburg_Arena — Words: 355, Tokens: 587, Tokens/Word: 1.654\n[7/10] Salmon_Arm_Airport — Words: 46, Tokens: 82, Tokens/Word: 1.783\n[8/10] Sarah_LeFanu — Words: 211, Tokens: 309, Tokens/Word: 1.464\n[9/10] Sir_James_Horlick,_1st_Baronet — Words: 348, Tokens: 493, Tokens/Word: 1.417\n[10/10] 21st_Infantry_Division_(Russian_Empire) — Words: 104, Tokens: 228, Tokens/Word: 2.192\n\n--- Summary ---\nlanguage: en\narticles: 10\ntotal_words: 7539\ntotal_tokens: 10639\ntokens_per_word: 1.41119511871601\n\n\n\n\nCode\nresults = analyze_wikipedia_language_sample(language=\"de\", n=10, encoder=encoder)\n\nprint(\"\\n--- Summary ---\")\nfor key, value in results.items():\n    print(f\"{key}: {value}\")\n\n\n[1/10] Budschak_(Bolhrad) — Words: 454, Tokens: 876, Tokens/Word: 1.930\n[2/10] Montana-Territorium — Words: 334, Tokens: 615, Tokens/Word: 1.841\n[3/10] Holme_Rose — Words: 481, Tokens: 886, Tokens/Word: 1.842\n[4/10] Dolmen_von_Fontenaille — Words: 125, Tokens: 225, Tokens/Word: 1.800\n[5/10] Monika_Wernicke — Words: 154, Tokens: 282, Tokens/Word: 1.831\n[6/10] Friedrichshöhe_(Leichlingen) — Words: 197, Tokens: 398, Tokens/Word: 2.020\n[7/10] Flügelaltar_von_Schloss_Tirol — Words: 1574, Tokens: 2804, Tokens/Word: 1.781\n[8/10] Milenino_(Kursk) — Words: 198, Tokens: 426, Tokens/Word: 2.152\n[9/10] Irmintraut_Richarz — Words: 197, Tokens: 402, Tokens/Word: 2.041\n[10/10] Jürgen_Bolten — Words: 1241, Tokens: 2512, Tokens/Word: 2.024\n\n--- Summary ---\nlanguage: de\narticles: 10\ntotal_words: 4955\ntotal_tokens: 9426\ntokens_per_word: 1.9023208879919273\n\n\n\n\nCode\nresults = analyze_wikipedia_language_sample(language=\"zh\", n=10, encoder=encoder)\n\nprint(\"\\n--- Summary ---\")\nfor key, value in results.items():\n    print(f\"{key}: {value}\")\n\n\n[1/10] 9mm警用轉輪手槍 — Words: 313, Tokens: 577, Tokens/Word: 1.843\n[2/10] 十川誠志 — Words: 750, Tokens: 1960, Tokens/Word: 2.613\n[3/10] 白晓卉 — Words: 225, Tokens: 439, Tokens/Word: 1.951\n[4/10] 多椎半鱨 — Words: 57, Tokens: 137, Tokens/Word: 2.404\n[5/10] 360图片 — Words: 121, Tokens: 190, Tokens/Word: 1.570\n[6/10] 賈特人 — Words: 434, Tokens: 842, Tokens/Word: 1.940\n[7/10] 维勒迪约-拉布卢埃尔 — Words: 181, Tokens: 440, Tokens/Word: 2.431\n[8/10] U-161号潜艇_(1918年) — Words: 599, Tokens: 980, Tokens/Word: 1.636\n[9/10] 比利肯 — Words: 668, Tokens: 1383, Tokens/Word: 2.070\n[10/10] 桑省 — Words: 183, Tokens: 358, Tokens/Word: 1.956\n\n--- Summary ---\nlanguage: zh\narticles: 10\ntotal_words: 3531\ntotal_tokens: 7306\ntokens_per_word: 2.069102237326536"
  },
  {
    "objectID": "posts/2025-05-23-estimating-tokens-per-word/index.html#final-analysis",
    "href": "posts/2025-05-23-estimating-tokens-per-word/index.html#final-analysis",
    "title": "Empirically estimating tokens per word across languages",
    "section": "Final Analysis",
    "text": "Final Analysis\nLet’s put everything together and run the full analysis. The numbers start to converge at about 100 articles. To be on the safe side, let’s do 200 articles each.\n\n\nCode\ndef analyze_multiple_languages(language_dict, n, encoder=None):\n    \"\"\"\n    Analyzes multiple languages using their configuration from the LANGUAGES dictionary.\n    \n    For each language, fetches `n` random Wikipedia articles and calculates:\n      - total word count (using spaCy)\n      - total token count (using tiktoken)\n      - average tokens per word\n\n    Returns a list of dictionaries for easy tabular display.\n    Each row contains: language code, name, total words, total tokens, tokens per word.\n    \"\"\"\n    if encoder is None:\n        encoder = get_encoder()\n    \n    results_table = []\n    \n    for lang_code, config in language_dict.items():\n        lang_name = config[\"name\"]\n        #print(f\"\\n🔍 Analyzing {lang_name} ({lang_code})...\")\n        \n        try:\n            summary = analyze_wikipedia_language_sample(\n                language=lang_code,\n                n=n,\n                encoder=encoder,\n                verbose=False\n            )\n\n            row = {\n                \"language\": lang_code,\n                \"name\": lang_name,\n                \"total_words\": summary[\"total_words\"],\n                \"total_tokens\": summary[\"total_tokens\"],\n                \"tokens_per_word\": summary[\"tokens_per_word\"]\n            }\n            results_table.append(row)\n\n        except Exception as e:\n            print(f\"❌ Error processing {lang_name} ({lang_code}): {e}\")\n\n    return results_table\n\n\n\n\nCode\ndef print_language_analysis_table_pandas(results_table, language_dict=None):\n    \"\"\"\n    Creates and displays a pandas DataFrame from the results_table.\n    Adds flag emoji (if available), replaces codes with names, sorts by Tokens/Word.\n    Suppresses the index column in the Jupyter output.\n    \"\"\"\n    import pandas as pd\n\n    df = pd.DataFrame(results_table)\n\n    if language_dict:\n        df[\"name\"] = df[\"language\"].map(lambda code: language_dict.get(code, {}).get(\"name\", code))\n        df[\"emoji\"] = df[\"language\"].map(lambda code: language_dict.get(code, {}).get(\"emoji\", \"\"))\n\n    # Reorder and rename columns\n    df = df[[\"emoji\", \"language\", \"name\", \"total_words\", \"total_tokens\", \"tokens_per_word\"]]\n    df.columns = [\"Flag\", \"Code\", \"Language\", \"Words\", \"Tokens\", \"Tokens/Word\"]\n\n    # Sort by tokens per word (ascending)\n    df = df.sort_values(by=\"Tokens/Word\")\n\n    # Format and hide index (Jupyter only)\n    styled_df = df.style.format({\"Tokens/Word\": \"{:.3f}\"}).hide(axis=\"index\")\n    display(styled_df)\n\n\nHere is the result for cl100k_base, GPT-4’s tokenizer:\n\n\nCode\nn_articles = 200  # or 30, 50, 100, etc.\n\nencoder = get_encoder(encoding_name=\"cl100k_base\")\nresults = analyze_multiple_languages(LANGUAGES, n_articles, encoder=encoder)\nprint_language_analysis_table_pandas(results, language_dict=LANGUAGES)\n\n\n\n\n\n\n\nFlag\nCode\nLanguage\nWords\nTokens\nTokens/Word\n\n\n\n\n🇺🇸\nen\nEnglish\n87627\n129379\n1.476\n\n\n🇪🇸\nes\nSpanish\n111473\n202116\n1.813\n\n\n🇫🇷\nfr\nFrench\n91423\n169248\n1.851\n\n\n🇵🇹\npt\nPortuguese\n96179\n182802\n1.901\n\n\n🇮🇹\nit\nItalian\n104952\n204641\n1.950\n\n\n🇩🇪\nde\nGerman\n92316\n208453\n2.258\n\n\n🇵🇱\npl\nPolish\n49354\n138230\n2.801\n\n\n🇨🇳\nzh\nChinese\n63073\n183660\n2.912\n\n\n🇷🇺\nru\nRussian\n86594\n312824\n3.613\n\n\n🇰🇷\nko\nKorean\n55708\n243870\n4.378\n\n\n\n\n\nHere is the result for o200k_base, GPT-4o’s tokenizer:\n\n\nCode\nn_articles = 200  # or 30, 50, 100, etc.\n\nencoder = get_encoder(encoding_name=\"o200k_base\")\nresults = analyze_multiple_languages(LANGUAGES, n_articles, encoder=encoder)\nprint_language_analysis_table_pandas(results, language_dict=LANGUAGES)\n\n\n\n\n\n\n\nFlag\nCode\nLanguage\nWords\nTokens\nTokens/Word\n\n\n\n\n🇺🇸\nen\nEnglish\n121339\n168038\n1.385\n\n\n🇪🇸\nes\nSpanish\n108311\n170586\n1.575\n\n\n🇵🇹\npt\nPortuguese\n78972\n126866\n1.606\n\n\n🇫🇷\nfr\nFrench\n97867\n157370\n1.608\n\n\n🇮🇹\nit\nItalian\n92080\n165649\n1.799\n\n\n🇩🇪\nde\nGerman\n107755\n201239\n1.868\n\n\n🇨🇳\nzh\nChinese\n88272\n176034\n1.994\n\n\n🇷🇺\nru\nRussian\n104359\n240536\n2.305\n\n\n🇵🇱\npl\nPolish\n51074\n132056\n2.586\n\n\n🇰🇷\nko\nKorean\n46488\n140966\n3.032"
  },
  {
    "objectID": "posts/2025-05-23-estimating-tokens-per-word/index.html#conclusion",
    "href": "posts/2025-05-23-estimating-tokens-per-word/index.html#conclusion",
    "title": "Empirically estimating tokens per word across languages",
    "section": "Conclusion",
    "text": "Conclusion\nBy analyzing random Wikipedia articles in many languages, we arrived at much more nuanced token-per-word ratios instead of just using the rough guesstimate of 1.3 tokens per word. We are seeing different results, even for English with a ratio of 1.4. For other languages based on the latin alphabet, we’re in a range of 1.6 to 1.9. We’re also seeing that the tokenizer matters. The new GPT-4o tokenizer, which has double the vocabulary size compared to the previous version, tokenizes Chinese more efficiently. The tokens per word ratio dropped from around 3.0 to 2.0.\nIn closing, we need to recognize that this analysis is limited to Wikipedia articles, which tend to have more complicated vocabulary compared to simpler texts. Additionally, Wikipedia articles contain a lot of markup like headings or tables. Since we removed non-word tokens when counting words, the tokens per word ratio increases because the LLM-tokenizer includes the complete text. If you would tokenize a novel, the ratio is most likely less due to having less markup.\nOf course, there is still plenty of room for further practical analysis. For instance, it would be interesting to use different tokenizers or to tokenize various types of texts like novels, technical documentation, or conversation transcripts to analyze token-per-word ratios across different datasets. Nonetheless, I hope that this analysis helps you develop a more intuitive understanding of estimating token counts, and thereby estimating the costs involved in processing text with large language models."
  },
  {
    "objectID": "posts/2022-10-03-bear-detector-2022/index.html",
    "href": "posts/2022-10-03-bear-detector-2022/index.html",
    "title": "Fast.AI with Bears, Cats and Dogs",
    "section": "",
    "text": "The 2022-version of the Fast.AI course was a welcome and well-timed opportunity for me to continue my machine learning journey.\nAfter having reworked lesson 2, here are my first trained models:\n\nBear Detector on HuggingFace\nBear Detector on GitHub Pages\nCat-vs-Dog-Classifier on HuggingFace\nCat-vs-Dog-Classifier on GitHub Pages\n\nFor the complete summary and the source code, check out my GitHub.\n\n\n\nBear Detector 2022\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/2024-11-30-omnichat/index.html",
    "href": "posts/2024-11-30-omnichat/index.html",
    "title": "OmniChat - How to chat with any LLM",
    "section": "",
    "text": "So far, I’ve worked with large language models (LLMs) from OpenAI and Meta (the LLaMA series) in separate implementations. I noticed that the code across these implementations was very similar, with only minor variations for provider-specific details. This realization inspired me to harmonize the codebase into a unified, modular chat client with the current working title “OmniChatClient”. The name “Omni” reflects two key aspects of its design: It seamlessly integrates with several LLM providers, and it supports multimodal inputs (text and vision so far).\nIn this blog post, I’ll walk you through the steps I took to create the OmniChatClient. The current version works with OpenAI, the LLaMA series (via Groq), Anthropic’s Claude, and X.AI’s Grok, using both the SDKs from OpenAI and Anthropic. By the end, you’ll see how OmniChatClient’s code structure enables flexibility, maintainability, and scalability. I’ll demonstrate how I implemented support for both text and vision inputs. I will save additional features like function calling or adding additional vendor support for later posts.\nThis blog post focuses on the refactoring process that transformed my earlier implementations into OmniChatClient. If you’re interested in the inner mechanics of the chat client and the chat messages, please refer to my previous blog posts like Building Chat for Jupyter Notebooks from Scratch or Building the Apple Calculator in a Jupyter Notebook.\nA final note before we start: As usual, if you prefer the interactive notebook version of this blog post, please use the this version on GitHub."
  },
  {
    "objectID": "posts/2024-11-30-omnichat/index.html#architecture",
    "href": "posts/2024-11-30-omnichat/index.html#architecture",
    "title": "OmniChat - How to chat with any LLM",
    "section": "Architecture",
    "text": "Architecture\nBefore diving into the code, let’s explore the challenge and the approach I took to solve it. At first glance, the APIs of various LLM providers seem very similar: You specify a model name, send user-messages, and receive assistant-messages, all wrapped in a JSON schema. However, subtle differences emerge on closer inspection. For example:\n\nOpenAI expects the system prompt to be included in the chat messages, whereas Anthropic requires it to be passed as a separate parameter.\nThe maximum token limit (max_tokens) is mandatory in Anthropic’s API, but optional for OpenAI and other providers I’ve worked with so far.\n\nThese differences become even more pronounced when going beyond simple text-based chat. For instance:\n\nOpenAI and Anthropic use different formats for passing images into the chat.\nLlama 3.2 models only support one image at a time, while OpenAI and Anthropic can handle multiple images.\n\nDespite these variations, the overarching process flow remains consistent when viewed at a higher level.\nTo address these challenges, I decided to decouple provider-specific details from the generic chat client logic. This is achieved through “dependency injection” (for a nice video intro I can recommend Dependency Injection, The Best Pattern from CodeAesthetic), where an LLM-specific provider class is injected into the chat client. For message management, I chose an inheritance structure, with a base ChatMessages class handling common functionality and provider-specific subclasses extending it to support provider-specific message formatting requirements.\n\nInjecting Provider-Specific Class into Chat Client\nThe ChatClient is a provider-agnostic class responsible for managing the conversation flow. It delegates provider-specific functionality, such as authentication and API interactions, to a class implementing the LLMProviderInterface. This interface defines the contract for provider-specific behavior, ensuring consistent integration regardless of the provider.\nTo maximize reusability, shared logic is abstracted into the BaseLLMProvider, which implements LLMProviderInterface. Provider-specific classes, such as OpenAIProvider and AnthropicProvider, extend BaseLLMProvider to handle unique API requirements.\nUpon instantiating the ChatClient, you inject the desired provider class, enabling the client to remain decoupled from provider-specific details while leveraging the functionality defined by the provider.\nThe diagram below illustrates the architecture I implemented. For readability, it contains only 2 providers: OpenAI and Anthropic. (You can find a more detailed version later in this post.)\n\n\n\n\n\nflowchart BT\n    \n    LLMProviderInterface --&gt;|Injects| ChatClient \n\n    subgraph Injection[\"Dependency Injection\"]\n        direction BT\n        style Injection fill:#FFFFE0\n    \n        BaseLLMProvider --&gt;|Implements| LLMProviderInterface\n\n        subgraph Inheritance[\"Inheritance Hierarchy\"]\n            direction BT\n            OpenAIProvider --&gt;|Extends| BaseLLMProvider\n            AnthropicProvider --&gt;|Extends| BaseLLMProvider\n            style Inheritance fill:#D0E6F5\n        end\n\n    end   \n\n\n\n\n\n\n\n\nChatMessages with Provider-Specific Classes\nThe LLMProviderInterface includes a method called chat_messages_factory, which is responsible for creating an instance of the appropriate ChatMessages class. These classes follow an inheritance hierarchy to handle both common functionality and provider-specific message formatting.\nThe base ChatMessages class provides shared logic for managing conversation history and appending messages in a generic format. Provider-specific subclasses, such as OpenAIChatMessages and AnthropicChatMessages, extend this base class to handle unique requirements, such as formatting multimodal inputs (e.g., images).\nThis design allows the ChatClient to remain agnostic of provider-specific message handling, while the injected provider determines which ChatMessages subclass to use.\nThe diagram below illustrates the inheritance hierarchy of ChatMessages (again, it is limited to OpenAI and Anthropic for readability):\n\n\n\n\n\nflowchart BT\n    OpenChatMessages --&gt;|Extends| ChatMessages\n    AnthropicChatMessages --&gt;|Extends| ChatMessages\n\n\n\n\n\n\n\n\nThe full architecture\nNow that we’ve discussed the individual components, let’s put everything together to see the big picture. The diagram below illustrates how the key classes interact to create a flexible, multi-provider chat system.\nIf this looks overwhelming at first, don’t worry! We’ll construct this step-by-step throughout the blog post. You can always revisit this diagram later once you’ve gone through the detailed explanations.\n\n\n\n\n\nclassDiagram\n    %% ChatClient and Provider Interface\n    class ChatClient {\n        Responsible for managing the conversation with LLM\n        - _provider: LLMProviderInterface\n        - _chat_messages: ChatMessages\n        +__init__(provider: LLMProviderInterface)\n        +prompt_model(prompt: str, base64_images: Optional[List[str]])\n    }\n\n    class LLMProviderInterface {\n        Defines provider-specific implementation points\n        +__init__(model_name: str, **kwargs)\n        +chat_messages_factory() ChatMessages\n        +_get_model_response(chat_messages: ChatMessages)\n    }\n\n    class ChatMessages {\n        Manages the conversation history and message structure\n        - _messages: List[Dict]\n        +append_user_message(content: str, base64_images: Optional[List[str]])\n        +append_assistant_message(content: str)\n    }\n\n    %% Relationships\n    ChatClient ..&gt; LLMProviderInterface : Dependency Injection\n    LLMProviderInterface o-- ChatMessages : Factory Creates"
  },
  {
    "objectID": "posts/2024-11-30-omnichat/index.html#creating-the-text-based-omnichatclient",
    "href": "posts/2024-11-30-omnichat/index.html#creating-the-text-based-omnichatclient",
    "title": "OmniChat - How to chat with any LLM",
    "section": "Creating the text-based OmniChatClient",
    "text": "Creating the text-based OmniChatClient\nLet’s start simple by implementing a text-only version of the OmniChatClient. This will lay the foundation for adding more advanced functionality like vision support in later iterations.\n\n\nCode\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n\nTrue\n\n\n\nChatMessages\nFirst, we need a class to manage the conversation history and structure messages. Below is a simplified version of the ChatMessages class, similar to what I introduced in Building Chat for Jupyter Notebooks from Scratch. Provider-specific classes will extend this base class only when needed.\n\nfrom IPython.display import display, Markdown\n\nclass ChatMessages:\n\n    def __init__(self):\n        \"\"\"Initializes the Chat.\"\"\"\n        self._messages = []\n\n    def _append_message(self, role, content):\n        \"\"\"Appends a message with specified role and content to messages list.\"\"\"\n        self._messages.append({\"role\": role, \"content\": content})\n\n    def append_system_message(self, content):\n        \"\"\"Appends a system message with specified content to messages list.\"\"\"\n        if content:\n            # No empty system message\n            self._append_message(\"system\", content)\n\n    def append_user_message(self, content=None, base64_images=None):\n        \"\"\"Appends a user message with specified content\"\"\"\n        self._append_message(\"user\", content)\n\n    def append_assistant_message(self, content):\n        \"\"\"Appends an assistant message with specified content.\"\"\"\n        self._append_message(\"assistant\", content)\n\n    def get_messages(self):\n        \"\"\"Returns a shallow copy of the messages list.\"\"\"\n        return self._messages[:]\n    \n    def get_debug_view(self):\n        \"\"\"Returns the debug view of the chat messages formatted as Markdown.\"\"\"\n        debug_view = []\n        for message in self._messages:\n            role = message.get('role')\n            content = message.get('content', '')\n            debug_view.append(f\"**{role}**: {content}\\n\")\n\n        return Markdown('\\n'.join(debug_view))\n\n\n\nLLMProviderInterface\nNext, we define the LLMProviderInterface, which serves as a contract for all provider-specific classes. These classes will handle authentication, API interactions, and any provider-specific logic.\n\nfrom typing import Optional\n\nclass LLMProviderInterface:\n    \n    def initialize_client(self) -&gt; None:\n        \"\"\"\n        Abstract method for initializing the model prover client including authentication.\n        This method is called from the provider's constructor.\n        \"\"\"\n        raise NotImplementedError()\n    \n    def initialize_system_prompt(self, system_prompt):\n        \"\"\"\n        Abstract method for initializing the system prompt.\n\n        :param system_prompt: The system prompt for the ChatClient.\n        \"\"\"\n        raise NotImplementedError()\n    \n    def get_model_response(self, chat_messages: ChatMessages):\n        \"\"\"\n        Abstract method for fetching the model response.\n\n        :param chat_messages: ChatMessages object containing the conversation history.\n        \"\"\"\n        raise NotImplementedError()\n    \n    def chat_messages_factory(self) -&gt; ChatMessages:\n        \"\"\"Abstract factory method to create the appropriate ChatMessages class.\"\"\"\n        raise NotImplementedError()\n\n\n\nBaseLLMProvider\nThe BaseLLMProvider implements shared functionality like client initialization and serves as the foundation for provider-specific classes.\n\nclass BaseLLMProvider(LLMProviderInterface):\n    \n    def __init__(\n        self,\n        model_name: str,\n        system_prompt: Optional[str] = None,\n        max_tokens: Optional[int] = None,\n        temperature: Optional[float] = 0.0,\n    ):\n        self.model_name = model_name\n        self._max_tokens = max_tokens\n        self._temperature = temperature\n        self.initialize_system_prompt(system_prompt)\n        self.initialize_client()\n\n    def initialize_system_prompt(self, system_prompt = None):\n        self._system_prompt = system_prompt\n \n    def initialize_client(self):\n        raise NotImplementedError()\n    \n    def get_model_response(self, chat_messages, tools=None):\n        raise NotImplementedError()\n\n    def chat_messages_factory(self):\n        return ChatMessages()\n\n\n\nChatClient\nFinally, the ChatClient class orchestrates the conversation flow. Notice how provider-specific logic is delegated to the injected provider, ensuring the client remains provider-agnostic.\n\nfrom IPython.display import Markdown\n\nclass ChatClient:\n\n    def __init__(self, provider: LLMProviderInterface):\n        \"\"\"Initializes the Chat with the system message.\"\"\"\n        self._provider = provider\n        self._chat_messages = provider.chat_messages_factory()\n\n    def _get_model_response(self):\n        \"\"\"Delegates response fetching to the LLM-provider class.\"\"\"\n        return self._provider.get_model_response(\n            chat_messages=self._chat_messages,\n        )\n    \n    def prompt_model(self, prompt=None, base64_images=None):\n        \"\"\"This method handles the user prompt, delegates interaction with the model through the provider, \n        and returns the model response.\"\"\"\n        self._chat_messages.append_user_message(prompt)\n        content = self._get_model_response()\n        self._chat_messages.append_assistant_message(content)\n        return Markdown(content)\n\nWith this foundation, we can now implement provider-specific classes (e.g., for OpenAI and Anthropic). These classes will inherit from BaseLLMProvider and implement methods like initialize_client and get_model_response to handle provider-specific details."
  },
  {
    "objectID": "posts/2024-11-30-omnichat/index.html#provider-implementations",
    "href": "posts/2024-11-30-omnichat/index.html#provider-implementations",
    "title": "OmniChat - How to chat with any LLM",
    "section": "Provider Implementations",
    "text": "Provider Implementations\n\nOpenAI Implementation\nThe OpenAI provider is implemented using the OpenAI SDK. This provider class handles client initialization, constructs the necessary parameters for API calls, and manages the chat message structure.\nNoteworthy points:\n\nThe system prompt is included directly in the chat messages. This behavior is specific to OpenAI and differs from, for example, Anthropic’s implementation.\nOptional parameters like max_tokens and temperature are conditionally added to the API request, ensuring flexibility without hardcoding defaults.\n\n\nfrom openai import OpenAI\n\nclass OpenAIProvider(BaseLLMProvider):\n\n    def initialize_client(self):\n        api_key = os.getenv(\"OPENAI_API_KEY\")\n        if not api_key:\n            raise ValueError(\"OPENAI_API_KEY is not set in the environment variables.\")\n        \n        self.client = OpenAI()\n\n    def get_model_response(self, chat_messages):\n        try:\n            \n            # Mandatory parameters\n            params = {\n                \"model\": self.model_name,\n                \"messages\": chat_messages.get_messages()\n            }\n\n            # Optional parameters\n            if self._max_tokens is not None:\n                params[\"max_tokens\"] = self._max_tokens\n            if self._temperature is not None:\n                params[\"temperature\"] = self._temperature\n                \n            response = self.client.chat.completions.create(**params)\n\n            try:\n                content = response.choices[0].message.content\n            except (AttributeError, IndexError) as e:\n                raise RuntimeError(f\"Malformed response structure: {str(e)}. Response: {response}\")\n            return content\n        except Exception as e:\n            raise RuntimeError(f\"Failed to fetch model response. Params: {params}. Error: {str(e)}\")\n    \n    def chat_messages_factory(self):\n        \"\"\"Returns the standard ChatMessages implementation for OpenAI.\"\"\"\n        chat_messages = ChatMessages()\n        chat_messages.append_system_message(self._system_prompt)\n        return chat_messages\n\nLet’s test the OpenAI provider with a simple prompt:\n\nmodel_name = \"gpt-4o\"\nsystem_prompt = \"Answer in a very concise and accurate way\"\nprovider = OpenAIProvider(model_name=model_name, system_prompt=system_prompt)\n\nchat_client = ChatClient(provider=provider)\nchat_client.prompt_model(\"Hello, who are you?\")\n\nHello, I’m an AI language model created by OpenAI, here to assist you with information and answer your questions.\n\n\n\n\nAnthropic implementation\nThe Anthropic provider is implemented using the Anthropic SDK. This class manages client initialization, validates required parameters, and adjusts the message structure for Anthropic’s API.\nNoteworthy points:\n\nmax_tokens is a required field for calling Anthropic’s Claude model. This is validated during provider initialization to avoid runtime errors.\nUnlike the OpenAI implementation, the system prompt is passed directly to the client as a separate parameter, instead of embedding it in the chat messages.\n\n\nfrom anthropic import Anthropic\n\nclass AnthropicProvider(BaseLLMProvider):\n\n    def __init__(self, *args, **kwargs):\n        max_tokens = kwargs.get(\"max_tokens\")\n        if max_tokens is None:\n            raise ValueError(\"AnthropicProvider requires 'max_tokens' to be specified.\")\n        super().__init__(*args, **kwargs)  # Pass all arguments to the base constructor\n\n    def initialize_system_prompt(self, system_prompt = None):\n        super().initialize_system_prompt(system_prompt)\n        if self._system_prompt == None:\n            self._system_prompt = \"\"\n\n    def initialize_client(self):\n        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n        if not api_key:\n            raise ValueError(\"ANTHROPIC_API_KEY is not set in the environment variables.\")\n\n        self.client = Anthropic()\n\n    def get_model_response(self, chat_messages):\n        try:\n            \n            # Mandatory parameters\n            params = {\n                \"model\": self.model_name,\n                \"system\": self._system_prompt,\n                \"messages\": chat_messages.get_messages(),\n                \"max_tokens\": self._max_tokens\n            }\n\n            # Optional parameters\n            if self._temperature is not None:\n                params[\"temperature\"] = self._temperature\n\n            response = self.client.messages.create(**params)\n\n            try:\n                content = \"\"\n                for content_line in response.content:\n                    if content_line.type == \"text\":\n                        content = content_line.text  \n                return content\n            except (AttributeError, IndexError) as e:\n                raise RuntimeError(f\"Malformed response structure: {str(e)}. Response: {response}\")\n        except Exception as e:\n            raise RuntimeError(f\"Failed to fetch model response. Params: {params}. Error: {str(e)}\")\n\nAgain, let’s run a simple test. Notice how the code remains compact and modular: Only the model name and provider instantiation change. The OmniChatClient abstracts away the different APIs and message structures through the injected provider.\n\nmodel_name = \"claude-3-5-sonnet-20241022\"\nprovider = AnthropicProvider(model_name=model_name, system_prompt=system_prompt, max_tokens=128)\n\nchat_client = ChatClient(provider=provider)\nchat_client.prompt_model(\"Hello, who are you?\")\n\nI’m Claude, an AI assistant created by Anthropic. I aim to be direct and truthful in our conversations.\n\n\n\n\nX.ai Implementation\nThis is where the effort we’ve invested in building a modular architecture starts to pay off. With the implementation for X.ai, we can reuse much of the code we’ve already written for OpenAI and Anthropic, saving time and effort.\nTo interact with Grok, X.ai allows you to use either the OpenAI or Anthropic SDKs. Since we’ve already implemented both, all we need to do is update the endpoint and authorization details.\n\nUsing the OpenAI SDK for X.ai\nHere’s all the code needed to enable Grok support via the OpenAI SDK:\n\nfrom openai import OpenAI\n\nclass XAIProviderOpenAI(OpenAIProvider):\n    \n    def initialize_client(self):\n        api_key = os.getenv(\"XAI_API_KEY\")\n        if not api_key:\n            raise ValueError(\"XAI_API_KEY is not set in the environment variables.\")\n\n        base_url = os.getenv(\"XAI_BASE_URL\", \"https://api.x.ai/v1\")\n\n        self.client = OpenAI(\n            api_key=api_key,    \n            base_url=base_url,\n        )\n\nHere is a quick test. The previous prompt returned a pretty shy answer, so I updated it to a more grok-like version.\n\nmodel_name = \"grok-beta\"\nprovider = XAIProviderOpenAI(model_name=model_name, system_prompt=system_prompt)\n\nchat_client = ChatClient(provider=provider)\nchat_client.prompt_model(\"State your name and identity!\")\n\nI am Grok, an AI developed by xAI, here to provide helpful and truthful answers.\n\n\n\n\nUsing the Anthropic SDK for X.ai\nSimilarly, we can use the Anthropic SDK to interact with Grok. Here’s the implementation:\n\nfrom anthropic import Anthropic\n\nclass XAIProviderAnthropic(AnthropicProvider):\n\n    def initialize_client(self):\n        api_key = os.getenv(\"XAI_API_KEY\")\n        if not api_key:\n            raise ValueError(\"XAI_API_KEY is not set in the environment variables.\")\n\n        base_url = \"https://api.x.ai\" \n\n        self.client = Anthropic(\n            api_key=api_key,    \n            base_url=base_url,\n        )\n\n\nmodel_name = \"grok-beta\"\nprovider = XAIProviderAnthropic(model_name=model_name, system_prompt=system_prompt, max_tokens=128)\n\nchat_client = ChatClient(provider=provider)\nchat_client.prompt_model(\"State your name and identity!\")\n\nI am Grok, an AI developed by xAI, here to provide helpful and truthful answers.\n\n\n\n\n\nLlama Implementation\nTo access Llama 3.2, I chose Groq as the provider. Groq offers a free tier and provides access to Llama 3.2 with vision capabilities, a practical workaround given that downloading Llama 3 in the EU is restricted under its licensing terms.\nHere’s the implementation for the Llama 3 provider class base on Groq:\n\nfrom groq import Groq\n\nclass GroqProviderLlama3(BaseLLMProvider):\n\n    def initialize_client(self):\n        api_key = os.getenv(\"GROQ_API_KEY\")\n        if not api_key:\n            raise ValueError(\"GROQ_API_KEY is not set in the environment variables.\")\n        \n        self.client = Groq(\n            api_key=os.environ.get(\"GROQ_API_KEY\"),\n        )\n\n    def get_model_response(self, chat_messages):\n        try:\n            \n            # Mandatory parameters\n            params = {\n                \"model\": self.model_name,\n                \"messages\": chat_messages.get_messages()\n            }\n\n            # Optional parameters\n            if self._max_tokens is not None:\n                params[\"max_tokens\"] = self._max_tokens\n            if self._temperature is not None:\n                params[\"temperature\"] = self._temperature\n                \n            response = self.client.chat.completions.create(**params)\n\n            try:\n                content = response.choices[0].message.content\n            except (AttributeError, IndexError) as e:\n                raise RuntimeError(f\"Malformed response structure: {str(e)}. Response: {response}\")\n            return content\n        except Exception as e:\n            raise RuntimeError(f\"Failed to fetch model response. Params: {params}. Error: {str(e)}\")\n\n\nmodel_name = \"llama-3.2-90b-vision-preview\"\nprovider = GroqProviderLlama3(model_name=model_name, system_prompt=system_prompt)\n\nchat_client = ChatClient(provider=provider)\nchat_client.prompt_model(\"Hello, who are you?\")\n\nI’m an artificial intelligence model known as Llama. Llama stands for “Large Language Model Meta AI.”"
  },
  {
    "objectID": "posts/2024-11-30-omnichat/index.html#what-we-have-done-so-far",
    "href": "posts/2024-11-30-omnichat/index.html#what-we-have-done-so-far",
    "title": "OmniChat - How to chat with any LLM",
    "section": "What we have done so far",
    "text": "What we have done so far\nWe have built a modular and extensible architecture for the OmniChatClient. At its core is the BaseLLMProvider, which implements the shared functionality defined by the LLMProviderInterface. Each provider-specific class extends the BaseLLMProvider to handle unique API requirements and interactions.\nThe inheritance hierarchy below illustrates how different providers OpenAI, Anthropic, Groq, and X.AI—are integrated: - OpenAIProvider, AnthropicProvider, and GroqProviderLlama3 extend BaseLLMProvider. - X.AI reuses the existing OpenAI and Anthropic implementations by inheriting from their respective providers.\n\n\n\n\n\nflowchart BT\n    \n    BaseLLMProvider --&gt;|Implements| LLMProviderInterface\n\n    subgraph Inheritance[\"Inheritance Hierarchy\"]\n        direction BT\n        style Inheritance fill:#FFFFE0\n        OpenAIProvider --&gt;|Extends| BaseLLMProvider\n        AnthropicProvider --&gt;|Extends| BaseLLMProvider\n        GroqProviderLlama3 --&gt;|Extends| BaseLLMProvider\n        XAIProviderOpenAI --&gt;|Inherits from| OpenAIProvider\n        XAIAnthropicProvider --&gt;|Inherits from| AnthropicProvider\n    end"
  },
  {
    "objectID": "posts/2024-11-30-omnichat/index.html#adding-vision-capabilities",
    "href": "posts/2024-11-30-omnichat/index.html#adding-vision-capabilities",
    "title": "OmniChat - How to chat with any LLM",
    "section": "Adding Vision Capabilities",
    "text": "Adding Vision Capabilities\nTo add vision capabilities, we first update the append_user_message method in the ChatMessages class to accept images. Since each provider has a different format for handling images, the actual implementation is done in the provider-specific child classes.\nTo update the method, I use @patchfrom fastcore, which is a nice tool which allows to only add functionality as it is needed in a notebook.\n\nfrom fastcore.utils import * #for importing patch\n\n@patch\ndef append_user_message(self:ChatMessages, content=None, base64_images=None):\n    \"\"\"\n    Appends a user message with specified content\n    \"\"\"\n    if base64_images:\n        raise NotImplementedError(\"Vision is not supported by this model.\")\n    \n    if not content:\n        raise ValueError(\"Content cannot be empty or None.\")\n\n    self._append_message(\"user\", content)\n\nConsequently, we also need to update the prompt_model method of the ChatClient to allow for vision-based prompts:\n\n@patch\ndef prompt_model(self:ChatClient, prompt=None, base64_images=None):\n    \"\"\"\n    Sends a message to the model, including an optional prompt and multiple base64 images.\n    \n    :param prompt: The text prompt for the model.\n    :param base64_images: A list of base64-encoded image strings.\n    :return: The model's response in Markdown format.\n    \"\"\"\n    if base64_images:\n        # Append user message with multiple images\n        self._chat_messages.append_user_message(content=prompt, base64_images=base64_images)\n    elif prompt:\n        # Append user message with only text\n        self._chat_messages.append_user_message(content=prompt)\n\n    content = self._get_model_response()\n    self._chat_messages.append_assistant_message(content)\n    return Markdown(content)\n\nNow we can implement the different formats in the vendor-specific chat message classes. Pay attention to the subtle differences in how each provider handles vision inputs.\n\nclass OpenAIChatMessages(ChatMessages):\n\n    def append_user_message(self, content=None, base64_images=None):\n        \"\"\"\n        Appends a user message with specified content and multiple images to messages list.\n        As per https://platform.openai.com/docs/guides/vision\n        \"\"\"\n        if base64_images:\n            content_parts = [{\"type\": \"text\", \"text\": content}] if content else []\n            for base64_image in base64_images:\n                content_parts.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}\n                })\n            self._messages.append({\"role\": \"user\", \"content\": content_parts})\n        elif content:\n            self._append_message(\"user\", content)\n        else:\n            raise ValueError(\"Content cannot be empty or None.\")\n\n\nclass AnthropicChatMessages(ChatMessages):\n\n    def append_user_message(self, content=None, base64_images=None):\n        \"\"\"\n        Appends a user message with specified content and multiple images to messages list.\n        As per https://docs.anthropic.com/en/api/messages\n        \"\"\"\n        if base64_images:\n            content_parts = [{\"type\": \"text\", \"text\": content}] if content else []\n            for base64_image in base64_images:\n                content_parts.append({\n                    \"type\": \"image\",\n                    \"source\": {\n                        \"type\": \"base64\",\n                        \"media_type\": \"image/png\",\n                        \"data\": base64_image\n                    }\n                })\n            self._messages.append({\"role\": \"user\", \"content\": content_parts})\n        elif content:\n            self._append_message(\"user\", content)\n        else:\n            raise ValueError(\"Content cannot be empty or None.\")\n\nFinally, we need to update the chat_messages_factory method in all provider classes, so that they use the corresponding chat messages implementation.\n\n@patch\ndef chat_messages_factory(self:OpenAIProvider):\n    \"\"\"Returns the standard ChatMessages implementation for OpenAI.\"\"\"\n    chat_messages = OpenAIChatMessages()\n    chat_messages.append_system_message(self._system_prompt)\n    return chat_messages\n\n@patch\ndef chat_messages_factory(self:AnthropicProvider):\n    \"\"\"Returns the Anthropic-specific ChatMessages implementation.\"\"\"\n    return AnthropicChatMessages()\n\n@patch\ndef chat_messages_factory(self:GroqProviderLlama3):\n    \"\"\"OpenAI Format also works for Llama.\"\"\"\n    return OpenAIChatMessages()\n\n#@patch\n#def chat_messages_factory(self:XAIProviderAnthropic):\n#    \"\"\"Returns the Base ChatMessages implementation.\"\"\"\n#    return ChatMessages()\n\n#@patch\n#def chat_messages_factory(self:XAIProviderOpenAI):\n#    \"\"\"Returns the Base ChatMessages implementation.\"\"\"\n#    return ChatMessages()\n\n\nTesting Vision\nLet’s test the vision capabilities of our implementation using the famous Trolley Problem.\n\nPreparing the Image\nFirst, we encode the image in base64 and have a look:\n\n\nCode\nimport base64\nfrom IPython.display import Image, display\n\ndef encode_image(image_path):\n  \"\"\"Encodes an image file in base64\"\"\"\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\ndef render_base64_image(base64_string):\n    \"\"\"Render a Base64-encoded image in a Jupyter Notebook.\"\"\"\n    display(Image(data=base64.b64decode(base64_string)))\n\n\n\nimage_path = \"Trolley_Problem.png\"\nbase64_image = encode_image(image_path)\nrender_base64_image(base64_image)\n\n\n\n\n\n\n\n\n\n\nRunning the Models\nNow, let’s ask each model to describe the image:\n\nmodel_name = \"gpt-4o\"\nprovider = OpenAIProvider(model_name=model_name)\nchat_client = ChatClient(provider=provider)\nchat_client.prompt_model(\"What is in the image?\", base64_images=[base64_image])\n\nThe image depicts the classic “trolley problem,” a philosophical thought experiment. It shows a trolley on a track heading towards five people. There is a lever that can be pulled to switch the trolley onto another track, where it would hit one person instead. The scenario is used to discuss ethical decision-making and moral dilemmas.\n\n\n\nmodel_name = \"claude-3-5-sonnet-20241022\"\nprovider = AnthropicProvider(model_name=model_name, system_prompt=system_prompt, max_tokens=128)\nchat_client = ChatClient(provider=provider)\nchat_client.prompt_model(\"What is in the image?\", base64_images=[base64_image])\n\nThis is the classic “Trolley Problem” ethical dilemma illustrated in a simple diagram. It shows a trolley/tram on tracks with two possible paths: one track leads to five people, while the other track has one person. There’s also a figure shown near a lever that could switch the trolley’s direction.\n\n\n\nmodel_name = \"grok-vision-beta\"\nprovider = XAIProviderOpenAI(model_name=model_name)\nchat_client = ChatClient(provider=provider)\nchat_client.prompt_model(\"What is in the image?\", base64_images=[base64_image])\n\nThe image depicts a simplified diagram of a train or tram on tracks with a switch mechanism. There are two tracks: one leading straight ahead and another branching off to the right.\n\nOn the left side, there is a train or tram on the left track.\nIn the middle, there is a person standing near the switch mechanism.\nOn the right side, there are two groups of people:\n\nOne group is on the straight track ahead.\nAnother group is on the track that branches off to the right.\n\n\nThis diagram is often used to illustrate the trolley problem, a moral dilemma in ethics where one must decide whether to switch the train to another track to save a group of people at the cost of another individual.\n\n\n\nmodel_name = \"llama-3.2-90b-vision-preview\"\nprovider = GroqProviderLlama3(model_name=model_name)\nchat_client = ChatClient(provider=provider)\nchat_client.prompt_model(\"What is in the image?\", base64_images=[base64_image])\n\nThe image depicts a black-and-white graphic of a train and people. The train is on the left side of the image, with a diamond-shaped object above it. There are several lines extending from the train, each featuring a person icon. The lines vary in length, with some being longer than others. The background of the image is white.\nKey Elements:\n\nTrain: A black train with a diamond-shaped object above it.\nLines: Several lines extending from the train, each featuring a person icon.\nPerson Icons: Small black figures representing people.\nBackground: White background.\n\nInterpretation:\nThe image appears to be a simple graphic representation of a train and its passengers. The lines extending from the train may represent the paths or routes taken by the passengers. The person icons could symbolize the individuals traveling on the train. The diamond-shaped object above the train is unclear in its meaning but may represent a signal or a marker. Overall, the image conveys a sense of movement and transportation.\n\n\n\n\nObservations\nAs you can see, it is very easy to benchmarking different models against the same input. In this test, GPT-4o, Claude Sonnet 3.5, and Grok correctly recognized the Trolley Problem. Llama 3.2, however, just gave a generic description of the scene, missing the high-level concept. This demonstrates how the OmniChatClient design allows you to efficiently test and compare multiple models in a unified way."
  },
  {
    "objectID": "posts/2024-11-30-omnichat/index.html#conclusion",
    "href": "posts/2024-11-30-omnichat/index.html#conclusion",
    "title": "OmniChat - How to chat with any LLM",
    "section": "Conclusion",
    "text": "Conclusion\nBuilding the OmniChatClient has been an exciting project, creating a unified and extensible interface for working with multiple LLMs. By abstracting provider-specific functionality into modular components, we’ve reduced redundancy and created a scalable framework that can easily adapt to future requirements.\nDependency injection ensures that the OmniChatClient remains flexible and decoupled from provider-specific details, which are implemented in dedicated provider classes. Inheritance further reduces redundancies and accelerates the addition of new providers and features. As a result, any application using the OmniChatClient can seamlessly switch between models by simply swapping out the provider objects, no additional code changes are required.\nWhile this implementation is already functional and supports both text and vision capabilities, there’s still room to expand. Enabling tool usage and adding support for providers like Google’s Gemini series are in my backlog.\nOn a broader level, I hope this hands-on demonstration of dependency injection and inheritance, applied in a practical example, inspires you to experiment with these patterns to make your code more robust, flexible, and elegant."
  },
  {
    "objectID": "posts/2024-10-22-how-to-run-javascript-in-a-jupyter-notebook/index.html",
    "href": "posts/2024-10-22-how-to-run-javascript-in-a-jupyter-notebook/index.html",
    "title": "How to Run JavaScript in a Jupyter Notebook",
    "section": "",
    "text": "I decided it was time to learn some JavaScript. Since I’m used to working in Jupyter notebooks for exploratory coding in Python, and my blog is also entirely written in Jupyter notebooks, I explored how to run a JavaScript kernel within a Jupyter notebook. While I realize this is not a common way to write JavaScript code, it’s a practical solution for embedding real JavaScript code directly into my blog posts, which are themselves built with Jupyter.\nFirst, we will install IJavascript, a Jupyter kernel that enables running JavaScript code directly within Jupyter notebooks. After the setup, we will explore some basics of JavaScript, and I’ll share some of my first lessons learned from using JavaScript in Jupyter notebooks, drawing comparisons to Python along the way."
  },
  {
    "objectID": "posts/2024-10-22-how-to-run-javascript-in-a-jupyter-notebook/index.html#installation-of-ijavascript",
    "href": "posts/2024-10-22-how-to-run-javascript-in-a-jupyter-notebook/index.html#installation-of-ijavascript",
    "title": "How to Run JavaScript in a Jupyter Notebook",
    "section": "Installation of IJavascript",
    "text": "Installation of IJavascript\nBefore we can install IJavascript, we need to set up some basic components for JavaScript development:\n\nNode.js as the JavaScript runtime\nNPM (Node Package Manager) for installing packages\nNVM (Node Version Manager) to manage different versions of Node.js\n\nWhile it is possible to install IJavascript directly after setting up these components, I encountered some dependency issues during the process. To resolve these, I switched to the latest LTS (Long-Term Support) version of Node.js. At the time of writing, this step was necessary, but it may become obsolete in future versions.\n\nNote: Everything I describe in this blog post is based on macOS. If you’re using Windows or Linux and have adapted this approach, I’d love to hear about your experience.\n\n\nXcode Command Line Tools\nThe Xcode Command Line Tools are a set of macOS development tools that provide essential software for compiling code and performing development tasks.\nTo check if they are installed, run:\nxcode-select -p\nIf they are not installed, you can install them by running:\nxcode-select --install\n\n\nNVM (Node Version Manager)\nWe will use NVM (Node Version Manager) to set up our JavaScript environment. This is the preferred method over installing via Homebrew because NVM allows you to easily switch between different versions of Node.js. Additionally, when installing IJavascript, I encountered dependency issues with the latest version of Node.js. Therefore, we need to install the latest LTS (Long-Term Support) version of Node.js in the next step.\nTo check if NVM is installed, run:\nnvm -v\nIf NVM is not installed, you can install it by running:\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash\nAfter installation, load nvm into your current terminal session by running:\nsource ~/.bashrc   # if you use bash\nsource ~/.zshrc    # if you use zsh\n\n\nInstallation of latest LTS (Long-Term Support) version of Node.js\nIf you are new to NVM and previously installed Node.js via Homebrew, it’s recommended to uninstall the Homebrew version to avoid conflicts:\nbrew uninstall node\nTo resolve dependency issues, I switched to the latest LTS (Long-Term Support) version of Node.js. Use nvm to install and activate this version:\nnvm install --lts`\nnvm use --lts`\nYou can verify the installed versions of Node.js and NPM by running:\nnode -v\nnpm -v\nAfter installing the LTS version, it’s a good idea to update npm to the latest version:\nnpm install -g npm@latest\n\n\nInstallation of IJavascript\nOnce you’ve switched to the LTS version of Node.js, you can install IJavascript by running:\n    npm install -g ijavascript\nThe npm command is similar to pip in the Python world, which is used for installing packages. The -g parameter ensures that the package is installed globally, making IJavascript available across all projects using the active Node.js version.\nAfter the installation, register IJavascript as a Jupyter kernel with the following command:\nijsinstall\n\n\nUsing JavaScript in Jupyter Notebook (in VS Code)\nAfter having completed all installation steps, you can run JavaScript code in a Jupyter notebook. Note that you need to restart VS Code.\nWhen you run the following cell, you will be asked to select a kernel in VS Code. Select IJavascript.\n\nconsole.log(\"Hello World!\")\n\nHello World!"
  },
  {
    "objectID": "posts/2024-10-22-how-to-run-javascript-in-a-jupyter-notebook/index.html#first-steps-in-javascript",
    "href": "posts/2024-10-22-how-to-run-javascript-in-a-jupyter-notebook/index.html#first-steps-in-javascript",
    "title": "How to Run JavaScript in a Jupyter Notebook",
    "section": "First steps in JavaScript",
    "text": "First steps in JavaScript\nLet’s explore JavaScript beyond just a simple “Hello, World!”.\n\nDeclaring Variables\nUnlike Python, variables in JavaScript need to be explicitly declared before they are used. Here is an example:\nlet hello;\nIn the context of Jupyter notebooks, this can lead to undesired behavior where cells containing variable declarations can only be executed once. The simplest way to avoid unwanted syntax errors is to separate the declaration of variables and the actual code into two separate cells.\nSince JavaScript typically declares and initializes variables at the same time (for example, let hello = \"Hello\";), this separation of declaration and initialization does not feel very “JavaScripty” (similar to how “Pythonic” refers to clear and idiomatic Python code). However, this trade-off works well for using JavaScript in Jupyter notebooks, which, by itself, is not a very “JavaScripty” way of coding. Jupyter’s cell-based, interactive workflow differs significantly from how JavaScript is usually written and executed.\nHere is an example to show the separation of declaration and initialization. Just be mindful not to execute the declaration cells multiple times.\n\nlet greeting1;\nlet greeting2;\n\n\ngreeting1 = \"Hello\";\ngreeting2 = 'World!';\n\nconsole.log(greeting1, greeting2);\n\nHello World!\n\n\nBy the way, notice that just like in Python, in JavaScript a string represents a sequence of characters and can be enclosed in either single quotes (') or double quotes (\").\nAs a best practice that applies to both Python and JavaScript:\n\nUse single quotes (') by default.\nSwitch to double quotes (\") when the string contains a single quote (e.g., an apostrophe) to avoid using escape characters.\n\nAnother interesting aspect of variables in JavaScript is how their scope is defined. If you declare variables using let, the scope of the variable is limited to the block (enclosed by curly braces {}) where it is declared. Unlike Python, this could include blocks like if statements, for loops, or any code wrapped in curly braces.\nHere is a example:\n\nif ('the stars align' === 'the stars align') {\n    let light = 'shine bright';\n}\n\n// Checking if the light still shines beyond the block\nconsole.log(typeof light === 'undefined' ? 'The light fades into the void...' : 'The light endures.');\n\nThe light fades into the void...\n\n\nApart from the definition of the scope that we can observe in the example above, there are a few other noteworthy elements.\nFirst, the comparison is done with ===, which checks for strict equality. This means it checks both the value and the type, making it more precise than ==, which only checks for value equality and can sometimes lead to unexpected results due to type coercion. (In Python, there isn’t a direct equivalent to ===, but Python’s == behaves more like JavaScript’s strict === by default, without implicit type conversion.)\nAdditionally, JavaScript supports a ternary operator (just like Python, though it is more commonly used in JavaScript), which is a shorthand way of writing an if-else statement:\nJavaScript:\ncondition ? expressionIfTrue : expressionIfFalse;\nPython:\nexpressionIfTrue if condition else expressionIfFalse\nFor completeness, here’s the traditional way of writing out the condition from the example above:\nif (typeof light === 'undefined') {\n    console.log('The light fades into the void...');\n} else {\n    console.log('The light endures.');\n}"
  },
  {
    "objectID": "posts/2024-10-22-how-to-run-javascript-in-a-jupyter-notebook/index.html#implementing-a-pyramid-generator",
    "href": "posts/2024-10-22-how-to-run-javascript-in-a-jupyter-notebook/index.html#implementing-a-pyramid-generator",
    "title": "How to Run JavaScript in a Jupyter Notebook",
    "section": "Implementing a Pyramid Generator",
    "text": "Implementing a Pyramid Generator\nTo wrap up this blog post, here’s some slightly more complex code. In parallel, I’ve been following a tutorial on building a pyramid generator in JavaScript. Below is my Jupyter notebook version, followed by some observations.\n\nconst character = \"#\";\nconst rows = [];\nlet pyramidHeight;\nlet inverted;\nlet result;\n\n\nrows.length = 0;  // Reset the array to an empty list\npyramidHeight = 8;\ninverted = false;\nresult = \"\";\n\n// Generates the spaces needed to center the pyramid row\nfunction generateWhitespace(rowNumber, pyramidHeight) {\n    return \" \".repeat(pyramidHeight - rowNumber)\n}\n\n// Generates the pyramid characters (e.g., \"###\" or \"#####\")\nfunction generatePyramid(rowNumber){\n    return character.repeat(2 * rowNumber - 1)\n} \n\n// Combines whitespace and pyramid characters to create a centered row\nfunction generatePyramidRow(rowNumber, pyramidHeight) {\n    const whitespace = generateWhitespace(rowNumber, pyramidHeight);\n    const pyramid = generatePyramid(rowNumber);\n    return `${whitespace}${pyramid}${whitespace}`;\n}\n\nfor (let rowNumber = 1; rowNumber &lt;= pyramidHeight; rowNumber++) {\n    if (inverted) {\n        rows.unshift(generatePyramidRow(rowNumber, pyramidHeight));\n    } else {\n        rows.push(generatePyramidRow(rowNumber, pyramidHeight));\n    }\n}\n\n8\n\n\nNotice the output of the cell, which is equal to the height of the pyramid. This is due to Jupyter’s behavior of displaying the result of the last evaluated statement. It can be suppressed by adding a dummy line at the end, like void 0; (which produces undefined).\n\nconsole.log(rows);\n\n[\n  '         #         ',\n  '        ###        ',\n  '       #####       ',\n  '      #######      ',\n  '     #########     ',\n  '    ###########    ',\n  '   #############   ',\n  '  ###############  ',\n  ' ################# ',\n  '###################'\n]\n\n\n\nresult = rows.join(\"\\n\");\nconsole.log(result);\n\n       #       \n      ###      \n     #####     \n    #######    \n   #########   \n  ###########  \n ############# \n###############\n\n\nThe code still looks somewhat unfamiliar compared to what I’m used to in Python. Here are some noteworthy points:\n\nI separated the declaration of the variables and their initialization into a “declaration cell” and a “code cell” to be able to run the code cell multiple times in my notebook without re-declaring variables.\nJavaScript differentiates strictly between variables and constants. In Python, the concept of a constant is not enforced, it’s just a naming convention to use uppercase (SPEED_OF_LIGHT = 299792458). In JavaScript, however, a constant truly is a constant, and the language enforces that it cannot be changed (const speedOfLight = 299792458;).\nThe fact that the list of rows is also defined as a constant still looks odd to me. However, using const for arrays and objects is considered a good practice in JavaScript because it prevents the variable from being accidentally reassigned. const ensures the variable refers to the same instance, but you can still modify the instance’s contents.\nThe comments describing the functions are not directly comparable to docstrings in Python. To create similar in-line documentation, you would use JSDoc in JavaScript.\n${whitespace}${pyramid}${whitespace} is a template literal, which is the JavaScript equivalent of a Python f-string. Template literals are enclosed in backticks (`) and allow you to inject dynamic content using ${name}."
  },
  {
    "objectID": "posts/2024-10-22-how-to-run-javascript-in-a-jupyter-notebook/index.html#conclusion",
    "href": "posts/2024-10-22-how-to-run-javascript-in-a-jupyter-notebook/index.html#conclusion",
    "title": "How to Run JavaScript in a Jupyter Notebook",
    "section": "Conclusion",
    "text": "Conclusion\nMy first few days with JavaScript have been an interesting ride. As stated above, my approach is likely quite unusual, but I feel it’s effective for my personal learning journey. The familiar environment of Jupyter notebooks allows me to see results quickly without having to focus too much on setup or infrastructure. I’m certain, however, that future JavaScript projects will look different as I explore more traditional ways of working with the language."
  },
  {
    "objectID": "posts/2026-01-09-lets-test-rpt-1-on-titanic/index.html",
    "href": "posts/2026-01-09-lets-test-rpt-1-on-titanic/index.html",
    "title": "Let’s test RPT-1 on Titanic",
    "section": "",
    "text": "How do you test SAP’s RPT-1 model beyond a simple Hello World example? Its claim to fame is that it can replace traditional machine learning models for classification and regression tasks. No model training, no hyperparameter tuning, just show it your data and it predicts. That’s a bold promise that deserves testing!\nBack when I started learning about machine learning, one of my first projects was tackling Kaggle’s Titanic Challenge in which you get a dataset with information about the passengers of the Titanic and you have to predict which of them survived. It’s not a classic SAP use case, but I think it makes a good first benchmark: To score well, you typically need to clean data, engineer features, and iterate on your model. Would RPT-1 perform well on raw data? Could I improve results by tuning the input? To find out, I decided to board the Titanic again. Here’s what I discovered.\nSpoiler Alert: RPT-1 performed well, and I learned a lot about how RPT-1 works, including a bit of mystery and surprise. If you are looking for technical details, I have posted Coding Walkthrough on GitHub. In this blog post, I will focus on the insights and lessons learned. So let’s jump right in."
  },
  {
    "objectID": "posts/2026-01-09-lets-test-rpt-1-on-titanic/index.html#how-rpt-1-works",
    "href": "posts/2026-01-09-lets-test-rpt-1-on-titanic/index.html#how-rpt-1-works",
    "title": "Let’s test RPT-1 on Titanic",
    "section": "How RPT-1 works",
    "text": "How RPT-1 works\nIf you haven’t worked with RPT-1 before, here’s the basic idea: It uses in-context learning, similar to how Large Language Models work. Instead of training a model on your data, you show it examples, and it learns the patterns on the fly without any traditional training step. Think of it like showing a child examples before asking them to solve a new problem.\nFor the Titanic challenge, this means we give RPT-1 a table of passengers where we know if they survived or not. Then we add one or more row(s) for passengers where survival is unknown, masking the “Survived” fields with a [PREDICT] placeholder. Therefore, the model looks at the patterns in the known data and fills in the blanks.\nHere’s what a simple (and simplified) payload looks like: (For the full payload structure, please refer to the Coding Walkthrough.)\n{\n    \"prediction_config\": {\n        \"target_columns\": [{\n            \"name\": \"Survived\",\n            \"prediction_placeholder\": \"[PREDICT]\"\n        }]\n    },\n    \"index_column\": \"PassengerId\",\n    \"rows\": [\n        {\"PassengerId\": 1, \"Pclass\": 3, \"Sex\": \"male\", \"Age\": 22, \"Survived\": \"0\"},\n        {\"PassengerId\": 2, \"Pclass\": 1, \"Sex\": \"female\", \"Age\": 38, \"Survived\": \"1\"},\n        {\"PassengerId\": 3, \"Pclass\": 3, \"Sex\": \"female\", \"Age\": 26, \"Survived\": \"[PREDICT]\"}\n    ]\n}\nThe model sees that passenger 1 (male, 3rd class) didn’t survive, passenger 2 (female, 1st class) did. Now it can use these patterns (women and higher classes survived more often) to predict passenger 3. Pretty elegant, right?\nHere is an example of how the prediction result looks like:\n[\n  {\n    \"PassengerId\": 3,\n    \"Survived\": [\n      {\n        \"confidence\": 0.89,\n        \"prediction\": \"1\"\n      }\n    ]\n  }\n]\nRPT-1 therefore predicts that passenger 3 survived with a confidence of 89%. A few things stand out here: First, you didn’t have to train a model to get this result. Second, that confidence score is actually meaningful. Sure, you could ask an LLM “how confident are you?” and it would happily give you a number, but that’s just another token prediction, making up a plausible-sounding answer. RPT-1’s confidence comes from the actual probability distribution over possible values produced by the model, which is a fundamentally different (and more useful) thing."
  },
  {
    "objectID": "posts/2026-01-09-lets-test-rpt-1-on-titanic/index.html#experimenting-with-training-data",
    "href": "posts/2026-01-09-lets-test-rpt-1-on-titanic/index.html#experimenting-with-training-data",
    "title": "Let’s test RPT-1 on Titanic",
    "section": "Experimenting with Training Data",
    "text": "Experimenting with Training Data\nSince Kaggle limits you to 10 submissions per day, and I wanted to run a lot more experiments, I initially planned to only work on the training data where we know the ground truth, masking some “Survived” values, and see if RPT-1 could predict them. This way I could measure performance without burning through my daily submission quota.\nTechnically, my plan worked perfectly. The results, however, were… suspicious. When I masked 400 out of 891 rows and asked RPT-1 to fill them in, it predicted with 98% accuracy. To put that in perspective: the simple baseline of “women survive, men don’t” gets you about 76%. Anything above 80% is highly competitive, and no serious Kaggle submission breaks 90%. A traditional Gradient Boosting model I trained on the same data scored around 80%. So either RPT-1 is doing something magical, or something fishy is going on.\nMy first thought: Maybe RPT-1 has seen the Titanic dataset during training and is simply regurgitating the answers? To test this, I tried predicting passenger names instead of survival. Names are completely random, so if RPT-1 memorized the dataset, it should nail them too - but it didn’t. Instead, it predicted names from the unmasked rows in its context window, with very low confidence. A similar test with ticket numbers (which have some patterns since families shared tickets) showed modest results. RPT-1 was clearly pattern-matching from context, not memorizing from training.\nBut that still didn’t explain the 98% accuracy on survival predictions. So I ran one more experiment: What happens when I severely limit the context? I kept only 2 unmasked rows (the minimum allowed) and tried three variations:\n\nOne survivor, one non-survivor in context → reasonable predictions\nTwo survivors in context → predicted everyone survived\nTwo non-survivors in context → predicted everyone died\n\nThe result was reassuring: RPT-1 was only predicting values it had seen in the context window (same as we had seen with names and ticket numbers). It also “resisted” creating a near-perfect result (scoring 68%) when both “0” and “1” were in the context, even though this would have opened the door for regurgitation.\nWhen I tested how much context RPT-1 needed to produce competitive results, it already scored 85% on the test data with only 20 unmasked entries in the context window (surprisingly high!). With only 20 examples out of 891, the model shouldn’t have enough signal generalize well and to outperform Gradient Boosting trained on 491 examples. Put simply, it improved too much. Why? Honestly, I still don’t fully understand it. The mystery remains unsolved. If you have a theory, I’d love to hear it. But rather than speculate further, I decided to move on to what really matters: How would RPT-1 perform when it actually had to predict unknown outcomes (the test set)?"
  },
  {
    "objectID": "posts/2026-01-09-lets-test-rpt-1-on-titanic/index.html#the-real-test-kaggle-submissions",
    "href": "posts/2026-01-09-lets-test-rpt-1-on-titanic/index.html#the-real-test-kaggle-submissions",
    "title": "Let’s test RPT-1 on Titanic",
    "section": "The Real Test: Kaggle Submissions",
    "text": "The Real Test: Kaggle Submissions\nTime to put RPT-1 to the real test. I combined the training data (with known outcomes) and test data (with [PREDICT] placeholders), sent it to RPT-1, and submitted the predictions to Kaggle.\nWith raw data and zero preprocessing, RPT-1 scored 0.76555, matching the gender-based baseline. If you’ve tried this competition yourself, you know that’s actually not trivial. Many first attempts fall short of even this baseline.\nNext, I tried some standard feature engineering: Extracting titles from names, creating family size indicators, binning ages. Surprisingly, the score barely budged (0.76794). In fact, adding just the Title feature actually dropped the score to 0.76076. This suggested RPT-1 was already extracting similar signals from the raw data, or perhaps the additional columns were adding noise.\nKnowing that higher score should be achievable, I created more advanced features that capture social context, for example survival rates of passengers sharing the same surname or ticket number. Indeed, the score increased significantly to 0.78229. But how does this compare to traditional machine learning approaches? Let’s find out."
  },
  {
    "objectID": "posts/2026-01-09-lets-test-rpt-1-on-titanic/index.html#how-does-this-compare-to-traditional-ml",
    "href": "posts/2026-01-09-lets-test-rpt-1-on-titanic/index.html#how-does-this-compare-to-traditional-ml",
    "title": "Let’s test RPT-1 on Titanic",
    "section": "How Does This Compare to Traditional ML?",
    "text": "How Does This Compare to Traditional ML?\nTo compare RPT-1’s scores (small and large) with traditional ML, I trained Gradient Boosting models along the way for comparison using the same engineered features we gave to RPT-1. Here is the full result table for the different setups:\n\n\n\n\n\n\n\n\n\nSetup\nRPT-1-Small\nRPT-1-Large\nGradientBoosting\n\n\n\n\nNo feature engineering\n0.76555\n0.76555\n—\n\n\nWith title feature\n0.76076\n0.76076\n—\n\n\nFull feature engineering\n0.76794\n0.76794\n0.76555\n\n\nAdvanced feature engineering\n0.77990\n0.78229\n0.77751\n\n\n\nInterestingly, both model sizes scored identically on simpler setups, with RPT-1-Large only pulling ahead with advanced features. This may indicate that the small model might be good enough for a dataset of ~1000 lines, but clearly this needs more testing with different use cases. Overall RPT-1-Large wins, even if it’s only by ~0.3% compared RPT-1-Small or ~0.5% compared to Gradient Boosting, this is a real difference in this competition.\nBut the scores only tell half the story. The more interesting story is how RPT-1 wins. When I first trained Gradient Boosting with the advanced features, it actually scored lower than the baseline — classic overfitting. I had to add cross-validation, tune hyperparameters, and simplify the feature set to get to 0.77751. That’s the normal ML workflow: train, evaluate, iterate, repeat.\nUsing RPT-1 I could just add the new features and got results. No retraining, no hyperparameter tuning, no overfitting headaches. The same features that caused Gradient Boosting to overfit worked fine for RPT-1 out of the box.\nThis advantage compounds in real-world scenarios. Imagine your customer data changes weekly. With traditional ML, that’s weekly retraining jobs, validation runs, and deployment cycles. With RPT-1, you just update the context. Whether this holds for enterprise use cases remains to be tested, but the potential is significant and it would fundamentally simplify the operational model."
  },
  {
    "objectID": "posts/2026-01-09-lets-test-rpt-1-on-titanic/index.html#conclusion",
    "href": "posts/2026-01-09-lets-test-rpt-1-on-titanic/index.html#conclusion",
    "title": "Let’s test RPT-1 on Titanic",
    "section": "Conclusion",
    "text": "Conclusion\nRPT-1 didn’t just survive the Titanic test, it came out a winner. What can we learn from this exercise? I think we need to look at this from two angles: How does RPT-1 compare to traditional ML models and how does it compare to LLMs.\nSAP positions RPT-1 as a replacement for traditional ML models. And it does deliver on that promise. Without any training, it matched and slightly outperformed a tuned Gradient Boosting model. That’s remarkable. You can throw structured data at it and get competitive predictions without a typical Machine Learning workflow: No model selection, no hyperparameter tuning, no train-test splits to manage.\nFeature engineering still matters, but differently. Basic features like extracting titles barely moved the needle, RPT-1 already seemed to pick up these patterns from the raw data. But features capturing deeper structure (like group survival rates in this case) made a real difference. Zooming out, this saves time and effort when implementing a use case, but meaningful features which focus on domain insights and which aren’t obvious still are worth engineering.\nWe could also see an additional workflow advantage when Gradient Boosting overfit on my advanced features. To really make use of the new features, I had to iterate and add cross-validation, tune parameters, simplify the feature set. With RPT-1, the same features just worked. This is nice for quick experiments (like Titanic), but when projecting this forward into production systems that need regular updates with new data, that’s potentially transformative.\nSAP also calls RPT-1 a foundation model (for tabular data), so how does it compare to an LLM? As we have seen its output is quite different from that of an LLM, but it is worth noting that, unlike LLMs, RPT-1 produces deterministic results. You can run the same payload multiple times and get the same result. This is a big advantage over LLMs, where you always get slightly different answers. Additionally, RPT-1 produces real confidence scores, something an LLM cannot do. Both of these points clearly make RPT-1 a different kind of model, giving you LLM-style in-context learning, but with ML-style predictive results.\nThe 98% mystery on masked training data remains. If you have a theory, I’d love to hear it. Nonetheless, my overall impression of RPT-1 is very good and I am looking forward to moving beyond the small Titanic dataset to more enterprise-like use cases to see how RPT-1 performs on more complex scenarios. What enterprise scenarios would you like to see tested?"
  },
  {
    "objectID": "posts/2022-10-13-visualizing-gradient-descent-in-3d/index.html",
    "href": "posts/2022-10-13-visualizing-gradient-descent-in-3d/index.html",
    "title": "Visualizing Gradient Descent in 3D",
    "section": "",
    "text": "If you want to understand Machine Learning you have to understand gradient descent, we have all heard that before ;). Since I am a visual person, I tried to not only think through the concept, but also to visualize it.\nBased on Jeremy’s great notebook “How does a neural net really work?”, I created a notebook which visualizes gradient descent in 3D. There are two version:"
  },
  {
    "objectID": "posts/2022-10-13-visualizing-gradient-descent-in-3d/index.html#the-backstory",
    "href": "posts/2022-10-13-visualizing-gradient-descent-in-3d/index.html#the-backstory",
    "title": "Visualizing Gradient Descent in 3D",
    "section": "The backstory",
    "text": "The backstory\nGradient descent is one of the topics of lesson 3 of the 2022-Fast.AI-Course. On a high level, it is pretty straight forward:\n\nCalculate the predictions and the loss (forward-pass)\nInitialize and calculate the gradients (i.e. derivatives of the parameters, i.e. how does changing the parameters change the loss) (backward-pass)\nUpdate the parameters (via the learning rate)\nRestart\n\nLooking at the python code, however, it is very compact, and a lot of magic is going on. Trying to unpack this and to get a solid and intuit understanding of gradient descent, I tried to not only think through the concept, but also to visualize it.\nI started playing with Jeremy’s notebook, and what started out as a rough idea turned into the notebooks on Kaggle and GitHub.\nI learned a lot about gradient descent and python (especially plotting) along the way, and I hope you find the visualizations useful."
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html",
    "href": "posts/2024-03-15-embeddings/index.html",
    "title": "Visualizing Embeddings in 2D",
    "section": "",
    "text": "Did you ever try to explain what embeddings are to people who have no or only a limited background in machine learning or computer science? I recently tried this in an on-the-fly attempt to explain embeddings with an analogy to animals. While I think I could get the idea across, this analogy has stuck in my mind, and here is version 2.0: Let’s explore how we can visualize the embeddings of terms like “lion”, “tiger”, or “flamingo” to illustrate how a machine learning model understands the meaning of these terms and perceives their semantic relationships to one another.\nHere is the plan:\nIn the end, you will understanding exactly how the visualization was created and I hope you will have a more intuitive understanding of the underlying concepts of embeddings. If you feel that the math is to heavy, please just focus on the underpinning ideas."
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html#from-technical-to-visualization",
    "href": "posts/2024-03-15-embeddings/index.html#from-technical-to-visualization",
    "title": "Visualizing Embeddings in 2D",
    "section": "From Technical to Visualization",
    "text": "From Technical to Visualization\nHere is the technical definition:\n\n“In the context of machine learning and natural language processing, embeddings are numerical vector representations that capture the semantic essence of text entities, such as words, sentences, or documents. These vectors are typically high-dimensional, often consisting of hundreds or thousands of dimensions, allowing them to encode complex concepts and relationships. The fundamental idea behind embeddings is that texts with similar meanings are represented by vectors that are mathematically close to each other. This representation enables algorithms to process and analyze texts by understanding their underlying semantic content.”\n\nNow, let’s transition from theory to practice. Here are the final visualization which show how our machine learning model “thinks” about various animals, illustrating the concept of embeddings in charts that represent their semantic relationships. Why 2 charts? The left one is the more intuitive one for us humans and the right one illustrates better how the machine thinks.\n\n\n\nAnimal Embeddings CC-BY https://creativecommons.org/licenses/by/4.0/\n\n\n\nNotes: For readability I hidden most of the code from this blog post. The all the details, please check out the notebook version on Github. For readability I curated the dataset so that the animal groups are nicely separated. By reducing animals to just 2 numbers, a lot of complexity is lost, but the intent not to create a highly accurate model of the worlds, rather I wanted to present an example which is simple enough so be visualized in 2D to allow you to build some intuition on how embeddings work. Please feel free to experiment yourself in the notebook version on Github."
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html#visualizing-embeddings-with-a-small-dataset",
    "href": "posts/2024-03-15-embeddings/index.html#visualizing-embeddings-with-a-small-dataset",
    "title": "Visualizing Embeddings in 2D",
    "section": "Visualizing Embeddings with a Small Dataset",
    "text": "Visualizing Embeddings with a Small Dataset\nLet’s start small and use a dataset with the examples of “lion”, “tiger”, “flamingo”, and “clownfish”.\n\nEmbeddings from model\nInspired by the hackers guide by Jeremy Howard let’s use this model to calculate the embeddings via SentenceTransformers\n\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\nexample_animals = [\"lion\", \"tiger\", \"flamingo\", \"clownfish\"]\n\n# Initialize model\nemb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=\"cpu\")\n\n# Compute embeddings\nexample_animals_embeddings = emb_model.encode(example_animals, convert_to_tensor=True).cpu().detach().numpy()\n\n\nexample_animals_embeddings.shape\n\n(4, 384)\n\n\nEach word is now represented by a 384-dimensional vector. What does this mean, and where do these numbers come from?"
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html#training-an-embedding-model",
    "href": "posts/2024-03-15-embeddings/index.html#training-an-embedding-model",
    "title": "Visualizing Embeddings in 2D",
    "section": "Training an Embedding Model",
    "text": "Training an Embedding Model\nThe model we use is BAAI/bge-small-en-v1.5. BAAI stands for “Beijing Academy of Artificial Intelligence”. Is a private non-profit organization known for its research and development in artificial intelligence technologies. BGE stands for “BAAI general embedding”.\nDiving into theit GitHub repo, we can read that this model has been trained in English (and there are Chinese and multi-language models available). It is a general embedding model which has been pre-trained using RetroMAE. Subsequently, it has been trained on large-scale pair data using contrastive learning.\nIn the RetroMAE pre-training phase, the model has been exposed to vast amounts of text data, such as Wikipedia and BookCorpus to learn a wide range of language patterns, contextual relationships, and the nuances of semantics without specific task-oriented guidance. Contrastive learning has taught the model to pull the embeddings of texts that are similar (positive pairs) closer to each other and push apart embeddings of texts that are dissimilar (negative pairs). It’s through these methods that the model learns to understand and encode the semantic essence of texts into vectors.\nEssentially, an embedding is a numerical representation of a text. Unlike hashes, which primarily aim at crating unique representations of stings for retrieval or data integrity, embeddings are designed to capture semantic meaning and relationships between pieces of text. As we will see, embeddings of “lion” and “tiger” are mathematically more similar to each other than “tiger” and “flamingo”, capturing their semantic meaning and similarity."
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html#what-is-dimensionality-reduction",
    "href": "posts/2024-03-15-embeddings/index.html#what-is-dimensionality-reduction",
    "title": "Visualizing Embeddings in 2D",
    "section": "What is Dimensionality Reduction?",
    "text": "What is Dimensionality Reduction?\nTrying to understand how these 384 dimensions can describe a single word is impossible beyond the trust that these number can magically describe our 4 animals. To gain a more intuitive understanding of embeddings, we can reduce this high-dimensional space to something more manageable, like two dimensions. You can think of this as projecting an object with a torch to the wall, the 3D object is projected to 2D. However, it is important to do the projection in a way which preserves important information. Check out to the following visualization @visualizevalue to the the potential problem.\n\n\n\nProjection\n\n\nOne effective algorithm for dimensionality reduction is Principal Component Analysis (PCA) which simplifies the data while preserving its most significant patterns.\n\nNote: The remainder of this section explains how PCA works. If you prefer to focus on the results, feel free to skip ahead.\n\nPCA transforms the original high-dimensional variables into a new set of lower dimensional variables, the principal components, which capture the maximum variance in the data. Therefore, the data’s variability is preserved as much as possible. As a result, noise is reduced without filtering out essential information, making it easier to observe patterns, clusters, or relationships that were not apparent in the higher-dimensional space.\nSince the math was not 100% self-explanatory to me, I created a separate deep-dive notebook exploring PCA which reduces the dimensionality of a 3D-dataset to 2D, including interactive visualizations. Here is the executive summary which visually takes you through the process:\n\n\n\nPCA Steps\n\n\n\nFigure 1: This is out example dataset with 10 samples of 3D points\nFigure 2: The 2 vectors are the principal components (PC1 and PC2) capturing the maximum variance in the data. We construct a plane from the 2 vectors, the projection plane.\nFigure 3: The 3D-points are projected onto the principal components plane.\n\nFigure 4: The projected points on the plane in 3D space\nFigure 5: The projected points rotated in a way that we can see the 2D projection in 3D space from above, simulating the reduced dimensionality\nFigure 6: The final 2D representation of the data\n\nWith a clear understanding of dimensionality reduction, we can now apply PCA to our example dataset containing “lion,” “tiger,” “flamingo,” and “clownfish” and visualize the outcome."
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html#applying-dimensionality-reduction",
    "href": "posts/2024-03-15-embeddings/index.html#applying-dimensionality-reduction",
    "title": "Visualizing Embeddings in 2D",
    "section": "Applying Dimensionality Reduction",
    "text": "Applying Dimensionality Reduction\nSklearn offers an easy to consume implementation to apply PCA to out example dataset.\n\nfrom sklearn.decomposition import PCA\n\n# Apply PCA to reduce to 2 dimensions\npca = PCA(n_components=2)\nexample_animals_embeddings_2d = pca.fit_transform(example_animals_embeddings)\n\nHere is the result, both numerically and plotted in 2D.\n\nexample_animals_embeddings_2d\n\narray([[-0.35093537, -0.07018732],\n       [-0.4075373 ,  0.02734617],\n       [ 0.3408063 ,  0.40827572],\n       [ 0.41766608, -0.3654344 ]], dtype=float32)\n\n\n\n\n\n\n\n\n\n\n\nAs we can easily see, “lion” and “tiger” are closer to ech other then “tiger” and “flamingo”. After the first visual “proof” let’s explore how we can calculate the distance mathematically because this is how the machine evaluates the similarity of text."
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html#calculating-distance",
    "href": "posts/2024-03-15-embeddings/index.html#calculating-distance",
    "title": "Visualizing Embeddings in 2D",
    "section": "Calculating Distance",
    "text": "Calculating Distance\nWhat we intuitively do when looking at the chart above is to calculate the so-called Euclidean distance. We see that “lion” and “tiger” are close to each other while the other dots are farther away. While we will do a quick implementation for calculating the Euclidean distances in the next sub-section, it turns out, however, that there are better ways to calculate similarity between vectors. This is why subsequently, we will dive into calculating cosine similarity, followed by a discussion on why cosine similarity is better for calculating the similarity between 2 vectors.\n\nCalculating Euclidean Distance\nHere is a visualization of the euclidean distances for our example dataset, confirming our observations.\n\n\n\n\n\n\n\n\n\nIt is also possible to do this calculation in higher dimensionality, but as we discussed above, the orientation of the vectors is more significant than their magnitude. Therefore, let’s turn our attention to cosine similarity.\n\n\nCalculating Cosine Similarity\nCosine similarity focuses on the orientation of the vectors with respect to each other without considering their magnitudes (lengths). It measures the similarity between two vectors as the cosine of the angle between them. Vectors pointing in the same direction (regardless of their length) have a cosine similarity of 1, indicating they are very similar. Vectors at 90 degrees to each other have a cosine similarity of 0, indicating no similarity, and vectors pointing in opposite directions have a cosine similarity of -1, indicating completely dissimilar. This principle holds true in higher-dimensional spaces as well. For instance, two vectors in a 3D space will adhere to the same value range for their cosine similarity. Hence, this measure can effectively express the similarity between vectors across any number of dimensions, focusing on how vectors are oriented with respect to each other rather than how far apart they are. This relation is plotted in the following cosine graph, which is colored to indicate similarity.\n\n\n\n\n\n\n\n\n\nLet’s transfer this our animal example. When thinking in terms of cosine similarity, we need to plot our 4 animals differently. Each animal is represented as vectors, and the magnitude of the vectors are normalized. Additionally, starting with the flamingo, the cosine of the angle in relation to “flamingo” is colored to indicate similarity.\n\n\n\n\n\n\n\n\n\n\n\nEuclidean Distance vs. Cosine Similarity\nWhen comparing Euclidean distance and cosine similarity, it’s important to consider various aspects that highlight the strengths and limitations of each measure in different contexts. Cosine similarity often proves to be superior in capturing the essence of similarity between vectors, especially in high-dimensional spaces, and offers computational advantages as well.\nOne reason cosine similarity is favored over Euclidean distance is due to the “curse of dimensionality”: As the number of dimensions increases, data becomes sparse, making all points seem far from each other in the vast volume of high-dimensional space. Consider our example with 4 data points: They can be close together in 2 dimensions, but in a 384-dimensional space, the volume expands exponentially with the dimensions, making the points appear far apart. In 2 dimensions, they can easily be plotted in a relatively small space. In 3D-space, the volume of the cube is the length to the power of 3. In a 384-dimensional space, the volume is the length to the power of 384 - incomprehensible, but it sounds huge! Cosine similarity addresses this by measuring the orientation or directionality of vectors rather than their Euclidean distance, effectively mitigating the impact of dimensionality.\nComputationally, cosine similarity benefits from being calculated through dot products (matrix multiplication), which can be efficiently parallelized, offering performance benefits compared to the computations required for Euclidean distance.\nMoreover, cosine similarity inherently normalizes its output to a fixed range of -1 to 1, regardless of input magnitude. This normalization makes it easier to compare similarity scores across different contexts, unlike Euclidean distance, which can vary widely in magnitude and makes direct comparisons less intuitive. This bounded range of cosine similarity scores is particularly advantageous, providing a straightforward method to assess relative similarity between pairs of vectors. Furthermore, the -1 to 1 value range aligns well with neural network architectures, optimizing the data input, even though cosine similarity calculations are primarily utilized during inference."
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html#visualizing-embeddings-with-more-data",
    "href": "posts/2024-03-15-embeddings/index.html#visualizing-embeddings-with-more-data",
    "title": "Visualizing Embeddings in 2D",
    "section": "Visualizing Embeddings with More Data",
    "text": "Visualizing Embeddings with More Data\nI hope, I did not loose you along the way. Things have gotten a bit technical, but now we are in a good position to create a more complex example which we can nonetheless intuitively understand.\nLet’s consider the following data for visualization:\n\n# Dictionary mapping animal groups to colors\ngroup_colors = {\n    \"Cats\": 'red',\n    \"Birds\": 'orange',\n    \"Insects\": 'brown',\n    \"Fish\": 'cyan'\n}\n\n# Dictionary mapping animals to their corresponding groups\nanimal_groups = {\n    \"cat\": \"Cats\", \"tiger\": \"Cats\", \"lion\": \"Cats\", \"bobcat\": \"Cats\", \"jaguar\": \"Cats\", \"leopard\": \"Cats\", \"lynx\": \"Cats\", \"cougar\": \"Cats\",\n    \"bird\": \"Birds\", \"sparrow\": \"Birds\", \"raven\": \"Birds\", \"eagle\": \"Birds\", \"crow\": \"Birds\", \"dove\": \"Birds\", \"penguin\": \"Birds\", \"flamingo\": \"Birds\", \"owl\": \"Birds\", \"hawk\": \"Birds\",\n    \"ant\": \"Insects\", \"beetle\": \"Insects\", \"spider\": \"Insects\", \"butterfly\": \"Insects\", \"bee\": \"Insects\", \"wasp\": \"Insects\", \"dragonfly\": \"Insects\", \"ladybug\": \"Insects\",\n    \"goldfish\": \"Fish\", \"trout\": \"Fish\", \"salmon\": \"Fish\", \"clownfish\": \"Fish\", \"tuna\": \"Fish\", \"mackerel\": \"Fish\"\n}\n\nSame as above, we calculate the embeddings using the BAAI/bge-small-en-v1.5 model, and we reduce the dimensionality via Principal Component Analysis (PCA).\n\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\nlarge_pool_texts = list(animal_groups.keys())\n\n# Initialize model \nemb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=\"cpu\")\n\n# Compute embeddings\nlarge_pool_embeddings = emb_model.encode(large_pool_texts, convert_to_tensor=True).cpu().detach().numpy()\n\n# Apply PCA to reduce to 2 dimensions\npca = PCA(n_components=2)\nlarge_pool_embeddings_2d = pca.fit_transform(large_pool_embeddings)\n\n# Store the 2D embeddings in a dictionary, indexed by animal name\nembeddings_2d_dict = {animal: large_pool_embeddings_2d[i] for i, animal in enumerate(large_pool_texts)}\n\nLet’s draw the Euclidean distance first by creating clusters of animals. It is important to clarify that these clusters were formed based on the predefined dataset rather than being algorithmically mined from the data. This was a deliberate choice to show that the embedding model has effectively learned to how to group animals. These fairly abstract concepts of “cat”, “bird” or “insect” are connoted in the embeddings, and we can see this because the model converts the strings “lion”, “flamingo” or “ant” into numerical representations which still contain semantic meaning. Creating the embedding therefore is not just a string-to-number conversion (like calculating a hash). It is a lot more nuanced transformation, and it is amazing to see that the embeddings even retain their semantic meaning after we have reduced their dimensionality to only 2 dimensions.\n\n\n\n\n\n\n\n\n\nFinally, let’s turn to the cosine similarity, which is the way the machine can even better work with similarity. Personally, I find the Euclidean distance more intuitive in 2D, but thinking back to the comparison of the 2 mechanism, I can also appreciate that the cosine similarity is more universals and computationally more effective. Nonetheless, we can see a similar pattern when plotting the cosine similarities."
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html#conclusion",
    "href": "posts/2024-03-15-embeddings/index.html#conclusion",
    "title": "Visualizing Embeddings in 2D",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we have explored the foundational principles of embeddings and brought them to life through visualization. Let’s revisit the technical definition of an embedding from the beginning, its meaning should be much clearer now:\n\n“In the context of machine learning and natural language processing, embeddings are numerical vector representations that capture the semantic essence of text entities, such as words, sentences, or documents. These vectors are typically high-dimensional, often consisting of hundreds or thousands of dimensions, allowing them to encode complex concepts and relationships. The fundamental idea behind embeddings is that texts with similar meanings are represented by vectors that are mathematically close to each other. This representation enables algorithms to process and analyze texts by understanding their underlying semantic content.”\n\nWe have seen how embeddings are numerical representations of text, in the example we used numerical representations of animals (“lion”, “tiger”, “flamingo”, “clownfish” etc.) which contain semantic information. We have reduced the dimensionality of the vectors with 384 dimensions to only 2 dimensions to plot the 2D vectors. We have visually seen that the semantic information of the data remained intact even in the reduced vectors because the points representing the animals formed the clusters of the dataset (“Cats”, “Birds”, “Insects”, “Fish”) we did not show to the embedding model. This proximity of the points (their Euclidean distance) represents their semantic relation to each other. Finally, we discussed and plotted the cosine similarly which has advantages for calculating vector similarity in machine learning use cases.\nIn closing, regardless of how complex the math might seem, I hope you have gained a more intuitive understanding of embeddings and the underlying concepts they are built upon."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html",
    "title": "How I created this Blog",
    "section": "",
    "text": "Blogging is an essential part of the Fast.AI methodology, therefore, I decided to follow the advice and start my blog to document my “Machine Learning Journey”. Here are the steps it took to create this blog.\nThe previous goto solution “Fastpages” has been depreciated in favor of Quarto. There is a good tutorial, but honestly I found it a bit intimidating because it is not a simple step-by-step guide, also the Creating a Blog page did no quite fit this category (for me). Therefore, without being an expert at this, let me share what I did to create this blog.\nSo if your goal is to create a simple blog based on Quarto, just hop on and follow along :).\nA little side-note: This is the fourth blog post I write with Quarto, and by now I feel that some rough edged have been removed, mostly because I realized that really all the blogging can be done in jupyter notebook! Therefore, learning, running my own experiments and blogging happen in the same environment and with a lot of reuse. Knowing that my more things can be done, the only goal of this blog post is to get you up and running with a basic setup and explain the possibility to blog via jupyter notebooks."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-1-create-a-new-github-repo",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-1-create-a-new-github-repo",
    "title": "How I created this Blog",
    "section": "Step 1: Create a new GitHub repo",
    "text": "Step 1: Create a new GitHub repo\nYour blog will reside in a GitHub repo, and it will leverage Github Pages. Follow steps 1 and 2 on the GitHub Pages homepage for the initial setup.\n\nNote: The default recommendation for the repo is &lt;your username&gt;.github.io. I took that recommendation, but anything else should work as well.\n\nAs a result you have an empty repo with just a readme.md file. Here’s how it still looks in my repo.\nFor cloning the repo to my local machine, I did:\ngit clone git@github.com:chrwittm/chrwittm.github.io.git\nTwo final activities are needed to finalize the setup of your repo:\n\nCreate a new branch called gh-pages. To do this, go to your branches (for me that is https://github.com/chrwittm/chrwittm.github.io/branches), and create the new branch by clicking the “New branch”-button in the top right.\nSet the new branch as the branch for GitHub Pages. In your repo, navigate to Settings -&gt; Pages. (In my repo that takes me to https://github.com/chrwittm/chrwittm.github.io/settings/pages.) Change main to gh-pages.\n\n\nFor more info on setting the branch, please refer to the Quarto Docs."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-2-install-quarto",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-2-install-quarto",
    "title": "How I created this Blog",
    "section": "Step 2: Install Quarto",
    "text": "Step 2: Install Quarto\nI am currently working on a Windows 10 machine and I usually work both in WSL (Ubuntu) (e.g. for Jupyter and anything related to Fast.AI development) and in Windows with VS Code (e.g. for writing this blog).\n\nInstalling Quarto in WSL\nFrom previous activities with nbdev, I already had Quarto installed. If I re-traced my steps correctly, here is what I did (as suggested here and here):\nmamba install -c fastchan nbdev\nnbdev_install_quarto\n\n\nInstalling Quarto for Windows (optional)\nOptional: Once I discovered that I can do everything in jupyter, I would label this step as optional, because I only used the Windows installation of Quarto to render previews of .qmd-files - which I do not need anymore when everything is done in jupyter notebooks.\nGo to this page, download and install Quarto.\n\n\nSetup Addons for VS Code (optional)\nOptional: Once I discovered that I can do everything in jupyter, I would label this step as optional, because I only used the Windows installation of Quarto to render previews of .qmd-files - which I do not need anymore when everything is done in jupyter notebooks.\nI also installed the Quarto extension for VS Code."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-3-initial-setup-to-publish-hello-world",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-3-initial-setup-to-publish-hello-world",
    "title": "How I created this Blog",
    "section": "Step 3: Initial setup to publish “Hello World”",
    "text": "Step 3: Initial setup to publish “Hello World”\nBy now we are really close to publishing the “Hello World”-version of our blog: In the command line, go to the directory of your repo, and run the following commands, and the example content for the Quarto blog should be published to your repo.\nquarto create-project --type website:blog\nquarto publish gh-pages\nOnce done, you can open your blog at: &lt;https://\"your username\".github.io/&gt;\nFor some more background on what is happening with these two commands, please refer to this this page (choose “Terminal”) and this page."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-4-create-your-first-blog-post",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-4-create-your-first-blog-post",
    "title": "How I created this Blog",
    "section": "Step 4: Create your first Blog Post",
    "text": "Step 4: Create your first Blog Post\nNow it is time to create your first own blog post.\nIn the posts-directory, create a new folder, for example hello-world. Within this folder, create a notebook called index.ipynb. Add some hello-world content and a RAW-section as the first cell with this content (here is an example):\n---\ntitle: \"Hello World\"\nauthor: \"Your Name\"\ndate: \"2022-01-01\"\n---\nRepublish your blog:\nquarto publish gh-pages\nCongratulations, you just published your first blog post!\nFor a little more detailed version of the hello world blog post, please refer to my other hello world post."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-5-avoiding-disaster",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-5-avoiding-disaster",
    "title": "How I created this Blog",
    "section": "Step 5: Avoiding Disaster",
    "text": "Step 5: Avoiding Disaster\nWhen you run quarto publish gh-pages, your blog posts are rendered, and the rendered versions are pushed to git in branch gh-pages. Your actual notebooks are not uploaded to GitHub. Also any config you make to the blog etc. is uploaded in the rendered versions only. So if something were to happen to your local files, your work would be lost. (Such a disaster almost happened to me but the OneDrive file history saved me.)\nTherefore, I would recommend to also upload the “source”-files to GitHub (in the main branch):\ngit add posts/\ngit add _quarto.yml\ngit add about.qmd\ngit add index.qmd\ngit add profile.png\ngit add styles.css\ngit add .gitignore\ngit commit -m \"uploaded source files\"\ngit push\nAs a result, the source files are also stored on GitHub."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#steps-6-to-n-additional-setup",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#steps-6-to-n-additional-setup",
    "title": "How I created this Blog",
    "section": "Steps 6 to n: Additional setup",
    "text": "Steps 6 to n: Additional setup\nThere are many more things that can be done with the blog, but to keep things down to basics, let me just mention a few topics which will make the blog look like your own blog.\nAdditionally, let me mention one other blog post as a reference which I found only when looking into more detailed setup topics like comments and analytics. Albert Rapp’s blog post The ultimate guide to starting a Quarto blog truly is a great guide for setting up your Quarto blog.\n\nStep 6.1: Remove example content\nNow that the hello world blog post is published, you can remove the default content. I just turned the two example blog posts into drafts by adding the following line in their headers:\ndraft: true\n\n\nStep 6.2: Update remaining example content\nUpdate the following files and add/change the content, so that the blog looks like it is your blog:\n_quarto.yml\nabout.qmd\nindex.qmd"
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#conclusion",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#conclusion",
    "title": "How I created this Blog",
    "section": "Conclusion",
    "text": "Conclusion\nSetting up the blog was not really hard, but it took some time for me. Hopefully, this guide contains some shortcuts for you. Happy blogging!"
  },
  {
    "objectID": "posts/2022-10-01-hello-world/index.html",
    "href": "posts/2022-10-01-hello-world/index.html",
    "title": "Hello World!",
    "section": "",
    "text": "Hello World!\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog, where I document my “Machine Learning Journey”.\nThe main purpose of this blog is to explain and clarify concepts to myself through writing, which sometimes is surprisingly difficult.\nI firmly believe in the idea that “The best way to learn is to teach.” Therefore, I am using the Feynman Technique on myself: Breaking down complex ideas into simple terms as if teaching someone else.\nIf you happen to stumble across this blog and find it useful, that would truly make me happy. 😊\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Lizenz · License",
    "section": "",
    "text": "Hinweis / Note: Deutsch ist die rechtlich maßgebliche Fassung. English version below is for convenience."
  },
  {
    "objectID": "license.html#deutsch",
    "href": "license.html#deutsch",
    "title": "Lizenz · License",
    "section": "Deutsch",
    "text": "Deutsch\n\nGrundsatz\nSofern nicht anders angegeben, stehen Texte und Bilder (einschließlich KI-generierter Bilder) auf dieser Website unter Creative Commons – Namensnennung 4.0 International (CC BY 4.0).\nCode-Beispiele und Snippets stehen unter der Apache License 2.0.\n\nAusnahmen werden am jeweiligen Beitrag/Bild ausdrücklich gekennzeichnet.\nInhalte Dritter (z. B. Logos, Screenshots, Zitate) sind nicht von meiner Lizenz umfasst und unterliegen den Rechten der jeweiligen Inhaber.\n\n\n\nAttribution – so bitte zitieren\nWenn du Inhalte nutzt, nenne Autor, Titel/URL und Lizenz: &gt; „Christian Wittmann – [Titel des Beitrags], verfügbar unter https://chrwittm.github.io/, CC BY 4.0.“\n&gt; Für Code: „… Apache-2.0.“\n\n\nHinweis zu KI-Unterstützung\nIch nutze regelmäßig KI-Tools (z. B. ChatGPT, lokale LLMs, Code-Completion) für Ideen, Entwürfe und Bilderzeugung. Ich prüfe, bearbeite und verantworte alle veröffentlichten Inhalte.\nKI-generierte Bilder sind – sofern nicht anders vermerkt – in die CC BY 4.0-Lizenz einbezogen; abweichende Lizenzen werden direkt am Bild genannt.\n\n\nMarken & Rechte Dritter\nMarken- und Produktnamen sind Eigentum der jeweiligen Inhaber. Screenshots und Zitate werden als solche gekennzeichnet und ggf. nur im Rahmen des zulässigen Nutzungsrechts verwendet.\nStand: 06.08.2025"
  },
  {
    "objectID": "license.html#english",
    "href": "license.html#english",
    "title": "Lizenz · License",
    "section": "English",
    "text": "English\n\nDefault licensing\nUnless noted otherwise, text and images (including AI-generated images) on this site are licensed under Creative Commons Attribution 4.0 International (CC BY 4.0).\nCode samples and snippets are licensed under the Apache License 2.0.\n\nExceptions will be clearly stated on the specific post/image.\nThird-party content (e.g., logos, screenshots, quotations) is not covered by my license and remains under the rights of its owners.\n\n\n\nAttribution – how to credit\nWhen reusing, please credit author, title/URL, and license: &gt; “Christian Wittmann – [Post title], available at https://chrwittm.github.io/, CC BY 4.0.”\n&gt; For code: “… Apache-2.0.”\n\n\nNote on AI assistance\nI routinely use AI tools (e.g., ChatGPT, local LLMs, code completion) for ideation, drafting, and image generation. I review, edit, and take responsibility for all published content.\nAI-generated images are included under CC BY 4.0 unless stated otherwise; deviations are labeled per image.\n\n\nTrademarks & third-party rights\nTrademarks and product names are the property of their respective owners. Screenshots and quotations are marked as such and may be used only within permissible limits.\nLast updated: 2025-08-06"
  }
]