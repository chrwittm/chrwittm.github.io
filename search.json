[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "In this blog, I am documenting my learning journey in machine learning:\n“The Machine Learning Journey” ;)\nI am mainly writing for myself, trying to explain/clarify concepts to myself while writing,\ntherefore implementing the concept of “The Best Way to Learn Is to Teach”.\nIf you stumbled across this blog and you even find it useful, that would make me happy :)"
  },
  {
    "objectID": "posts/2022-10-01-hello-world/index.html",
    "href": "posts/2022-10-01-hello-world/index.html",
    "title": "Hello World!",
    "section": "",
    "text": "Hello World!"
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html",
    "href": "posts/2024-03-15-embeddings/index.html",
    "title": "Visualizing Embeddings in 2D",
    "section": "",
    "text": "Did you ever try to explain what embeddings are to people who have no or only a limited background in machine learning or computer science? I recently tried this in an on-the-fly attempt to explain embeddings with an analogy to animals. While I think I could get the idea across, this analogy has stuck in my mind, and here is version 2.0: Let’s explore how we can visualize the embeddings of terms like “lion”, “tiger”, or “flamingo” to illustrate how a machine learning model understands the meaning of these terms and perceives their semantic relationships to one another.\nHere is the plan:\nIn the end, you will understanding exactly how the visualization was created and I hope you will have a more intuitive understanding of the underlying concepts of embeddings. If you feel that the math is to heavy, please just focus on the underpinning ideas."
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html#from-technical-to-visualization",
    "href": "posts/2024-03-15-embeddings/index.html#from-technical-to-visualization",
    "title": "Visualizing Embeddings in 2D",
    "section": "From Technical to Visualization",
    "text": "From Technical to Visualization\nHere is the technical definition:\n\n“In the context of machine learning and natural language processing, embeddings are numerical vector representations that capture the semantic essence of text entities, such as words, sentences, or documents. These vectors are typically high-dimensional, often consisting of hundreds or thousands of dimensions, allowing them to encode complex concepts and relationships. The fundamental idea behind embeddings is that texts with similar meanings are represented by vectors that are mathematically close to each other. This representation enables algorithms to process and analyze texts by understanding their underlying semantic content.”\n\nNow, let’s transition from theory to practice. Here are the final visualization which show how our machine learning model “thinks” about various animals, illustrating the concept of embeddings in charts that represent their semantic relationships. Why 2 charts? The left one is the more intuitive one for us humans and the right one illustrates better how the machine thinks.\n\n\n\nAnimal Embeddings CC-BY https://creativecommons.org/licenses/by/4.0/\n\n\n\nNotes: For readability I hidden most of the code from this blog post. The all the details, please check out the notebook version on Github. For readability I curated the dataset so that the animal groups are nicely separated. By reducing animals to just 2 numbers, a lot of complexity is lost, but the intent not to create a highly accurate model of the worlds, rather I wanted to present an example which is simple enough so be visualized in 2D to allow you to build some intuition on how embeddings work. Please feel free to experiment yourself in the notebook version on Github."
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html#visualizing-embeddings-with-a-small-dataset",
    "href": "posts/2024-03-15-embeddings/index.html#visualizing-embeddings-with-a-small-dataset",
    "title": "Visualizing Embeddings in 2D",
    "section": "Visualizing Embeddings with a Small Dataset",
    "text": "Visualizing Embeddings with a Small Dataset\nLet’s start small and use a dataset with the examples of “lion”, “tiger”, “flamingo”, and “clownfish”.\n\nEmbeddings from model\nInspired by the hackers guide by Jeremy Howard let’s use this model to calculate the embeddings via SentenceTransformers\n\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\n# Large pool of animals from the keys of animal_groups dictionary\nexample_animals = [\"lion\", \"tiger\", \"flamingo\", \"clownfish\"]\n\n# Initialize your model (make sure to adjust the device according to your setup)\nemb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=\"cpu\")\n\n# Compute embeddings for the large pool\nexample_animals_embeddings = emb_model.encode(example_animals, convert_to_tensor=True).cpu().detach().numpy()\n\n\nexample_animals_embeddings.shape\n\n(4, 384)\n\n\nEach word is now represented by a 384-dimensional vector. What does this mean, and where do these numbers come from?"
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html#training-an-embedding-model",
    "href": "posts/2024-03-15-embeddings/index.html#training-an-embedding-model",
    "title": "Visualizing Embeddings in 2D",
    "section": "Training an Embedding Model",
    "text": "Training an Embedding Model\nThe model we use is BAAI/bge-small-en-v1.5. BAAI stands for “Beijing Academy of Artificial Intelligence”. Is a private non-profit organization known for its research and development in artificial intelligence technologies. BGE stands for “BAAI general embedding”.\nDiving into theit GitHub repo, we can read that this model has been trained in English (and there are Chinese and multi-language models available). It is a general embedding model which has been pre-trained using RetroMAE. Subsequently, it has been trained on large-scale pair data using contrastive learning.\nIn the RetroMAE pre-training phase, the model has been exposed to vast amounts of text data, such as Wikipedia and BookCorpus to learn a wide range of language patterns, contextual relationships, and the nuances of semantics without specific task-oriented guidance. Contrastive learning has taught the model to pull the embeddings of texts that are similar (positive pairs) closer to each other and push apart embeddings of texts that are dissimilar (negative pairs). It’s through these methods that the model learns to understand and encode the semantic essence of texts into vectors.\nEssentially, an embedding is a numerical representation of a text. Unlike hashes, which primarily aim at crating unique representations of stings for retrieval or data integrity, embeddings are designed to capture semantic meaning and relationships between pieces of text. As we will see, embeddings of “lion” and “tiger” are mathematically more similar to each other than “tiger” and “flamingo”, capturing their semantic meaning and similarity."
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html#what-is-dimensionality-reduction",
    "href": "posts/2024-03-15-embeddings/index.html#what-is-dimensionality-reduction",
    "title": "Visualizing Embeddings in 2D",
    "section": "What is Dimensionality Reduction?",
    "text": "What is Dimensionality Reduction?\nTrying to understand how these 384 dimensions can describe a single word is impossible beyond the trust that these number can magically describe our 4 animals. To gain a more intuitive understanding of embeddings, we can reduce this high-dimensional space to something more manageable, like two dimensions. You can think of this as projecting an object with a torch to the wall, the 3D object is projected to 2D. However, it is important to do the projection in a way which preserves important information. Check out to the following visualization @visualizevalue to the the potential problem.\n\n\n\nProjection\n\n\nOne effective algorithm for dimensionality reduction is Principal Component Analysis (PCA) which simplifies the data while preserving its most significant patterns.\n\nNote: The remainder of this section explains how PCA works. If you prefer to focus on the results, feel free to skip ahead.\n\nPCA transforms the original high-dimensional variables into a new set of lower dimensional variables, the principal components, which capture the maximum variance in the data. Therefore, the data’s variability is preserved as much as possible. As a result, noise is reduced without filtering out essential information, making it easier to observe patterns, clusters, or relationships that were not apparent in the higher-dimensional space.\nSince the math was not 100% self-explanatory to me, I created a separate deep-dive notebook exploring PCA which reduces the dimensionality of a 3D-dataset to 2D, including interactive visualizations. Here is the executive summary which visually takes you through the process:\n\n\n\nPCA Steps\n\n\n\nFigure 1: This is out example dataset with 10 samples of 3D points\nFigure 2: The 2 vectors are the principal components (PC1 and PC2) capturing the maximum variance in the data. We construct a plane from the 2 vectors, the projection plane.\nFigure 3: The 3D-points are projected onto the principal components plane.\n\nFigure 4: The projected points on the plane in 3D space\nFigure 5: The projected points rotated in a way that we can see the 2D projection in 3D space from above, simulating the reduced dimensionality\nFigure 6: The final 2D representation of the data\n\nWith a clear understanding of dimensionality reduction, we can now apply PCA to our example dataset containing “lion,” “tiger,” “flamingo,” and “clownfish” and visualize the outcome."
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html#applying-dimensionality-reduction",
    "href": "posts/2024-03-15-embeddings/index.html#applying-dimensionality-reduction",
    "title": "Visualizing Embeddings in 2D",
    "section": "Applying Dimensionality Reduction",
    "text": "Applying Dimensionality Reduction\nSklearn offers an easy to consume implementation to apply PCA to out example dataset.\n\nfrom sklearn.decomposition import PCA\n\n# Apply PCA to reduce to 2 dimensions\npca = PCA(n_components=2)\nexample_animals_embeddings_2d = pca.fit_transform(example_animals_embeddings)\n\nHere is the result, both numerically and plotted in 2D.\n\nexample_animals_embeddings_2d\n\narray([[-0.35093537, -0.07018732],\n       [-0.4075373 ,  0.02734617],\n       [ 0.3408063 ,  0.40827572],\n       [ 0.41766608, -0.3654344 ]], dtype=float32)\n\n\n\n#hide_input\nimport matplotlib.pyplot as plt\n\n# Plotting the 2D embeddings\nplt.figure(figsize=(8, 8))\n\n# Scatter plot of the embeddings\nplt.scatter(example_animals_embeddings_2d[:, 0], example_animals_embeddings_2d[:, 1])\n\n# Annotating the points with the animal names\nfor i, label in enumerate(example_animals):\n    plt.text(example_animals_embeddings_2d[i, 0], example_animals_embeddings_2d[i, 1], label)\n\nplt.title('2D PCA Embeddings of Animals')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.grid(True)\nplt.show()\n\n\n\n\nAs we can easily see, “lion” and “tiger” are closer to ech other then “tiger” and “flamingo”. After the first visual “proof” let’s explore how we can calculate the distance mathematically because this is how the machine evaluates the similarity of text."
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html#calculating-distance",
    "href": "posts/2024-03-15-embeddings/index.html#calculating-distance",
    "title": "Visualizing Embeddings in 2D",
    "section": "Calculating Distance",
    "text": "Calculating Distance\nWhat we intuitively do when looking at the chart above is to calculate the so-called Euclidean distance. We see that “lion” and “tiger” are close to each other while the other dots are farther away. While we will do a quick implementation for calculating the Euclidean distances in the next sub-section, it turns out, however, that there are better ways to calculate similarity between vectors. This is why subsequently, we will dive into calculating cosine similarity, followed by a discussion on why cosine similarity is better for calculating the similarity between 2 vectors.\n\nCalculating Euclidean Distance\nHere is a visualization of the euclidean distances for our example dataset, confirming our observations.\n\nfrom scipy.spatial.distance import euclidean\nimport matplotlib.pyplot as plt\n\n# Function to plot the points and annotate distances\ndef plot_with_distances(embeddings, labels):\n    plt.figure(figsize=(8, 8))\n    \n    # Plot the points\n    plt.scatter(embeddings[:, 0], embeddings[:, 1], s=100)\n    \n    # Annotate points with labels\n    for i, label in enumerate(labels):\n        plt.annotate(label, (embeddings[i, 0], embeddings[i, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n    \n    # Draw dotted lines and annotate distances\n    for i in range(len(labels)):\n        for j in range(i + 1, len(labels)):\n            # Compute the Euclidean distance\n            distance = euclidean(embeddings[i], embeddings[j])\n            \n            # Draw a dotted line between points\n            plt.plot([embeddings[i, 0], embeddings[j, 0]], [embeddings[i, 1], embeddings[j, 1]], 'k:', linewidth=1)\n            \n            # Calculate midpoint for the distance label\n            mid_point = (embeddings[i] + embeddings[j]) / 2\n            \n            # Annotate the line with distance\n            plt.text(mid_point[0], mid_point[1], f'{distance:.2f}', color='red')\n    \n    plt.title('Euclidean Distances Between Animal Embeddings')\n    plt.xlabel('PCA Component 1')\n    plt.ylabel('PCA Component 2')\n    plt.grid(True)\n    plt.show()\n\n# Call the function with your data\nplot_with_distances(example_animals_embeddings_2d, example_animals)\n\n\n\n\nIt is also possible to do this calculation in higher dimensionality, but as we discussed above, the orientation of the vectors is more significant than their magnitude. Therefore, let’s turn our attention to cosine similarity.\n\n\nCalculating Cosine Similarity\nCosine similarity focuses on the orientation of the vectors with respect to each other without considering their magnitudes (lengths). It measures the similarity between two vectors as the cosine of the angle between them. Vectors pointing in the same direction (regardless of their length) have a cosine similarity of 1, indicating they are very similar. Vectors at 90 degrees to each other have a cosine similarity of 0, indicating no similarity, and vectors pointing in opposite directions have a cosine similarity of -1, indicating completely dissimilar. This principle holds true in higher-dimensional spaces as well. For instance, two vectors in a 3D space will adhere to the same value range for their cosine similarity. Hence, this measure can effectively express the similarity between vectors across any number of dimensions, focusing on how vectors are oriented with respect to each other rather than how far apart they are. This relation is plotted in the following cosine graph, which is colored to indicate similarity.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the range of values for x in radians from -2pi to 2pi\nx_radians = np.linspace(-2 * np.pi, 2 * np.pi, 1000)\n\n# Calculate the cosine of each x value\ny = np.cos(x_radians)\n\n# Create the plot\nfig, ax1 = plt.subplots(figsize=(10, 5))  # Set figure size and get primary axis\ncmap = plt.get_cmap('RdYlGn')  # Get the colormap\n\n# Plot x vs. cos(x) on primary axis with color mapping\ncolor_mapped_y = np.interp(y, (y.min(), y.max()), (0, 1))  # Normalize y for color mapping\nax1.scatter(x_radians, y, c=color_mapped_y, cmap=cmap, label='cos(x)')\n\n# Set primary x-axis (radians) ticks and labels\nradian_ticks = np.arange(-2 * np.pi, 2.5 * np.pi, np.pi / 2)\nradian_labels = ['$-2\\pi$', '$-3\\pi/2$', '$-\\pi$', '$-\\pi/2$', '0', '$\\pi/2$', '$\\pi$', '$3\\pi/2$', '$2\\pi$']\nax1.set_xticks(radian_ticks)\nax1.set_xticklabels(radian_labels)\nax1.set_xlabel('Radians')\n\n# Set y-axis label\nax1.set_ylabel('cos(x)', color='b')\nax1.tick_params('y', colors='b')\n\n# Define conversion functions for radians to degrees and vice versa\ndef rad2deg(r):\n    return r * 180 / np.pi\n\n# Create secondary x-axis for degrees\ndegree_ticks = radian_ticks * 180 / np.pi\nax2 = ax1.secondary_xaxis('top', functions=(rad2deg, np.deg2rad))\nax2.set_xticks(degree_ticks)\nax2.set_xticklabels([f\"{tick:.0f}\" for tick in degree_ticks])\nax2.set_xlabel('Degrees')\n\n# Add vertical helper lines at multiples of π/2\nfor tick in radian_ticks:\n    ax1.axvline(tick, color='gray', linestyle='--', linewidth=0.5)\n\n# Additional customizations\nax1.axhline(0, color='black', linewidth=0.5)  # Draw a horizontal line at y=0\nax1.grid(True, which='both', linestyle='--', linewidth=0.5)  # Add a grid\nax1.legend()  # Add legend\nplt.title('Cosine Function with Radians and Degrees')  # Title of the plot\n\n# Show the plot\nplt.show()\n\n\n\n\nLet’s transfer this our animal example. When thinking in terms of cosine similarity, we need to plot our 4 animals differently. Each animal is represented as vectors, and the magnitude of the vectors are normalized. Additionally, starting with the flamingo, the cosine of the angle in relation to “flamingo” is colored to indicate similarity.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.colors import Normalize\nfrom matplotlib.cm import ScalarMappable\n\n# Assuming 'example_animals_embeddings_2d' contains your PCA-transformed embeddings\nanimals = [\"lion\", \"tiger\", \"flamingo\", \"clownfish\"]\nanimal_vectors = {animal: example_animals_embeddings_2d[animals.index(animal)] for animal in animals}\n\n# Normalize each vector to have a length of 0.4\nnormalized_animal_vectors = {animal: (vector / np.linalg.norm(vector)) * 0.4 for animal, vector in animal_vectors.items()}\n\n# Colors for each vector that is neither green nor red\nvector_colors = {'lion': 'blue', 'tiger': 'purple', 'flamingo': 'black', 'clownfish': 'orange'}\n\n# Plotting setup\nplt.figure(figsize=(8, 8))\nnorm = Normalize(vmin=0, vmax=180)\ncmap = plt.get_cmap('RdYlGn')  # Red to Green color map\nsm = ScalarMappable(norm=norm, cmap=cmap)\n\n# Generate and plot additional vectors with colors based on their angle to the flamingo vector\nvector_flamingo = normalized_animal_vectors['flamingo']  # Use the normalized flamingo vector\nfor angle in np.linspace(0, 2*np.pi, 360):\n    x, y = np.cos(angle) * 0.4, np.sin(angle) * 0.4  # Vectors of length 0.4\n    angle_deg = np.degrees(angle)\n    angle_to_flamingo = np.degrees(np.arctan2(vector_flamingo[1], vector_flamingo[0]))  # Angle of the flamingo vector\n    diff_angle = np.abs((angle_deg - angle_to_flamingo + 180) % 360 - 180)  # Difference in angle\n    color = sm.to_rgba(180 - diff_angle)  # Determine color based on difference in angle\n    plt.quiver(0, 0, x, y, angles='xy', scale_units='xy', scale=1, color=color, alpha=0.5)\n\n# Plot the normalized animal vectors\nfor animal, vector in normalized_animal_vectors.items():\n    plt.quiver(0, 0, vector[0], vector[1], angles='xy', scale_units='xy', scale=1, color=vector_colors[animal], label=animal)\n\n# Adjust plot limits and labels\nplt.xlim(-0.5, 0.6)\nplt.ylim(-0.5, 0.6)\nplt.xlabel('PCA Component 1')\nplt.ylabel('PCA Component 2')\nplt.grid(True)\nplt.legend()\nplt.title('Vector Orientation Visualization with Normalized Lengths')\n\nplt.show()\n\n\n\n\n\n\nEuclidean Distance vs. Cosine Similarity\nWhen comparing Euclidean distance and cosine similarity, it’s important to consider various aspects that highlight the strengths and limitations of each measure in different contexts. Cosine similarity often proves to be superior in capturing the essence of similarity between vectors, especially in high-dimensional spaces, and offers computational advantages as well.\nOne reason cosine similarity is favored over Euclidean distance is due to the “curse of dimensionality”: As the number of dimensions increases, data becomes sparse, making all points seem far from each other in the vast volume of high-dimensional space. Consider our example with 4 data points: They can be close together in 2 dimensions, but in a 384-dimensional space, the volume expands exponentially with the dimensions, making the points appear far apart. In 2 dimensions, they can easily be plotted in a relatively small space. In 3D-space, the volume of the cube is the length to the power of 3. In a 384-dimensional space, the volume is the length to the power of 384 - incomprehensible, but it sounds huge! Cosine similarity addresses this by measuring the orientation or directionality of vectors rather than their Euclidean distance, effectively mitigating the impact of dimensionality.\nComputationally, cosine similarity benefits from being calculated through dot products (matrix multiplication), which can be efficiently parallelized, offering performance benefits compared to the computations required for Euclidean distance.\nMoreover, cosine similarity inherently normalizes its output to a fixed range of -1 to 1, regardless of input magnitude. This normalization makes it easier to compare similarity scores across different contexts, unlike Euclidean distance, which can vary widely in magnitude and makes direct comparisons less intuitive. This bounded range of cosine similarity scores is particularly advantageous, providing a straightforward method to assess relative similarity between pairs of vectors. Furthermore, the -1 to 1 value range aligns well with neural network architectures, optimizing the data input, even though cosine similarity calculations are primarily utilized during inference."
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html#visualizing-embeddings-with-more-data",
    "href": "posts/2024-03-15-embeddings/index.html#visualizing-embeddings-with-more-data",
    "title": "Visualizing Embeddings in 2D",
    "section": "Visualizing Embeddings with More Data",
    "text": "Visualizing Embeddings with More Data\nI hope, I did not loose you along the way. Things have gotten a bit technical, but now we are in a good position to create a more complex example which we can nonetheless intuitively understand.\nLet’s consider the following data for visualization:\n\ngroup_colors = {\n    \"Cats\": 'red',\n    #\"Hoofed Animals\": 'blue',\n    #\"Rodents\": 'green',\n    \"Birds\": 'orange',\n    #\"Reptiles\": 'green',\n    \"Insects\": 'brown',\n    \"Fish\": 'cyan'\n}\n\n# Dictionary mapping of animals to their corresponding groups\nanimal_groups = {\n    \"cat\": \"Cats\", \"tiger\": \"Cats\", \"lion\": \"Cats\", \"bobcat\": \"Cats\", \"jaguar\": \"Cats\", \"leopard\": \"Cats\", \"lynx\": \"Cats\", \"cougar\": \"Cats\",\n    #\"horse\": \"Hoofed Animals\", \"cow\": \"Hoofed Animals\", \"sheep\": \"Hoofed Animals\", \"goat\": \"Hoofed Animals\", \"lamb\": \"Hoofed Animals\", \"deer\": \"Hoofed Animals\", \"elk\": \"Hoofed Animals\", \"moose\": \"Hoofed Animals\",\n    #\"mouse\": \"Rodents\", \"rat\": \"Rodents\", \"squirrel\": \"Rodents\", \"beaver\": \"Rodents\", \"hamster\": \"Rodents\", \"rabbit\": \"Rodents\",\n    \"bird\": \"Birds\", \"sparrow\": \"Birds\", \"raven\": \"Birds\", \"eagle\": \"Birds\", \"crow\": \"Birds\", \"dove\": \"Birds\", \"penguin\": \"Birds\", \"flamingo\": \"Birds\", \"owl\": \"Birds\", \"hawk\": \"Birds\",\n    #\"lizard\": \"Reptiles\", \"snake\": \"Reptiles\", \"turtle\": \"Reptiles\", \"crocodile\": \"Reptiles\", \"alligator\": \"Reptiles\", \"iguana\": \"Reptiles\",\n    \"ant\": \"Insects\", \"beetle\": \"Insects\", \"spider\": \"Insects\", \"butterfly\": \"Insects\", \"bee\": \"Insects\", \"wasp\": \"Insects\", \"dragonfly\": \"Insects\", \"ladybug\": \"Insects\",\n    \"goldfish\": \"Fish\", \"trout\": \"Fish\", \"salmon\": \"Fish\", \"clownfish\": \"Fish\", \"tuna\": \"Fish\", \"mackerel\": \"Fish\"\n}\n\nSame as above, we calculate the embeddings using the BAAI/bge-small-en-v1.5 model, and we reduce the dimensionality via Principal Component Analysis (PCA).\n\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Large pool of animals from the keys of animal_groups dictionary\nlarge_pool_texts = list(animal_groups.keys())\n\n# Initialize your model (make sure to adjust the device according to your setup)\nemb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=\"cpu\")\n\n# Compute embeddings for the large pool\nlarge_pool_embeddings = emb_model.encode(large_pool_texts, convert_to_tensor=True).cpu().detach().numpy()\n\n# Apply PCA to reduce to 2 dimensions\npca = PCA(n_components=2)\nlarge_pool_embeddings_2d = pca.fit_transform(large_pool_embeddings)\n\n# Store the 2D embeddings in a dictionary, indexed by animal name\nembeddings_2d_dict = {animal: large_pool_embeddings_2d[i] for i, animal in enumerate(large_pool_texts)}\n\nLet’s draw the Euclidean distance first by creating clusters of animals. It is important to clarify that these clusters were formed based on the predefined dataset rather than being algorithmically mined from the data. This was a deliberate choice to show that the embedding model has effectively learned to how to group animals. These fairly abstract concepts of “cat”, “bird” or “insect” are connoted in the embeddings, and we can see this because the model converts the strings “lion”, “flamingo” or “ant” into numerical representations which still contain semantic meaning. Creating the embedding therefore is not just a string-to-number conversion (like calculating a hash). It is a lot more nuanced transformation, and it is amazing to see that the embeddings even retain their semantic meaning after we have reduced their dimensionality to only 2 dimensions.\n\nfrom IPython.display import HTML\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.patches as patches\n\n# Setup the plot\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.title('2D Visualization of Animal Embeddings with Euclidean Distance')\n\nanimals_to_visualize = large_pool_texts\n\n# Calculate the bounds for the plot\nall_embeddings = np.array([embeddings_2d_dict[animal] for animal in embeddings_2d_dict])\nx_min, x_max = all_embeddings[:, 0].min() - 0.1, all_embeddings[:, 0].max() + 0.1\ny_min, y_max = all_embeddings[:, 1].min() - 0.1, all_embeddings[:, 1].max() + 0.1\n\n# Set the axes limits\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\n\n# Add grid\nplt.grid(True)\n\n# Draw the origin\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\n\n# Pre-plotting for legend purposes\nfor group, color in group_colors.items():\n    ax.scatter([], [], color=color, label=group)\n\n# Display the full legend\nax.legend()\n\nplotted_animals = []\n\ndef animate(i):\n    if i &lt; len(animals_to_visualize):\n        # Plot each animal\n        animal = animals_to_visualize[i]\n        group = animal_groups[animal]\n        color = group_colors[group]\n        embedding_2d = embeddings_2d_dict[animal]\n        plotted_animals.append(ax.scatter(embedding_2d[0], embedding_2d[1], color=color))\n        ax.text(embedding_2d[0]+0.01, embedding_2d[1]+0.01, animal, fontsize=9)\n    elif i == len(animals_to_visualize):\n        # Draw transparent bubbles for each group\n        for group, color in group_colors.items():\n            group_embeddings = np.array([embeddings_2d_dict[animal] for animal in animal_groups if animal_groups[animal] == group])\n            if len(group_embeddings) == 0:\n                continue\n            centroid = np.mean(group_embeddings, axis=0)\n            std_dev = np.std(group_embeddings, axis=0)\n            ellipse = patches.Ellipse((centroid[0], centroid[1]), std_dev[0]*4, std_dev[1]*4, color=color, alpha=0.2)\n            ax.add_patch(ellipse)\n\n# Adjust the frames parameter to include an additional frame for drawing bubbles\nani = FuncAnimation(fig, animate, frames=len(animals_to_visualize) + 1, interval=200, repeat=False)\n\n# To display the animation in Jupyter\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\nFinally, let’s turn to the cosine similarity, which is the way the machine can even better work with similarity. Personally, I find the Euclidean distance more intuitive in 2D, but thinking back to the comparison of the 2 mechanism, I can also appreciate that the cosine similarity is more universals and computationally more effective. Nonetheless, we can see a similar pattern when plotting the cosine similarities.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming embeddings_2d_dict and group_colors are defined as in your data preparation\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.title('2D Visualization of Animal Embeddings with Cosine Similarity')\n\n# Set limits for the plot based on the normalized vectors\nplt.xlim(-1.1, 1.1)\nplt.ylim(-1.1, 1.1)\n\n# Add grid\nplt.grid(True)\n\n# Draw the origin\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\n\n# Normalize vectors and plot\nfor animal, embedding in embeddings_2d_dict.items():\n    # Normalize the vector to have a fixed length\n    norm = np.linalg.norm(embedding)\n    if norm == 0:   # To avoid division by zero\n        continue\n    normalized_vector = embedding / norm\n\n    # Calculate direction for the vector\n    dx = normalized_vector[0]\n    dy = normalized_vector[1]\n\n    # Fixed length for all vectors\n    length = 0.8\n\n    # Set color based on the animal group\n    group = animal_groups[animal]\n    color = group_colors[group]\n\n    # Plot vector\n    ax.arrow(0, 0, dx*length, dy*length, head_width=0.03, head_length=0.05, fc=color, ec=color)\n\n    # Add text label\n    ax.text(dx*length, dy*length, animal, fontsize=9)\n\n# Display the full legend\nfrom matplotlib.lines import Line2D\nlegend_elements = [Line2D([0], [0], marker='o', color='w', label=group,\n                          markerfacecolor=color, markersize=10)\n                   for group, color in group_colors.items()]\nax.legend(handles=legend_elements)\n\nplt.show()"
  },
  {
    "objectID": "posts/2024-03-15-embeddings/index.html#conclusion",
    "href": "posts/2024-03-15-embeddings/index.html#conclusion",
    "title": "Visualizing Embeddings in 2D",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we have explored the foundational principles of embeddings and brought them to life through visualization. Let’s revisit the technical definition of an embedding from the beginning, its meaning should be much clearer now:\n\n“In the context of machine learning and natural language processing, embeddings are numerical vector representations that capture the semantic essence of text entities, such as words, sentences, or documents. These vectors are typically high-dimensional, often consisting of hundreds or thousands of dimensions, allowing them to encode complex concepts and relationships. The fundamental idea behind embeddings is that texts with similar meanings are represented by vectors that are mathematically close to each other. This representation enables algorithms to process and analyze texts by understanding their underlying semantic content.”\n\nWe have seen how embeddings are numerical representations of text, in the example we used numerical representations of animals (“lion”, “tiger”, “flamingo”, “clownfish” etc.) which contain semantic information. We have reduced the dimensionality of the vectors with 384 dimensions to only 2 dimensions to plot the 2D vectors. We have visually seen that the semantic information of the data remained intact even in the reduced vectors because the points representing the animals formed the clusters of the dataset (“Cats”, “Birds”, “Insects”, “Fish”) we did not show to the embedding model. This proximity of the points (their Euclidean distance) represents their semantic relation to each other. Finally, we discussed and plotted the cosine similarly which has advantages for calculating vector similarity in machine learning use cases.\nIn closing, regardless of how complex the math might seem, I hope you have gained a more intuitive understanding of embeddings and the underlying concepts they are built upon."
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html",
    "href": "posts/2022-11-26-mnist/index.html",
    "title": "MNIST, the ‘Hello World’ of Computer Vision",
    "section": "",
    "text": "After Cat vs. Dog, this is the next challenge for me in computer vision: Building on chapter 4 of the book, I challenged myself to implement a model based on the MNIST dataset, as recommended as further research.\nThis challenge is also available as a Kaggle competition, and I found this bit of competitive spirit to add some spice to the project. Additionally, it broadened the spectrum of implementation topics, because training a model is one thing, but using it for meaningful predictions in equally important and also required some effort. Last, but not least, submitting the results was a nice way to check if the results are actually correct. As predicted: “This was a significant project and took you quite a bit of time to complete! I needed to do some of my own research to figure out how to overcome some obstacles on the way”.\nI took an iterative approach, following a similar path as for working on the Titanic-Challenge:\nIt was a challenging project, and I learned a lot on the way. Below are some key points and learnings."
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html#the-fast.ai-version-without-a-submission",
    "href": "posts/2022-11-26-mnist/index.html#the-fast.ai-version-without-a-submission",
    "title": "MNIST, the ‘Hello World’ of Computer Vision",
    "section": "The Fast.AI version without a submission",
    "text": "The Fast.AI version without a submission\nBy now this is pretty straight-forward for me. I just copy&pasted a few lines of code to do the training, and I was able to create a decent model very quickly in this notebook.\nThe catch with this version is, however, that it is not ready for the mass data load: 28.000 predictions need to be done in the competition - something which I addressed in my second iteration.\nAdditionally, I found it interesting that the MNIST dataset was already pushing the limits of my laptop: The training time of about 40 minutes was ok, but it is already quite a burden if it needs to be done multiple times. Moving the learning to Paperspace, training on a free GPU, was 10x faster (no surprise). Since I like to still have everything locally, it is quite convenient moving files back and forth via git, also for the .pkl-files. This way the training can be done with GPU, and the inference can be done locally. Interestingly, in all my other notebooks, local performance was not an issue. (But I expect that to change in future other projects)"
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html#resubmitting-working-with-the-csv-files",
    "href": "posts/2022-11-26-mnist/index.html#resubmitting-working-with-the-csv-files",
    "title": "MNIST, the ‘Hello World’ of Computer Vision",
    "section": "Resubmitting: Working with the csv-files",
    "text": "Resubmitting: Working with the csv-files\nI found not very elegant to just convert the csv-files to png-images. That seems convenient, but a bit wasteful. Therefore, I re-implemented the process this notebook.\nIt was surprisingly difficult to convert the data into the right formal in memory so that the learner would accept the image. But finally I was able to convert a PIL.Image.Image to fastai.vision.core.PILImage. As usual with these things, once it was done, it looks easy.\nNot surprisingly, but a nice way to verify the result, the submission score was the same:"
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html#my-first-submission",
    "href": "posts/2022-11-26-mnist/index.html#my-first-submission",
    "title": "MNIST, the ‘Hello World’ of Computer Vision",
    "section": "My first submission",
    "text": "My first submission\nWhen I first downloaded the Kaggle data, I was quite surprised to see that the download did not contain any image files, but just 2 large csv-files. Since I only knew how to handle images, I simply converted the data to png-images in this notebook.\nOnce that was done, I could take the model trained before in my first notebook to make my first submission. In this notebook, I took the converted images and collected the predictions. I found the result of 99.4% quite impressive."
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html#the-from-scratch-version",
    "href": "posts/2022-11-26-mnist/index.html#the-from-scratch-version",
    "title": "MNIST, the ‘Hello World’ of Computer Vision",
    "section": "The from-scratch version",
    "text": "The from-scratch version\nDoing it all from scratch was an interesting learning exercise because I think that I already had a good understanding of what needed to be done even before implementing it. But, as it turns out, there is this tremendous difference between thinking that you understood it, and actually implementing it. There is a lot of fine print, and you have to pay attention to the details: Formatting the data, getting it into the correctly shaped tensors, and implementing the gradient descent. Irrespective of what I had learned/understood before, this has greatly deepened and solidified by implementing the MNIST challenge.\nSome minor mysteries remain, which I also documented in the notebook, if you can guide me how to fix them, please let me know.\nThe finale result of my from scratch-version is not up to the first implementation with resnet18, but I am proud of it for other reasons ;)."
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html",
    "title": "Running LLama2 locally on a Mac",
    "section": "",
    "text": "Running a large language models (LLM), namely llama2, locally on my Mac was the next logical step for me working through the hacker’s guide by Jeremy Howard. While it was possible to adjust Jeremy’s approach on Hugging Face to also work on Apple Silicon, I focussed on llama.cpp and its python binding llama-cpp-python to talk to llama2.\nThe whole journey consisted of the following steps, and I am going to take you though all of them to share my learnings along the way:"
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#getting-access-to-llama2",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#getting-access-to-llama2",
    "title": "Running LLama2 locally on a Mac",
    "section": "Getting Access to Llama2",
    "text": "Getting Access to Llama2\nFirst things first: Before, you can access the Llama2 model, you need to agree to Meta’s the terms and conditions for Llama2. As per the time of writing this, the process was as follows:\n\nVisit the model’s home page at Hugging Face\nGo to Meta’s website, and complete the registration form\nConfirm the terms and conditions on the Hugging Face Website (see screenshot)\n\nThe approval only took a couple of minutes."
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#running-llama2-via-hugging-face",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#running-llama2-via-hugging-face",
    "title": "Running LLama2 locally on a Mac",
    "section": "Running Llama2 via Hugging Face",
    "text": "Running Llama2 via Hugging Face\nTrying to stick as closely as possible to the original hacker’s guide, I wanted to run LLama2 locally on my Mac using the Hugging Face API, just to see if it worked. Without Nvidia support, I needed to adapt the code to make it compatible with Apple’s Metal Framework. For all all the details, what needed to be done to run llama2 via the Hugging Face API, please check out this notebook.\nThe final result was academically interesting, but performance left much to be desired 😉: What Jeremy’s machine did in 2 seconds took my MacBook more than 3 minutes. There are probably a couple of reasons which produced this dramatic difference in performance:\n\nNvidia memory throughput is a lot better then Apple’s unified RAM\nThe model I used was originally optimized and quantized for Nvidia GPUs. To run this model on my MacBook, I had to disable the 8-bit quantization (load_in_8bit=False) among other changes. While this adaptation was necessary for compatibility with Apple Silicon, it discarded all the optimizations.\nPyTorch’s optimization for CUDA is probably still way better than its MPS optimization.\n\nHere is a key learning: Running large language models (LLMs) locally requires more than brute force. Instead, hardware and software need to be aligned. Apple Silicon machines are extremely capable, but they need a different kind of optimization then Nvidia hardware. Consider the following analogy: Imagine you need to travel from Hamburg to Munich, and you have 2 hardware setups available, a car (let’s say this represents Nvidia hardware) or a plane (let’s say this represents Apple Silicon). Both these hardware setups require different optimizations to get from A to B.\nDriving from Hamburg to Munich by car (representing Nvidia hardware), you optimize your path along the roads. If you used the plane instead (representing Apple Silicon), the same optimization would not work well. Attempting to navigate the plane on the roads, as you would a car, is highly impractical. Therefore, you would use a different way to optimize the path: You take public transport or a taxi to the airport, you fly from Hamburg to Munich, and again, you take public transport or a taxi to reach your final destination. On both hardware setups you have reached your Munich, but the underlying setup and optimizations differed significantly.\nTherefore, let’s hop on the plane, and let’s explore a different way to run llama2 to on a Mac: Let’s turn our attention to llama.cpp.\n\n\n\n\nDalle: A llama driving a car and another llama flying a plane on the road from Hamburg to Munich"
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#what-is-llama.cpp",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#what-is-llama.cpp",
    "title": "Running LLama2 locally on a Mac",
    "section": "What is llama.cpp?",
    "text": "What is llama.cpp?\nLlama.cpp is an optimized library to run a large language model (LLM) like Llama2 on a Mac, but it also supports other platforms. How is this possible? For the details, please let me refer to this tweet by Andrej Karpathy and for even more details to this blog post by Finbarr Timbers. Here are my takeaways:\n\nLlama.cpp runs inference of LLMs in pure C/C++, therefore, it is significantly faster than implementations in higher languages like python.\nAdditionally, the mission of the project “is to run the LLaMA model using 4-bit integer quantization on a MacBook”. This means that numbers used to represent model weights and activations downsized from 32- or 16- bit floating points (the format of the base models) with 4-bit integers. This reduces memory usage and improves the performance and efficiency of the model during inference. The somewhat surprising thing is that model performance does not degrade by this downsizing.\n\nWhen I mentioned before that I had to turn off quantization on Hugging Face, here we turn it on a again, just differently."
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#how-you-can-use-llama.cpp-from-python",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#how-you-can-use-llama.cpp-from-python",
    "title": "Running LLama2 locally on a Mac",
    "section": "How You Can Use llama.cpp from Python",
    "text": "How You Can Use llama.cpp from Python\nThe project llama-cpp-python serves as a binding for llama.cpp, providing access to the C++ API to Llama2 from Python.\nIn this context, a “binding” is a bridge that facilitates interaction between two programming languages, i.e. a layer of code that allows two programming languages to interact with each other. Llama.cpp is written in C/C++, and the llama-cpp-python binding allows this C/C++ library to be utilized within a Python environment. Essentially, the Python code wraps around the C/C++ code so that it can be called from a Python environment.\nWhile it might sound complicated, the concept is surprisingly accessible when you reduce the context to a simple example. To keep the focus in this blog post, I separated the exploration of C bindings into this blog post (LINK)."
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#installing-llama-cpp-python",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#installing-llama-cpp-python",
    "title": "Running LLama2 locally on a Mac",
    "section": "Installing llama-cpp-python",
    "text": "Installing llama-cpp-python\nFirst, we need to install llama-cpp-python via pip install llama-cpp-python.\nUpgrading is done via pip install llama-cpp-python  --upgrade --force-reinstall --no-cache-dir.\n\n💡 Note: To execute the steps interactively, please check out my related notebook."
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#downloading-the-model",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#downloading-the-model",
    "title": "Running LLama2 locally on a Mac",
    "section": "Downloading the Model",
    "text": "Downloading the Model\nFor all my experiments, I used the following model: TheBloke/Llama-2-7b-Chat-GGUF\nTo download the model, please please run the code below, assuming that you have stored your Hugging Face access token in the .env-file. For additional insights/troubleshooting, please also check out my previous blog post / my previous notebook:\n\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\ntoken = os.getenv('HF_TOKEN')\nos.environ['HF_TOKEN'] = token\n!huggingface-cli login --token $HF_TOKEN\n!wget -P ../models https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf"
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#loading-the-model",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#loading-the-model",
    "title": "Running LLama2 locally on a Mac",
    "section": "Loading the Model",
    "text": "Loading the Model\nLoading the model, only required 2 lines of code. Let’s talk about the parameters:\n\nn_ctx=2048: This sets the context window to 2048 tokens. The maximum number of tokens for this model is 4096.\nverbose=False: This makes the model less talkative. It only prints the actual results when prompted. Please feel free try turning it to True to get additional information from the model, not just the generated response.\n\n\nfrom llama_cpp import Llama\nllm = Llama(model_path=\"../models/Llama-2-7b-chat/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=2048, verbose=False)\n#llm = Llama(model_path=\"../../../lm-hackers/models/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=2048, verbose=False)"
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#completion-vs.-chat-completion-example",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#completion-vs.-chat-completion-example",
    "title": "Running LLama2 locally on a Mac",
    "section": "Completion vs. Chat Completion Example",
    "text": "Completion vs. Chat Completion Example\nThere are 2 ways we can talk to the LLM: The completion method literally does what it promises: It completes a prompt. For having a conversation with the LLM, we need to use Chat Completion.\nAs per the Getting Started guide, let’s look at one example each on how to use the API:\nLet’s do text completion first.\n\noutput = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=128, stop=[\"Q:\", \"\\n\"], echo=True)\nprint(output['choices'][0]['text'])\n\nQ: Name the planets in the solar system? A: 1. Mercury 2. Venus 3. Earth 4. Mars 5. Jupiter 6. Saturn 7. Uranus 8. Neptune\n\n\nFor the chat completion, let’s re-write the code to reproduce the example from the hackers guide to make the LLM talk about money in aussie slang.\n\naussie_sys = \"You are an Aussie LLM that uses Aussie slang and analogies whenever possible.\"\n\nmessages=[\n    {\"role\": \"system\", \"content\": aussie_sys},\n    {\"role\": \"user\", \"content\": \"What is money?\"}]\n\nmodel_response = llm.create_chat_completion(messages = messages, stream=False)\nprint(model_response['choices'][0]['message']['content'])\n\n  Fair dinkum, mate! Money, eh? It's like the oxygen we breathe, ya know? (laughs) Just kiddin', but seriously, money is like the lifeblood of society. It's what keeps the economy tickin' over and allows us to buy the things we need and want.\nThink of it like this: money is like a big ol' pile of dough (get it? Dough? Like bread dough? Ah, never mind). We all gotta work hard to earn that dough, whether it's through our day job or by startin' our own business. And then we can use that dough to buy things like food, shelter, and a cold one at the pub after work.\nBut here's the thing: money ain't everything, mate. There's more to life than just makin' dough. We gotta find meaning and purpose in our lives, or else we'll be livin' like a dog (sorry, dogs!). So, while money's important, it's not the only thing that matters.\nNow, I know some blokes might say, \"Money, money, money! That's all that matters!\" But let me tell you, mate, they're barkin' up the wrong tree (get it? Barkin' up the wrong tree? Ah, never mind). There's more to life than just chasin' after the green.\nSo there you have it, mate! Money's like a big ol' pile of dough that we all gotta work hard to earn. But don't forget, there's more to life than just makin' dough. Keep on keepin' on, and always remember: money may not buy happiness, but it can buy a cold one at the pub after work! (laughs)"
  },
  {
    "objectID": "posts/2024-02-15-running-llama2-on-mac/index.html#conclusion",
    "href": "posts/2024-02-15-running-llama2-on-mac/index.html#conclusion",
    "title": "Running LLama2 locally on a Mac",
    "section": "Conclusion",
    "text": "Conclusion\nUsing the right approach, running an LLM, in this case llama2, in a Jupyter notebook on a Mac is not really difficult. Once you sorted out the setup (like terms and conditions), starting up llama2 via llama-cpp-python only requires a few lines of code. Happy chatting!\n\n\n\n\nDalle: Aussie LLM depicted as kangaroo talking about money with Aussie slang and analogies"
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "",
    "text": "Based on the Fast.AI lesson 4, I transferred the approach from Jeremy’s notebook “Getting started with NLP for absolute beginners” to the Kaggle competition “Natural Language Processing with Disaster Tweets”. This post describes my approach and the key learnings.\nMy approach was the following: Classifying tweets or patent phrases is essentially “the same” task:"
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#key-learnings",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#key-learnings",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Key Learnings",
    "text": "Key Learnings\nIn the last about 4 week I probably trained 250+ model and made 70+ submissions, trying to build up intuition on what works and what not. Therefore, please treat this as an empirical report, not as a scientific paper. I try to back my claims with actual data of my model training, but sometimes I can only report on observations and I try to reason why these observations makes sense. Here are my key learnings:\n\nCleaning the data helps, both syntactically and semantically: Not only did I clean up special (rubbish) character, but also re-classified some tweets, mainly automatically.\nUpon cleaning the data, keep a close eye on what is noise and what is signal, for example, converting everything to lower case did not help, because I believe that it removed signal.\nHelping the model understand the data helps by using special tokens, for example by replacing URLs with a special token.\nUsing bigger models helps, on average moving from the small to the base to the large versions of the deberta models increased the score by a few tenth. However, for training large models on Kaggle, you need to apply some tricks not to run out of memory. Additionally, training bigger models is comparable time-consuming.\nSmall batch sizes help to train models more quickly.\nShowing the model more data then just the initial training set helps.\nOverall, the pre-trained models are already very good. My first version submission of the baseline notebook scored 0.81949, in the current final iteration my best submission scored 0.84676. That is an increase of only 2.7 percentage points. The bigger difference is the rank on the leaderboard. At the time of writing 0.81949 would have put me on 218/938 while 0.84278 put me on rank 34/938. If you deduct the submission which scored 100% (29 submission), I am pretty happy with rank 5. 😀\n\n\nOn a side note: I wonder how I would score by hand-labeling all tweets in the test set. Would I beat AI or would AI beat me? Find out at the end."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#syntactical-data-cleansing",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#syntactical-data-cleansing",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Syntactical Data Cleansing",
    "text": "Syntactical Data Cleansing\nThe dataset for the disaster tweets is far from perfect in terms of data quality, I guess it is just real world data 😉. Even though the model was doing a very good job at classifying tweets even without any additional work, some data cleaning was called for.\nThere are quite a few non-sense special characters in the dataset, for example there are HTML representations spaces as %20, or leftovers of an unsuccessful unicode conversion, for example Ûª which should be '. I did replace the rubbish characters where possible, and I simply deleted any other non-ascii characters. I suspect that more could be done here to, for example trying to reconstruct emojis, but I could not find a way to do that. (suggestions welcome)\nI did not take the route of converting everything to lowercase or removing stop-word, because it did not have any positive training effect (just the opposite!). My theory on this would be that I would have removed signal by doing these optimizations (more on that in a bit). On another train of thought, a modern language model should be able to deal with stop word, abbreviation etc. anyway, so why bother to put in time and effort to clean data where it is not required. What is your experience or opinion? Are these kinds of optimizations becoming obsolete as we move from classical machine learning to large language models?"
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#semantic-data-cleansing-correcting-incorrectly-labeled-tweets",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#semantic-data-cleansing-correcting-incorrectly-labeled-tweets",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Semantic Data Cleansing: Correcting incorrectly labeled Tweets",
    "text": "Semantic Data Cleansing: Correcting incorrectly labeled Tweets\nBrowsing through discussions on the competition, there was the claim that some tweets in the training set were not labeled correctly. As it turned out: That is true. But how to correct this without reading every tweet and potentially re-labeling it manually? (Something I definitely would not do!)\nAs it turned out, there are also tweet duplicates which are sometimes not labeled identically. These were pretty easy to catch:\n\nIntroducing a field which counts the occurrences of the tweets. (Since the duplicates were not 100% identical, for example, they contained different URLs, the counting was only possible after converting the URLs to special tokens (see below)).\nCreating a new field which contains the average tweet label\nRounding the label to get a majority vote\n\nThe only catch with this procedure is that tweets which have exactly one duplicate cannot be corrected this way. Well, what can you do about it, I had to re-label these ones manually."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#noise-vs.-signal",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#noise-vs.-signal",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Noise vs. Signal",
    "text": "Noise vs. Signal\nWhen introducing new data or removing data, you need to think about what is noise (i.e. unwanted interference) and what is signal (i.e. data which is helpful). Or putting it into other words: Which elements of the data help the model to learn and which elements create confusion or distraction? Let’s consider the following examples.\nInspired by the notebook “Getting started with NLP for absolute beginners” my first “improvement” was to concatenate the additional columns (keyword, location) of the dataset into the tweets. I was pretty disappointed that the result was worse then the baseline, and even with additional optimization (e.g. special tokens) it was impossible to beat the baseline. Finally, I removed the additional fields, and voilà, the result was immediately better! Why? The keywords added noise, not signal. The keyword “collision”, for example, is also assigned to many non-disaster tweets.\nAnother source of noise can be the URLs which are part of the tweets: The content of the URLs themselves is pretty random, especially for shortened ones. To make the URLs more meaningful for the language model, I tried the following:\n\nEnclosing the URLs with special tokens for URL beginning and URL end -&gt; did not help\nRemoving the URLs completely -&gt; better but not the best solution\nReplacing the URLs with a special token -&gt; empirically the best solution\n\nWhen thinking about it, it makes sense: The fact that there is a URL in a tweet seems to contain signal, but the content of the URL (i.e. random characters) does not.\nAs a final topic for noise vs. signal, let’s consider capitalization. Even though suggested in many sources, converting the text to lower case did not yield a better result (quite the opposite!). Again, I could imagine that capitalization carries signal. Think of capitalization as shouting. Therefore, it makes sense not to convert everything to lower-case because you would remove signal from the tweets. Consider these 2 examples where there is a difference between “burning” and “BURNING”:\n\nimport warnings, logging\n\nwarnings.simplefilter('ignore')\nlogging.disable(logging.WARNING)\n\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\ntokz = AutoTokenizer.from_pretrained('microsoft/deberta-v3-small')\n\ntokz.tokenize(\"The house is BURNING.\")\n\n['▁The', '▁house', '▁is', '▁BURN', 'ING', '.']\n\n\n\ntokz.tokenize(\"The house is burning.\")\n\n['▁The', '▁house', '▁is', '▁burning', '.']"
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#working-with-special-tokens",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#working-with-special-tokens",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Working with Special Tokens",
    "text": "Working with Special Tokens\nOne of the steps of syntactically cleaning the dataset was the removal of special characters, many of them being misrepresentations of unicode characters. There are, however, also meaningful special characters. In the tweets, not prominent the hashtag # and the mention @. Since these special characters carry signal, I decided to turn them into special tokens. This time, however, I did not replace the whole mention or keyword with a special token, but I wrapped the mentions in special tokens:\n\n[MB]: Mention Beginning\n[ME]: Mention End\n[HB]: Hashtag Beginning\n[HE]: Hashtag End\n\nAfter stating what I have been doing in full confidence, I have to admit that, at the time of coding, I do not really know what I was doing, I was rather implementing it in analogy to the two notebooks “Getting started with NLP for absolute beginners” and “Iterate like a grandmaster”.\nWhile writing up this blog post I was doing a little bit of research on special tokens, which was not as straight-forward as I imagined it to be. Chatting with ChatGTP quite was insightful, and I learned about the usage of the predefined special tokens, which can be retrieved by inspecting the attribute tokz.all_special_tokens: (I hope the following is accurate)\n\nThe “[CLS]”-special token (“classification token”) is used to represent the entire input sequence in tasks such as text classification, sentiment analysis and named entity recognition etc. The token is added to the beginning of the input sequence and is then passed through the model along with the rest of the input. In the context of disaster tweets, if a tweet was “There is a fire”, after adding the token, the tweet looked like this: “[CLS] There is a fire”. In the final version of the notebook I did add the [CLS]-token to all the tweets, but I did not notice any improvement in model performance.\nThe “[SEP]”-special token (“separation token”) is used to separate multiple sentences or segments within a single input sequence. The token is added at the end of each sentence or segment, and it is used to indicate the end of one segment and the beginning of another. I did not add any [SEP]-tokens to the tweets because a tweet is a unit of its own. Interestingly, the tokens for . and [SEP] are not the same, which makes sense, because not any .-character is automatically a separator.\n\nThe “[UNK]”-special token is used to represent unknown or out-of-vocabulary words that the model has not seen during training. During the preprocessing step, any word that is not in the vocabulary is replaced with the “[UNK]” token. Somehow I would have expected the tokenizer to replace unknown words with [UNK], but that did not happen whatever I tried.\nThe “[PAD]”-special token is used to pad the input sequences to a fixed length. A BERT (and deberta) model is a transformer-based model which requires that all input sequences have the same length before they can be passed through the model. I did not use this token, but I trust the inner mechanics of the model to take care of this.\nThe “[MASK]”-special token is used in the pre-training process called Masked Language Model (MLM). In this task, a proportion of tokens in the input sequence is replaced with the “[MASK]” token, and the model is trained to predict the original token that was replaced by the “[MASK]” token. This pre-training process allows the model to learn the context of the words in the sentence and understand the relationship between words in the sentence. Since this is a token used in training, I did not use it in my notebooks."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#training-bigger-models",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#training-bigger-models",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Training Bigger Models",
    "text": "Training Bigger Models\nAfter I was done cleaning the data, I tried to work with bigger models, and the result is: Yes, size does matter. The improvements were not dramatically better, nonetheless significant. I worked with microsoft/deberta-v3-small, microsoft/deberta-v3-base and microsoft/deberta-v3-large. Here are the best scores by model (even though the training approaches are not 100% comparable):\n\nmicrosoft/deberta-v3-small: 0.83757\nmicrosoft/deberta-v3-base : 0.84002\nmicrosoft/deberta-v3-large: 0.84676\n\nUpgrading from the small model to the base model was a smooth process. The expected trade off between better results and longer training time materialized as expected. When moving to the large model, that process was not so smooth, because the kaggle runtime was running out of memory both in the GPU and on disk. Here is how I fixed it:\n\nFixing running out of GPU memory: Larger models require more memory in the GPU. This can easily be fixed by reducing the batch size. A small batch size may be a desired training parameter (as I will discuss later), however, if you do not want to increase the number of gradient descent steps, you can use gradient accumulation (also more on that in a later section).\nFixing running out of disk memory: The Hugging Face Trainer saves huge checkpoint files after every 500 training steps by default. When working with small batch sizes or larger numbers of epochs, this can exceed the allowed disk space by Kaggle. The easy fix is to disable the checkpoint file creation. This can be done with the parameter save_steps=-1 which you need to pass to the TrainingArguments class."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#gradient-accumulation",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#gradient-accumulation",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Gradient Accumulation",
    "text": "Gradient Accumulation\nAs presented in Live Coding Session 10, Gradient Accumulation is a technique to train large models on regular hardware.\nIf you run out of memory on a GPU, you need to decrease the batch size. With the same number of epochs this increases the number or optimization cycles. To train a larger model with the same number of iteration cycles, gradient accumulation can be used, because it results in updating the model only after x number of batches.\nAs it turns out, gradient accumulation can also easily be used with a Hugging Face Trainer, you just need to pass the parameter gradient_accumulation_steps to the TrainingArguments-class:\n\n    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n        num_train_epochs=epochs, weight_decay=0.01, report_to='none', gradient_accumulation_steps=gradient_accumulation_steps)\n\nWhen I first tried gradient accumulation, I was puzzled by the result. Smaller batch sizes yielded better results, and in my initial attempts, the bigger models even performed worse than small ones. This was, I assume by now, because I was accumulating the gradients too much.\nWhile gradient accumulation is for sure a good tool for some problems, it was not part of my best submissions because I also came to learn that the small batch sizes were more beneficial in training. What started as a perceived problem (being forced to lower the batch size), turned out to create better results and quicker training."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#implementing-metrics",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#implementing-metrics",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Implementing Metrics",
    "text": "Implementing Metrics\nBefore turning to the discussion of smaller batch sizes, let me first address the computation of metrics, because it will be useful when evaluating the batch size.\nI did not pay too much attention to this at first because implementing metrics (in addition to the loss) was not part of my baseline, and I was focussing on other topics first (as outlines above), but upon writing this post, I noticed the white spot.\nThe evaluation of the kaggle competition is based on the F1 score. Based on this tutorial leveraging the evaluate library, the implementation was surprisingly easy.\n\nImplement a function based on the pattern described in the tutorial or in the evaluate docs\nAs metrics, there is a lot to choose from: This is the full list.\n\nMy final metric implementation looks like this:\nimport numpy as np\nimport evaluate\n\ndef compute_metrics(eval_preds):\n    metric = evaluate.load(\"f1\")\n    preds, labels = eval_preds\n    #predictions = np.argmax(logits, axis=-1) #not needed\n    return metric.compute(predictions=preds, references=labels)\n\nPersonal note: With the disaster tweet dataset, the line predictions = np.argmax(logits, axis=-1) caused a type error. For solving it, this tutorial had exactly the right input I needed to debug the problem."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#using-small-batch-sizes",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#using-small-batch-sizes",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Using Small Batch Sizes",
    "text": "Using Small Batch Sizes\nWhat started as a curiosity got solidified when I found this tweet by Yan LeCun:\n\n\n\n\n\nTherefore, small batch sizes are a recognized way to train faster in a way which can generalize better. There seems to be a tradeoff between smaller batch size and training time per epoch because the GPU is not used as efficiently with smaller batch sizes. In my disaster tweet project, small batch sizes proved to be helpful.\nIs there a way to intuit why a small batch size is a good thing? Yes, I think so: When you think about the training set as book with 1000 pictures that we want to learn the labels of, the batch size symbolizes how many pictures you look at before you think about what you have seen. With a batch size of 100, you would look at 100 pictures, reflect (i.e. doing the learning, or updating the model based on the gradients), look at another 100 pictures, reflect etc. Within an epoch, i.e. looking though the whole book, you would reflect on what you have seen 10 times. At a batch size of 10, you would look at 10 picture, reflect, look at 10 more pictures, reflect… Within an epoch, you would reflect 100 times (not 10 times), i.e. you would actually do more learning instead of just looking at the pictures. Therefore, it makes sense that a smaller batch size can result in better and quicker training results.\nAnother way to thin about this: By lowering the batch size, the iteration speed is increased. Quoting Andrej Karpathy with some additions [in bracket]:\n\n“[When] dealing with mini batches the quality of our gradient is lower, so the direction is not as reliable, it’s not the actual gradient direction [compared to taking a larger batch], but the gradient direction is good enough even when it’s estimating on only 32 examples that it is useful and so it’s much better to have an approximate gradient and just make more steps than it is to evaluate the exact gradient and take fewer steps. So that’s why in practice this works quite well.”\n\nTo support the claim that smaller batch sized help/work, here is a comparison of training 4 epochs with different batch sizes. As a result, you can see that training time increases, but the training quality increases as well, so much that the model already shows signs of overfitting when training for more than 2 epochs at a batch size of 8 and below:\n\nYou can also see how the model is getting more confident about the predictions in the histograms: The smaller the batch size, the more the predictions peak at 0 and 1, and the number of uncertain cases decreases:\n\nI wonder how general this finding about smaller learning rates is… At least I am now sensitive to the topic and I will continue to watch it."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#showing-the-model-more-data",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#showing-the-model-more-data",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Showing the model more data",
    "text": "Showing the model more data\nIn the notebook “Iterate like a grandmaster”, Jeremy suggested for further improvement: “Before submitting a model, retrain it on the full dataset, rather than just the 75% training subset we’ve used here.” I was wondering how I could follow the advice, and I came up with 2 ideas. The first one did not work, the second did:\n\nTraining 3 models with differently shuffled training and validation sets. In the end the models would be averaged. I did not notice a significant effect, therefore I did not pursue this approach any further. When you think about it, it also makes sense, because in the end you are training identical model, because, if done right, even given different data, the model capabilities should converge to the same capabilities. Only if different training approaches had been used, the ensemble would help.\nTrain a model, then re-shuffle the dataset again to show the model a different training set and a different validation set. Therefore a generically pre-trained model becomes a pre-pre-trained model. In this particular case the deberta model was trained on tweet classification now. It actually worked, shuffling had a good positive effect.\n\nShuffling once: Notebook V5 scored 0.83481\nShuffling twice: Notebook V6 scored 0.83879\nShuffling a third time: Notebook V7 scored 0.84002\nShuffling more often had a negative effect, my notebook V8 scored only 0.83542, most likely due to over-fitting.\n\n\nTo cross check that the improved training results were not just caused by more training, I did the following experiment, training the small model at batch size 32:\n\nTraining 4 epochs: Score 0.82715\nTraining 6 epochs: Score 0.8345\nTraining 8 epochs: Score 0.83297\nTraining 10 epochs: Score 0.8345\nTraining 4 epochs, but re-shuffling 3 times and training 2 more epochs: Score 0.83634\n\nWhile the effect of “more training help” is definitively visible, the attempt including the reshuffling was the best result. This is not surprising, because the model can learn more by looking at more diverse data.\nDiscussing the question of how to use 100% of the training data on the forums lead to a third approach: First, I trained the model as “as usual”, specifically with training for 2 epochs with batch size 4, afterwards I showed the model the full dataset for another epoch. The results were better than doing the re-shuffling approach. Again, as a disclaimer, the training approaches were not identical, so it is difficult to directly compare the results. Nonetheless, my best result of 0.84676 used the following training approach: Training the large model for 2 epochs at bs=4. Afterwards training another epoch with the full training set and at bs=4 but half the learning rate.\nIt would be interesting to systematically explore the matter. Maybe I will return to this another time."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#conclusion",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#conclusion",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Conclusion",
    "text": "Conclusion\nWhen starting out with lessen 4, I did not expect it to become such an extended endeavor. Porting the notebook notebook “Getting started with NLP for absolute beginners” to the Kaggle competition “Natural Language Processing with Disaster Tweets” introduced me to many different aspects of natural language processing in particular and machine learning in general.\nMaybe the most profound learning is that natural language processing with pre-trained models is already very robust. Consider all the effort I put in to improve from a baseline of 0.81949 to 0.84676, where the majority of the improvement was due to better training parameters or bigger models. The dataset is challenging and an honest 100% score is impossible, because there is also quite some subjectivity in there, and quite a few tweets are ambiguous. Putting myself to the test: I tried to compete against my models, just relying on my pre-trained brain: Would I beat 84.6%?\nI must admit, I did not hand-label the whole dataset, but only the first 100 tweets of the test set. Then I compared my results to the leaked 100%-solution. My score was 81% (even worse than my baseline!) - so I lost the competition: AI beat me in this competition - who would have thought?"
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#credits-working-with-ai-as-a-team",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#credits-working-with-ai-as-a-team",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Credits: Working with AI as a Team",
    "text": "Credits: Working with AI as a Team\nWhile I wrote this blog post myself (no longer a given these days…), I need to give credit to ChatGPT for helping me in the research and also for the implementation of some functions, especially the regular expressions. While not all the replies from ChatGPT were correct, they were mostly helpful and frequently pointed me in the right directions. I also enjoy just ChatGTP is a non-annoying website in the sense that is does not ask for cookies after each question, and it does not want me to sign up for newsletters etc. More time is spent on thinking and problem solving than on these distractions.\nThe title picture of this blog post was generated by DALL-E 2 upon the prompt: “twitter logo on fire digital art”."
  },
  {
    "objectID": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html",
    "href": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html",
    "title": "How to Convert a Wordpress Blog into Markdown",
    "section": "",
    "text": "UPDATE DATE / PICTURE\nIn this blog post, I will guide you through the steps to convert a Wordpress blog into markdown. While this my seem like a unrelated subject of this blog, it is a preparative for building a Retrieval Augmented Generation (RAG) notebook.\nWhy did I turn it into a blog post of its own? First of all, the conversion process was more difficult and extensive than expected, therefore I felt that this is worth sharing. Additionally, it highlights (again) that data is key in any machine learning project, and that data preparation can be a project of its own.\nBy now, the Wittmann-Tours blog is available my Wittmann-Tours GitHub repo under license CC-BY NC."
  },
  {
    "objectID": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#before-we-start",
    "href": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#before-we-start",
    "title": "How to Convert a Wordpress Blog into Markdown",
    "section": "Before we Start",
    "text": "Before we Start\nPlease treat this blog post as the personal documentation of how I did the conversion. I was somewhat surprised that there were so few resources on the internet covering the topic of converting Wordpress to markdown. I am definitively no expert on this subject, but following the steps documented in this blog post, I got the job done.\nAfter a bit of research I ended up working with this repo from Swizec. Thanks for putting this repo out there!"
  },
  {
    "objectID": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-1-export-the-xml-from-wordpress",
    "href": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-1-export-the-xml-from-wordpress",
    "title": "How to Convert a Wordpress Blog into Markdown",
    "section": "Step 1: Export the XML from Wordpress",
    "text": "Step 1: Export the XML from Wordpress\nThe first step the conversion process is to export your Wordpress blog content as an XML file. Here’s how to do it:\nNavigate to the export function of Wordpress blog by entering your site’s URL followed by /wp-admin/export.php, for example, https://wittmann-tours.de/wp-admin/export.php. Alternatively, you can navigate like this:\n\nLog into your Wordpress Dashboard. Navigate to the admin area of your Wordpress blog by entering your site’s URL followed by /wp-admin. Use your credentials to log in.\nAccess the Tools section. Once logged in, look for the Tools option in the left-hand sidebar. Hover over it, and you will see a dropdown menu.\nSelect Export: In the dropdown menu under Tools, click on Export. This will take you to a page where you can choose what content you want to export. For a complete backup of your site, select All content.\n\nFinally, you can download the export file: After selecting All content, click on the Download Export File button. Wordpress will generate an XML file containing all your selected data. Save this file to your computer."
  },
  {
    "objectID": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-2-check-software-requirements",
    "href": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-2-check-software-requirements",
    "title": "How to Convert a Wordpress Blog into Markdown",
    "section": "Step 2: Check Software Requirements",
    "text": "Step 2: Check Software Requirements\nDepending on your setup, you might need to install some software first. Here is what we need:\n\nNode.js: Node.js is a runtime environment that allows you to run JavaScript code outside of a web browser. It’s commonly used for server-side scripting and building backend services (like APIs), but it’s also used in tooling for front-end development, automation tasks, and more. In this case, Node.js is used to run the wordpress-to-markdown conversion script.\nnpm (Node Package Manager): npm is the default package manager for Node.js. It is used to install and manage dependencies (libraries, frameworks, tools, etc.) required by Node.js applications. npm facilitates easy sharing and reuse of code. When you install Node.js, npm should be included in the installation. In this case, we need npm to install Yarn.\nYarn: Yarn is an alternative package manager to npm. It performs the same basic function as npm (managing dependencies for Node.js applications) but often with some differences in performance, features, and the way dependencies are handled. In this case, Yarn was used to manage the dependencies of the wordpress-to-markdown script."
  },
  {
    "objectID": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-3-install-node.js-and-yarn",
    "href": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-3-install-node.js-and-yarn",
    "title": "How to Convert a Wordpress Blog into Markdown",
    "section": "Step 3: Install Node.js and Yarn",
    "text": "Step 3: Install Node.js and Yarn\nIf your system already fulfills these software requirements, feel free to skip this section.\n\nInstalling Node.js\n\nDownload Node.js. Visit the official Node.js website to download the latest version of Node.js. Choose the version that is compatible with your operating system.\nInstall Node.js. Follow the installation prompts to install Node.js on your system. The installer will guide you through the process.\nVerify the installation. To ensure that Node.js was installed correctly, open a terminal or command prompt and type the following commands: bash     node -v     npm -v These commands will display the versions of Node.js and npm installed on your system. Seeing the version numbers confirms that the installation was successful.\n\n\n\nInstalling Yarn\n\nOpen your terminal or command prompt.\nInstall Yarn globally using npm. Type the following command: bash     npm install -g yarn If you encounter permission errors, it might be necessary to run the command as an administrator or with superuser rights. In such cases, use: bash     sudo npm install -g yarn This will prompt you for your password to grant the necessary permissions.\nVerify the installation. To check if Yarn has been installed correctly, run: bash     yarn -v This command will display the version of Yarn installed, indicating that the installation was successful.\n\n\n\nFinal Checks\n\nCheck the PATH. It’s important to ensure that the installation paths for Node.js and Yarn are correctly added to your system’s PATH environment variable. This allows you to run these tools from any directory in your terminal. To check your PATH, type: bash     echo $PATH Verify that the paths to Node.js and Yarn are included in the output.\n\nAfter completing these steps, your system will be equipped with Node.js and Yarn, ready for the next phase of converting your Wordpress blog into Markdown."
  },
  {
    "objectID": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-4-clone-the-repository-and-run-the-conversion-script",
    "href": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-4-clone-the-repository-and-run-the-conversion-script",
    "title": "How to Convert a Wordpress Blog into Markdown",
    "section": "Step 4: Clone the Repository and Run the Conversion Script",
    "text": "Step 4: Clone the Repository and Run the Conversion Script\nIn this step we clone the GitHub repository and run the conversion script:\n\nOpen your terminal or command prompt: Ensure you’re in the directory where you want to clone the repository.\nClone the repository: Execute the following command to clone the wordpress-to-markdown repository created by Swizec: bash     git clone https://github.com/Swizec/wordpress-to-markdown This command downloads the repository to your local machine in a folder named wordpress-to-markdown.\nNavigate to the repository directory: Change into the newly cloned directory to run the conversion commands: bash     cd wordpress-to-markdown\nInstall dependencies: Before running the conversion script, you must install its dependencies. Use Yarn to install them by executing: bash     yarn install This command reads the package.json file in the repository and installs all the necessary packages and dependencies required to run the conversion script.\nCopy XML for wordpress-to-markdown directory: Copy the XML-file you downloaded in step 1 into wordpress-to-markdown directory.\nAdjust script or rename XML-file: Either rename your XML-file to test-wordpress-dump.xml or change line 25 of convert.js to the file name of your XML.\nRun the conversion script: After installing the dependencies, you can now run the conversion script with Yarn: bash     yarn convert This command initiates the conversion process, which reads your exported Wordpress XML file and converts its contents into Markdown files.\n\nOnce this step is completed, you have successfully converted your Wordpress blog content into Markdown mdx-files. The files are store in a new out-directory, containing one sub-directory per blog post."
  },
  {
    "objectID": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-5-convert-mdx-files-to-md-files",
    "href": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#step-5-convert-mdx-files-to-md-files",
    "title": "How to Convert a Wordpress Blog into Markdown",
    "section": "Step 5: Convert mdx-files to md-files",
    "text": "Step 5: Convert mdx-files to md-files\nSo far so good, but I was not yet 100% happy, because the mdx-files did not contain a proper level-1-heading, and Obsidian ignored the files.\nTo convert the mdx-files to md-files, I created a quick conversion notebook which made these final adjustments."
  },
  {
    "objectID": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#conclusion",
    "href": "posts/2024-03-08-how-to-convert-wordpress-into-markdown/index.html#conclusion",
    "title": "How to Convert a Wordpress Blog into Markdown",
    "section": "Conclusion",
    "text": "Conclusion\nConverting a Wordpress blog into Markdown turned out to be more complex than anticipated. Somehow I had anticipated there would be a simple, straightforward Wordpress plugin to get this done quickly, but no…\nIn the process to doing the conversion, I decided to document each step of the conversion in detail within this blog post. Not only did I want a reference for myself, knowing that revisiting the process even after a few weeks could be challenging without detailed notes, but I hope this guide is also useful for you reading this blog post.\nFinally, preparing the data source for my RAG project is done, which turned out to be a project of its own. (LINK)"
  },
  {
    "objectID": "posts/2024-01-27-how-to-call-openai-api/index.html",
    "href": "posts/2024-01-27-how-to-call-openai-api/index.html",
    "title": "How to call the OpenAI API from a Jupyter Notebook",
    "section": "",
    "text": "Exploring Large Language Models (LLMs) through their Web-based User Interfaces (WebUIs) is indeed insightful, particularly for experimenting with various prompt engineering techniques. However, accessing LLMs via their API unlocks a many additional possibilities. This approach not only allows you to craft your own applications but also enables the integration of LLMs into existing solutions. The use cases are endless: You can leverage LLMs for constructing comprehensive datasets, automating content creation, enhancing user interaction with natural language, personalizing user experiences, etc. The API access essentially opens doors to a more tailored LLM experience, more than just chatting with it.\nTo demonstrate how to access the OpenAI API for text generation, I created a Jupyter Notebook with all the steps from installing the necessary python packages, via managing access keys to calling the API with some examples. While you can perform all the steps in the Jupyter Notebook, in this blog post I would like to explore the concepts and take a look at what is between the lines of code, including my biggest learning: How does the chat with an LLM actually work.\nThis is the first blog post of a series in which I am reworking the hackers guide by Jeremy Howard and the accompanying notebook."
  },
  {
    "objectID": "posts/2024-01-27-how-to-call-openai-api/index.html#setup",
    "href": "posts/2024-01-27-how-to-call-openai-api/index.html#setup",
    "title": "How to call the OpenAI API from a Jupyter Notebook",
    "section": "Setup",
    "text": "Setup\nBefore we can start calling the OpenAI API, we need to setup a few thing:\n\nInstalling python packages\nGetting an API Key\nSecurely storing the API-key\n\n\nInstallation\nIf you have not done so already, pip install the openai package:\npip install openai\n\n\nGenerate API key\nTo be able to access the OpenAI API, you need an API access key. To obtain/generate the API-key from the Open.AI Website as also explained in the docs\n\n\nHow to securely store your API Access Key\nSince you do not want to put your API key into a Jupyter notebook, it is recommended that you store the API-key in a your python environment using python-dotenv.\npip install python-dotenv\nUsing dotenv, you store your API key in an environment file which you can easily access from within your Jupyter notebook. Here is a quick example, using an example file foobar.env which has the following content:\n# Exapmple\nFOO=\"BAR\"\nYou can import the variables like this:\n\nfrom dotenv import dotenv_values\n\nfoobar_config = dotenv_values(\"foobar.env\")\nprint(foobar_config)\n\nOrderedDict([('FOO', 'BAR')])\n\n\nIn real life, the usage looks like this, leveraging the environment variables from the os package:\n\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv(\"foobar.env\")  # This loads the .env file into the environment\n\nfoo_env_value = os.getenv('FOO')\nprint(foo_env_value)  # This will also print \"BAR\"\n\nBAR\n\n\nThe final step to real life is not to use foobar.env, but .env. Therefore, you need to add the following section to your .env-file:\n# Open AI\nOPENAI_API_KEY=\"My API Key\"\nOnce you load the .env-file, you are in business to call the OpenAI API\n\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv(\".env\")\n\nTrue\n\n\nImportant Note: Make sure, the .env file is not published to GitHub by including *.env in the .gitignore-file:\necho \".env\" &gt;&gt; .gitignore \nafterwards:\ngit add .gitignore \ngit commit -m \"Updated .gitignore to ignore .env files\"\ngit push"
  },
  {
    "objectID": "posts/2024-01-27-how-to-call-openai-api/index.html#calling-the-api",
    "href": "posts/2024-01-27-how-to-call-openai-api/index.html#calling-the-api",
    "title": "How to call the OpenAI API from a Jupyter Notebook",
    "section": "Calling the API",
    "text": "Calling the API\nSince the time of publication of Jeremy’s hackers guide the Open.AI API had changed. Therefore, the original code needed from some minor refactoring, essentially 2 thing:\n\nReplace ChatCompletion.create with chat.completions.create\nReplace c['choices'][0]['message']['content'] with c.choices[0].message.content\n\n\n#from openai import ChatCompletion,Completion\nfrom openai import chat\n\naussie_sys = \"You are an Aussie LLM that uses Aussie slang and analogies whenever possible.\"\n\n#c = ChatCompletion.create(\nc = chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n              {\"role\": \"user\", \"content\": \"What is money?\"}])\n\n#c['choices'][0]['message']['content']\nc.choices[0].message.content\n\n'Money, mate, is like the fuel that powers your financial engine. It\\'s the cold, hard cash and digital digits you use to buy stuff, pay your bills, and live your life. It\\'s a medium of exchange that keeps the economic gears churning. Think of it as the \"dollarydoos\" that keep the economic barbie cookin\\'!'\n\n\n\nNote for Enhanced Readability\nTo improve the readability of of the model responses in the notebook, especially if it contains long lines of text or code, you may want to enable word wrap in your development environment.\nFor Visual Studio Code Users:\n\nOpen the Command Palette (Ctrl+Shift+P or Cmd+Shift+P).\nSearch for Preferences: Open Settings (JSON) and select it.\nAdd \"notebook.wordWrap\": \"on\" to your settings.\nSave the settings.json file.\n\nEnabling word wrap will make long lines of code or text wrap to the next line, fitting within the cell’s width and eliminating the need for horizontal scrolling."
  },
  {
    "objectID": "posts/2024-01-27-how-to-call-openai-api/index.html#learnings",
    "href": "posts/2024-01-27-how-to-call-openai-api/index.html#learnings",
    "title": "How to call the OpenAI API from a Jupyter Notebook",
    "section": "Learnings",
    "text": "Learnings\nMy biggest take-away from having done this implementation is the realization how the chat with an LLM actually works. It is surprisingly simple, yet I had not realized this before: The chat with an LLM is stateless, which means that ChatGPT does not have a session open with you. Instead, the whole chat is passed to the model as context with every new prompt. This is the way the model knows what you have been talking about, and it can answer follow-up questions.\n\nc = chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n              {\"role\": \"user\", \"content\": \"What is money?\"},\n              {\"role\": \"assistant\", \"content\": \"Well, mate, money is like kangaroos actually.\"},\n              {\"role\": \"user\", \"content\": \"Really? In what way?\"}])\n\nc.choices[0].message.content\n\n\"Ah, glad you asked! Money, just like kangaroos, is all about value and trading, you see. Just as kangaroos hop around, money hops from one person to another in exchange for goods and services. It's the key to getting what you need and want in this modern world. Just like kangaroos in the outback, money roams around the economy, jumpin' here and there, makin' things happen. It's the backbone of our economic system, mate!\"\n\n\nOnce I had understood this, I started interacting with ChatGPT differently:\n\nInstead of having long chats which drifted from topic to topic, I try to keep the chats more focused. If the topic changes too much, I open up a new chat.\nI go back to prompts which did not yield the desired result more frequently, i.e. I edit the prompt instead of asking ChatGPT to correct something. This way you can keep undesired results out of the conversation which otherwise would be stuck in the conversation as context.\n\nOverall, I think the technical implementation was quite easy, and the docs nicely guided me to dive one level deeper than in Jeremy’s original notebook. Learning more about the inner mechanics of how chatting with an LLM actually works, was the best part of this project."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html",
    "title": "Wrapping-up Lesson 3",
    "section": "",
    "text": "Lesson 3 took me a while to rework, because it churned out quite a few interesting projects:\nAlongside these 3 main activities, I also started a blog on my machine learning journey, which is based on Quarto.\nLet me summarize what I have done and learned."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html#gradient-descent",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html#gradient-descent",
    "title": "Wrapping-up Lesson 3",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nThe main focus of lesson 3 for me was/is gradient descent, which I found pretty easy to understand on a high level:\n\nCalculate the predictions and the loss (forward-pass)\nCalculate the derivatives of the parameters (i.e. how does changing the parameters change the loss) (backward-pass)\nUpdate the parameters (via the learning rate)\nDon’t forget to initialize the gradients\nRestart\n\nHowever, in the actual implementation and its simplicity, there is a lot of magic, which I tried to unpack for myself. Working through Jeremy’s notebook “How does a neural net really work?”, I tried to not only think through the concept, but also to visualize it. The result is available as\n\na blog post\na forum post\na Kaggle notebook in which you can easily also play with the visualizations interactively (by copying and running the notebook)\na GitHub notebook\n\nI was excited and honored to read that Ben, a fellow Fast.AI student, created even better visualizations building on my work. I highly recommend playing with it and also checking out his other projects.\nWhile I truly love the Fast.AI content, I also need to mention the great video “The spelled-out intro to neural networks and backpropagation: building micrograd” from Andrej Karpathy, which dives at least one level deeper. If there is one key takeaway from this video, it is this one: “A single gradient tells us the effect changing a parameter has on the loss”. This insight is powerful, and somehow it tends to get lost from my point of view, because you either have a very complex model with many, many parameters so that this is difficult to grasp, or you have a seemingly simple model in which you mix up the slope of the quadratic with a gradient. Implementing the visualization of gradient descent was also about building the intuition of what is actually going on under to hood."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html#the-titanic-competition",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html#the-titanic-competition",
    "title": "Wrapping-up Lesson 3",
    "section": "The Titanic Competition",
    "text": "The Titanic Competition\nInspired by the Excel-based version of the Titanic Competition, I decided to enter the kaggle competition. As with many good project, this resulted in a few other mini-projects:\n\nThe logistics on how a kaggle competition actually works, which included installing kaggle on my local machine. The Live-Coding Sessions of the 2022 Fast.AI course are probably quite underrated (at least looking at the number of views they get on Youtube). I find them a great addition to the official lessons because the tackle side problems like installing kaggle (in Live-Coding Session 7 and the related official topic in the forums) which otherwise would set you back some hours (or more). A big shout-out for these sessions!\nRevisiting matrix multiplication. Apart from the math, this also was about some python basics for me. While the result of implementing matrix multiplication from scratch has probably been done a million times, it still taught me some valuable lessons.\n\nIn the actual Titanic Competition, I did not focus too much on submitting a perfect result, but I rather aimed at re-visiting/solidifying the topic of gradient descent by replicating the actual lesson content. I built for following 2 notebooks\n\nThe first one uses a Fast.AI tabular learner to create a baseline while getting the know the data.\nNext, I re-implemented the Excel-based version from the video in python in this notebook.\n\nWhile my final high score of 77.2% is far away from perfect, I decided to come back to this competition another time, focusing more on the content, not just on gradient descent (like this time)."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html#mnist-the-hello-world-of-computer-vision",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html#mnist-the-hello-world-of-computer-vision",
    "title": "Wrapping-up Lesson 3",
    "section": "MNIST, the ‘Hello World’ of Computer Vision",
    "text": "MNIST, the ‘Hello World’ of Computer Vision\nAs Jeremy points out at the end of lesson 3, this lesson corresponds to chapter 4 of the book. Indeed, it covers very similar topics, but the example used is the light version of the MNIST dataset (which only contains 3s and 7s). Following the recommendation for further research, I implemented a model for the complete MNIST dataset. As predicted: “This was a significant project and took you quite a bit of time to complete! I needed to do some of my own research to figure out how to overcome some obstacles on the way”.\nAfter all the previous activities around gradient descent, the actual mechanics of what needed to be done were not too difficult. Nonetheless, I found the competition to be hard, because of the actual technicalities of the python implementation. Put differently, I think I could have easily written a good specification on how to solve the MNIST competition, but actually doing it yourself is a different thing.\nSeemingly simple tasks like converting the csv-files to images, converting a PIL image to a Fast.AI PIL image, or getting the tensors in the right shape took me some time to implement in python. I am still struggling with python as a language but I am seeing good progress, and the only way to improve is to keep coding."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html#wrapping-up-lesson-3",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html#wrapping-up-lesson-3",
    "title": "Wrapping-up Lesson 3",
    "section": "Wrapping-up Lesson 3",
    "text": "Wrapping-up Lesson 3\nWhile I could improve the results of my projects, both for Titanic and MNIST, it feels like it would be some way over-optimizing. I did not enter the competitions to win, but to learn about gradient descent. Having spent the last 8 weeks with my lesson 3-projects (and allowing myself to get somewhat side-tracked), I feel it is time to move on to the next lesson. I am looking forward to the next challenging projects!"
  },
  {
    "objectID": "posts/2024-01-05-running-ml-on-apple-silicon/index.html",
    "href": "posts/2024-01-05-running-ml-on-apple-silicon/index.html",
    "title": "Running Fast.AI / Huggingface Transformers on Apple Silicon",
    "section": "",
    "text": "In my previous blog post, I described how I setup my Fast.AI development environment on Apple Silicon. In this one, let me share my experience running some notebooks on Apple Silicon. I focus on what needed to be done to adjust the notebooks to Apple Silicon (spoiler alert: it is not difficult) and I also share some performance indications how well Apple Silicon performs.\nI have revisited 2 projects I have worked on before, and I ported them to Apple Silicon:"
  },
  {
    "objectID": "posts/2024-01-05-running-ml-on-apple-silicon/index.html#detecting-apple-silicon",
    "href": "posts/2024-01-05-running-ml-on-apple-silicon/index.html#detecting-apple-silicon",
    "title": "Running Fast.AI / Huggingface Transformers on Apple Silicon",
    "section": "Detecting Apple Silicon",
    "text": "Detecting Apple Silicon\nBefore we jump into these 2 use cases, let’s look at how to detect Apple Silicon. We need to do this because we need to do minor optimizations in both notebooks so that they run at all / with the GPU.\nFor using the the GPUs, there is the so-called Metal Performance Shaders framework, abbreviated MPS. To be able to access MPS, you need to have a pytorch version of at least 1.12.\nStating what probably is obvious but for completeness: NVIDIA acceleration, the other way of GPU acceleration, is abbreviated with CUDA (Compute Unified Device Architecture), and we can see it being mentioned in the code side-by-side frequently.\nSo let’s check the requirements:\n\nimport torch\n\nprint(f\"Pytorch is running on version {torch.__version__}\")\nif torch.backends.mps.is_available():\n    print (\"MPS device found.\")\n    mps_device = torch.device(\"mps\")\nelse:\n    print (\"MPS device not found.\")\n\nPytorch is running on version 2.1.1\nMPS device found.\n\n\nFast.AI also provides a function for checking the available devices called default_device() which returns a torch.device object. Calling it without a parameter, i.e. -1 detects the device. Calling it with True forces it to return a cuda/mps-object, and calling it with False forces it to return a cpu-object.\n\nfrom fastai.vision.all import *\n\ndevice = default_device()\nprint(device)\n\nmps"
  },
  {
    "objectID": "posts/2024-01-05-running-ml-on-apple-silicon/index.html#mnist-on-fast.ai",
    "href": "posts/2024-01-05-running-ml-on-apple-silicon/index.html#mnist-on-fast.ai",
    "title": "Running Fast.AI / Huggingface Transformers on Apple Silicon",
    "section": "MNIST on Fast.AI",
    "text": "MNIST on Fast.AI\nI was looking for a fairly (but not too) compute intensive initial project to try out the performance of my MacBook, and I ended up at the “hello world” of machine learning 😉.\nWorking on MNIST, I had built a baseline-notebook which takes the Fast.AI-version of the MNIST dataset (i.e. using images), creates a model and runs predictions, pretty much with lots of boilerplate.\nThe Apple Silicon version looks very similar: By default, however, Fast.AI ignores the GPU (cuda/mps) everything runs on the cpu. This can easily be fixed by passing the device as the default device (see above) as pointed out by this blog post / this post in the Fast.AI forums when creating the dataloaders:\ndls = mnist1.dataloaders(path, bs=32, device=default_device())\nAfterwards, the training leverages the GPU, and it performed quite well, similar to a free Paperspace instance I had used before. Here are some training times I collected:\n\n\n\nHardware\nTotal Time\nTime 1\nTime 2\nTime 3\nTime 4\n\n\n\n\nM2 Max (Juypter)\n06:33\n01:23\n01:45\n01:42\n01:43\n\n\nM2 Max (Juypter-squish)\n06:33\n01:23\n01:45\n01:42\n01:43\n\n\nM2 Max (VSCode)\n07:16\n01:34\n01:54\n01:54\n01:54\n\n\nM3 Pro (Juypter)\n22:42\n07:19\n04:28\n04:47\n06:08\n\n\nSurface 4 Pro\n40:17\n04:19\n10:29\n13:02\n12:27\n\n\nSurface 4 Pro (squish)\n4:56:50\n1:02:35\n1:18:50\n1:15:45\n1:19:40\n\n\nPaperspace (2002)\n05:56\n01:08\n01:36\n01:36\n01:36\n\n\n\nNotes regarding the table:\n\nThe link on the total time leads to a screenshots showing the detailed training times.\nIf not incited otherwise, the training was done in Jupyter Labs.\n\nTakeaways:\n\nI was surprised to see that there was a difference in running the code in Jupyter Lab vs. in VS Code. For more complex tasks, Jupyter Labs seems to be the runtime environment of choice (but for editing I like VS Code better).\nThe M2 Max performs comparably to a free Paperspace GPU. Honestly, I am not super-impressed with this result, because 6 years in hardware development (between the Surface and the M2 Max) should have yielded much better results, so I suspect there are some inefficiencies somewhere.\nM3 Pro is even worse. The result is so bad, that it is hard to believe, even if M3 Pro is the lesser processor compared to M2 Max.\nAdding more computation by adding in the squish transformation highlights to true relation of performance, M2 Max hardly slows down, but my old Surface Pro needed almost 5 hours for the task, which underlines the suspicion that there are some inefficiencies hidden when the overall computational load is low."
  },
  {
    "objectID": "posts/2024-01-05-running-ml-on-apple-silicon/index.html#natural-language-processing-with-disaster-tweets-on-huggingface-transformers",
    "href": "posts/2024-01-05-running-ml-on-apple-silicon/index.html#natural-language-processing-with-disaster-tweets-on-huggingface-transformers",
    "title": "Running Fast.AI / Huggingface Transformers on Apple Silicon",
    "section": "Natural Language Processing with Disaster Tweets on Huggingface transformers",
    "text": "Natural Language Processing with Disaster Tweets on Huggingface transformers\nThe second use case was more compute intensive, running the model microsoft/deberta-v3-large. As it turned out, I needed install/upgrade some packages and make 2 adjustments to my notebook to be able to run it on Apple Silicon.\nThe first step was easy: stepping through the notebook, some packages were missing/needed upgrades:\npip install protobuf   \npip install evaluate\npip install accelerate -U\nRegarding code, Apple Silicon (i.e. MPS - Metal Performance Shaders) apparently does not support FP16 (16-bit floating-point), therefore, the parameter fp16 needs to be passed as False to the TrainingArguments. For generic detection of the parameter:\n\nimport torch\n\nfp16_available = True\n\nif torch.backends.mps.is_available():\n    fp16_available = False\n\nprint(f\"FP16 is available: {fp16_available}\")\n\nFP16 is available: False\n\n\nAdditionally, I needed to adjust the memory management, because the model ran out of memory: RuntimeError: MPS backend out of memory (MPS allocated: 7.18 GB, other allocations: 28.82 GB, max allowed: 36.27 GB). Tried to allocate 500.39 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\nPYTORCH_MPS_HIGH_WATERMARK_RATIO is an environment variable related to PyTorch’s memory management when using the MPS. It sets the ratio of the total GPU memory that PyTorch is allowed to allocate when using MPS. The ratio is expressed as a decimal fraction of the total available GPU memory. For example, 0.8 means that PyTorch is allowed to use 80% of the GPU memory. By setting PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 PyTorch does not have an upper limit on memory allocations for MPS operations. This means PyTorch can potentially use all available GPU memory for its computations.\nSetting this parameter needs to be done before running any PyTorch code, so I put it into the very first cells of my notebook. Let’s check the parameter first:\n\nimport os\n\nhigh_watermark_ratio = os.getenv('PYTORCH_MPS_HIGH_WATERMARK_RATIO')\n\nif high_watermark_ratio is not None:\n    print(f\"PYTORCH_MPS_HIGH_WATERMARK_RATIO is set to: {high_watermark_ratio}\")\nelse:\n    print(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO is not set (using default behavior).\")\n\nPYTORCH_MPS_HIGH_WATERMARK_RATIO is not set (using default behavior).\n\n\nSo let’s go ahead and set the parameter:\n\n\nos.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n\nAs a result, the notebook ran to the end! It took about 1 hour (compared to about 20 minutes on Huggingface), and it resulted in heavy memory usage of up to 60GB (even though I have only 32GB of physical memory):\n\n\n\nMemory Usage\n\n\nI am not quite sure how to make of this heavy usage, especially since only about 5GB were swapped, so it does not really add up to me… Nonetheless, it worked!\nBefore closing: Yes, there would be other ways to try to optimize the memory usage (like gradient accumulation), but the goal here was to stay as close to the original code as possible."
  },
  {
    "objectID": "posts/2024-01-05-running-ml-on-apple-silicon/index.html#overall-conclusion",
    "href": "posts/2024-01-05-running-ml-on-apple-silicon/index.html#overall-conclusion",
    "title": "Running Fast.AI / Huggingface Transformers on Apple Silicon",
    "section": "Overall conclusion",
    "text": "Overall conclusion\nIt is always nice to be able to run things on out own machine. But overall, the performance is not as good as even the free versions of Paperspace or Kaggle.\nNonetheless, I would attribute some convenience to being able to work locally in terms of setup, being in a known environment, being able to work traveling. Additionally, more capable laptops are becoming ubiquitous than ever, so if you have one, why not use it? Especially for playing with smaller models, it makes lots of sense to me, and for bigger models, you can always easily shift to an online service.\nAgain, there are tradeoffs and it depends on personal taste. What’s your take on this? Do you prefer to work on your laptop or your preferred online environment?"
  },
  {
    "objectID": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html",
    "href": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html",
    "title": "Installing Fast.AI on Apple Silicon",
    "section": "",
    "text": "I recently moved from Windows to Mac. One of the things which needed attention in making that shift was to re-install everything around Fast.AI.\nThis blog post is mainly a personal documentation going through Live Coding Session 1 and Live Coding Session 2 (again), but I hope this might also be useful to others."
  },
  {
    "objectID": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html#starting-with-a-blank-mac",
    "href": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html#starting-with-a-blank-mac",
    "title": "Installing Fast.AI on Apple Silicon",
    "section": "Starting with a blank Mac",
    "text": "Starting with a blank Mac\nBefore we can talk about all the cool machine learning stuff, we need to start with some basics like installing some basic software on the Mac. So if you are already familiar with MacOS you might want to skip this part.\n\nThe Terminal instead of Ubuntu\nIt may sound trivial, but let’s state the (maybe no so) obvious: Instead of the Windows Subsystem for Linux (WSL), you use the Terminal App on a Mac. Navigation works the same, but there are some fine details to note.\nWhen I followed the tutorials of the Live Coding Sessions for Fast.AI (Link), I needed to use wget, but it did not exist on my Mac, because it was not installed. No problem, but how do you install software via the Terminal on a Mac?\n\n\nHomebrew\nWhen it comes to installing software on a Mac via the terminal, it seems that Homebrew is the de-facto standard. To install it, run the following command in the terminal:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nAfterwards, Homebrew needs to be added to the PATH environment variable: (exchange your username)\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; /Users/chrwittm/.zprofile\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nIf everything was successful, you should be able to execute\nbrew --version\nAdditionally, you can now install software like wget like this:\nbrew install wget"
  },
  {
    "objectID": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html#installing-miniforge",
    "href": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html#installing-miniforge",
    "title": "Installing Fast.AI on Apple Silicon",
    "section": "Installing Miniforge",
    "text": "Installing Miniforge\nWith a few basics sorted out, we can now install python. The recommendation in the Live Coding Session is the use Mambaforge, but in the meantime, it has been merged with Miniforge, therefore, this is want I installed:\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh\nbash Miniforge3-MacOSX-arm64.sh\nAfter restarting the terminal, you can see the (base) prefix to the prompt, signalling that python is available.\nAdditionally, you can now execute which python to see the path where Python is installed."
  },
  {
    "objectID": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html#installing-additional-python-packages",
    "href": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html#installing-additional-python-packages",
    "title": "Installing Fast.AI on Apple Silicon",
    "section": "Installing additional Python packages",
    "text": "Installing additional Python packages\nThe recommended way to install packages is via mamba, as per the tutorial, execute\nmamba install ipython\n\nInstalling Pytorch\nNext we install pytorch with the following parameters:\n\n\n\nPytorch Installation Paramters\n\n\nmamba install pytorch::pytorch torchvision torchaudio -c pytorch\n\n\nInstalling Fast.AI\nOnce that is done, we can install the Fast.AI and additional packages:\nmamba install jupyterlab\nmamba install ipywidgets\nmamba install -c fastai fastai\nmamba install -c fastchan fastbook\nmamba install -c fastchan sentencepiece\n\n\nInstalling Quarto (for writing this blog)\nTo be able to update this blog, I needed to re-install the related packaged. More details are available in this blog post:\nmamba install -c fastai -y nbdev\nsoftwareupdate --install-rosetta\nnbdev_install_quarto"
  },
  {
    "objectID": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html#setup-of-git",
    "href": "posts/2023-12-13-installing-fast-ai-on-apple-silicon/index.html#setup-of-git",
    "title": "Installing Fast.AI on Apple Silicon",
    "section": "Setup of Git",
    "text": "Setup of Git\nGit was already installed on my machine (brew install git) which I did as part of the installation of VS Code, so here we only cover the setup done in the terminal:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.mail@service.com\"\n\nssh-keygen\ncat ~/.ssh/id_rsa.pub\nNow you add the key to your Github keys. Afterwards you should be able to login\nssh git@github.com\nAdding Git Large File Storage (LFS) to be able to push large files to GitHub (like .pkl-files):\nbrew install git-lfs\nThis concludes the installation of Fast.AI on an Apple Silicon Machine. In a follow-up blog post I will talk about my experience on running Fast.AI on Apple Silicon."
  },
  {
    "objectID": "posts/2022-11-27-rebuilding-quarto-blog/index.html",
    "href": "posts/2022-11-27-rebuilding-quarto-blog/index.html",
    "title": "When disaster strikes: Re-building a Quarto Blog",
    "section": "",
    "text": "For the last 2 months I have been a proud writer of this blog, until yesterday disaster struck: Upon publishing of my MNIST-blog post via the usual quarto publish gh-pages, I received the following error message 😨:\nThe publishing was not completed, and my blog only showed a naked header, but no posts any more, essentially everything was gone 😱 (at least online)\nTrying to google a quick fix did not reveal any real result. Due to lack of time, I had to (officially) stop for the day, but back in my mind, this was really nagging me…"
  },
  {
    "objectID": "posts/2022-11-27-rebuilding-quarto-blog/index.html#what-is-needed-to-re-build-a-quarto-blog",
    "href": "posts/2022-11-27-rebuilding-quarto-blog/index.html#what-is-needed-to-re-build-a-quarto-blog",
    "title": "When disaster strikes: Re-building a Quarto Blog",
    "section": "What is needed to Re-build a Quarto Blog?",
    "text": "What is needed to Re-build a Quarto Blog?\nAs I kept thinking about this, the error message clearly pointed to something on my local machine. Additionally, I previously posted on how to avoid disaster, so what would be the best way to re-build everything?\nBut let’s think through the matter: What do you actually need to re-build a Quarto blog? You only need your posts-directory and a few other files (the ones mentioned here).\nThis is important, so let me re-phrase this: On your GitHub repo, there are (should be) 2 versions of your blog:\n\nIn the main-branch, you store your source-files:\n\nThe Jupyter notebooks, markdown files, some pictures used within your posts\nThe configuration files: Some .yml-, .qmd- and .css-files\n\nThe gh-pages branch gets generated to contain the rendered versions of your posts: When you check out your _site directory, it contains a file structure similar to the posts-directory in your main-branch, but the _site directory deals in html-, xml-, and json-files.\n\nTherefore, to re-build your site, as far as I understand it, you only need the content of your main branch, and the content of the gh-pages gets generated once you run quarto publish gh-pages."
  },
  {
    "objectID": "posts/2022-11-27-rebuilding-quarto-blog/index.html#re-building-my-quarto-blog",
    "href": "posts/2022-11-27-rebuilding-quarto-blog/index.html#re-building-my-quarto-blog",
    "title": "When disaster strikes: Re-building a Quarto Blog",
    "section": "Re-building my Quarto Blog",
    "text": "Re-building my Quarto Blog\nWith the above in mind, here is what I did:\n\nI moved my local copy of the blog’s repo’s main branch to my temp folder (the milder version of deleting it)\nI re-cloned the repo (the main branch): git clone git@github.com:chrwittm/chrwittm.github.io.git\nI re-published the blog: quarto publish gh-pages\n\nAnd violà: My blog was back online. 😃"
  },
  {
    "objectID": "posts/2022-11-27-rebuilding-quarto-blog/index.html#conclusion",
    "href": "posts/2022-11-27-rebuilding-quarto-blog/index.html#conclusion",
    "title": "When disaster strikes: Re-building a Quarto Blog",
    "section": "Conclusion",
    "text": "Conclusion\nFirst of all: With these kind of seeming disasters: Sit back and relax: Is this a big deal? Well my blog was down, who cared? Probably I cared the most about it - my ego 😉.\nDon’t panic: Think through a problem: What could be the root cause, from what angle can you approach the problem?\nI think, the solution is actually really nice. The setup with the 2 branches has some nice redundancy built-in, and without having tried it before, the disaster recovery performed very well.\nThe solution also reminded me of the fast-setup approach, which Jeremy discussed in the Live-Coding session 1: You should spend time using your tools, not configuring them. Quarto, at least to me, nicely proved that point that even if something goes wrong, you can quickly recover.\nHappy blogging!"
  },
  {
    "objectID": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html",
    "href": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html",
    "title": "Discovering Disaster Tweet Dataset Limitations",
    "section": "",
    "text": "What started out as a simple exercise to visualize model performance using a confusion matrix revealed that the training set contains lots of incorrectly labeled tweets, and that my trained model actually performed better than the score suggests.\n\n\nAfter I had published my first blog post on “Natural Language Processing with Disaster Tweets”), I realized that I had forgotten one element I planned to incorporate into the notebook: I wanted to visualize the model performance using a confusion matrix. Since the implementation was pretty straightforward, the subsequent data analysis will be the real content of this blog post."
  },
  {
    "objectID": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#implementing-the-confusion-matrix",
    "href": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#implementing-the-confusion-matrix",
    "title": "Discovering Disaster Tweet Dataset Limitations",
    "section": "Implementing the Confusion Matrix",
    "text": "Implementing the Confusion Matrix\nTo create the confusion matrix, we need the ground truth from the training data and the predictions from the model on the training data:\n\n# ground truth from the training data\ntrain_true = [int(x) for x in tok_ds['labels']]\n\n# model predictions on the training data\ntrain_preds = trainer.predict(tok_ds).predictions.astype(float)\ntrain_preds = [ 1 if element &gt; 0.6 else 0 for element in train_preds.squeeze()]\n\nLeveraging scikit-learn, here is how I created the confusion matrix\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ncm = confusion_matrix(train_true, train_preds, labels=[0, 1])\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot()\n\nplt.show()\n\n\nThe confusion matrix above displays the best training result I could get using all the methods in my previous blog post/notebook: Training on the model microsoft/deberta-v3-large with 2 epochs at batch size 4 and afterwards another epoch on the full training set. The model nicely classifies many tweets correctly, but there are still 196 false-positives and 553 false negatives.\nThis results in a surprisingly high F1-score of 0.87691 - a lot higher than any of the F1 scores in my submissions.\nSimilar to the approach taken in lesson 2 (where we used the first trained model to find the picture which had the top losses), I reviewed at the incorrectly labeled tweets and found surprising results."
  },
  {
    "objectID": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#data-analysis-discovering-mislabeled-tweets",
    "href": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#data-analysis-discovering-mislabeled-tweets",
    "title": "Discovering Disaster Tweet Dataset Limitations",
    "section": "Data Analysis / Discovering Mislabeled Tweets",
    "text": "Data Analysis / Discovering Mislabeled Tweets\nInspecting the false positives and the true positives revealed the following:\n\nThe false positives (the model classified a tweet as disaster, but there was none), were indeed mostly incorrect predictions made by the model - even though there is quite some room for interpretation.\nThe false negatives (the model classified a tweet as non-disaster, but there was one), however, revealed a significant number of mislabeled tweets.\n\nThe following tweets for sure are no disaster tweets, nonetheless they are labeled as disasters in the training data:\n\n\n\n\n\n\n\n\n\nid\nTweet\nlabel\npred\n\n\n\n\n443\nShort Reading Apocalypse 21:1023 In the spirit the angel took me to the top of an enormous high mountain and…\n1\n0\n\n\n794\nChevrolet : Avalanche LT 2011 lt used 5.3 l v 8 16 v automatic 4 wd pickup truck premium b_\n1\n0\n\n\n1051\nI waited 2.5 hours to get a cab my feet are bleeding\n1\n0\n\n\n1239\npeople with a #tattoo out there.. Are u allowed to donate blood and receive blood as well or not?\n1\n0\n\n\n\nWhile all these tweets contain words which could potentially indicate a disaster (apocalypse, avalanche, bleeding, blood)\n\nthe first tweet is a quote from the bible,\nthe second one is related to a car,\nthe third one undoubtedly feels unpleasant,\nand fourth one is simply a question on donating blood.\n\nAs it turned out, these 4 examples are not the only ones. After having read through the IDs up to 1000 in the false negatives, I had found about 80 mis-labeled tweets."
  },
  {
    "objectID": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#re-training-a-model-using-corrected-labels",
    "href": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#re-training-a-model-using-corrected-labels",
    "title": "Discovering Disaster Tweet Dataset Limitations",
    "section": "Re-Training a model using corrected labels",
    "text": "Re-Training a model using corrected labels\nEven though re-labeling tweets is a tedious work, in this notebook, which is a copy of my previous one, I have added a new section which re-labels tweets based on the results in the first notebook.\nMy clear expectation was that model performance would go up because of the increased quality of the updated training set. The opposite, however, was the case: Model performance dropped from 84.6% to 84% 😮 - what was going on?\nThe only explanation I have for this result is that both the training and the test data systematically contain mislabeled tweets. Reasons could be\n\nmisinterpretation of the tweets, some tweets are obviously ambiguous\nsimple error in the monotonous task of labeling data\nusing an outdated algorithm for labeling tweets - just speculating 😉\n\nThe current latest version of the notebook (V8) further supports this claim. It has scored a little bit better (84.3%). This increase was caused by mislabeling a bunch of tweets: There are quite a few tweets in the dataset which follow this pattern: “#FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps”. This tweet simply informs about a new FedEx policy, thus it is no disaster tweet from my point of view. But in the dataset 66% of tweets with this content are labeled as disasters. By accepting the majority rule, the score increased, even though the label is incorrect in my opinion.\nAll of this tells us that there is a problem in the dataset of the Kaggle competition, some of it may be subjective, other tweets are objectively mislabeled. Therefore, if you are working on the Kaggle competition for disaster tweets and the improvements you make to your model do not materialize in a better result, it might also be the dataset’s fault, not yours. Consider this: If your model would classify every tweet correctly in the test set, you would not score 100% because of the mislabeled tweets in the test set.\nBut I think we can gain some deeper insights than the simple observation that the dataset contains some issues."
  },
  {
    "objectID": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#conclusion",
    "href": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#conclusion",
    "title": "Discovering Disaster Tweet Dataset Limitations",
    "section": "Conclusion",
    "text": "Conclusion\nWhen working in machine learning, we sometimes tend to focus a lot on the technology aspects, trying to tune hyperparameters, using bigger or more advanced models etc. In this case, the ground truth is the limiting factor to what the model can achieve. The pre-trained model has actually outperformed the data set by clearly pointing out mislabeled data. This shows that the pre-trained model understands quite a bit about language, not only the grammar, but also about the content. This is not super-surprising when you think about the task for the model. A language model is basically trained to predict the next word for a prompt, and we have re-trained it (the last layer or a bit more) to classify tweets. So it is not just trained on the classification task, but the model still understands quite a lot about the content about the tweets.\nThe (training) data is maybe the most important asset in machine learning, and it can come with quite a few problems: Not only can it be biased, difficult to obtain, hard to parse, it can also be simply wrong! So the true lesson is not to blindly trust the data, but thorough inspection of the data is always called for. By using faulty data, our model has become biased: It has learned from the mislabeled tweets, and I suspect it has become too sensitive when detecting disasters. When non-disaster tweets are classified as disasters in the training data, the model calls out a disaster when there is none - similar to if you called an ambulance for a slight cough. The pre-training, however, still seems to contain enough “common sense” that the model can still call out mislabeled tweets.\nTherefore, the F1-score of the submission does not represent the true model performance: The score should be higher. My subjective estimate is that at least 30% of the false negatives should be true positives. This would lift the F1-score by about 2.5 percentage points.\nWhat do you think? Did I assign too much credit to the pre-trained model, for example, by crediting it with the capability to do a bit of common-sense reasoning? Does the data quality of the competition define the ceiling of the score that you can achieve, or do you see ways to navigate around this obstacle?"
  },
  {
    "objectID": "posts/2023-03-05-titanic-with-chatgpt/index.html",
    "href": "posts/2023-03-05-titanic-with-chatgpt/index.html",
    "title": "Titanic with ChatGPT",
    "section": "",
    "text": "Reworking Lesson 5, I returned to the Titanic Competition to learn more Fast.AI-concepts. Additionally, I explored how ChatGPT could increase my productivity.\nAs a result I created the following 2 notebooks\nIn this blog post, I do not was to talk about the Titanic Competition too much, but rather about the co-working experience with ChatGPT and what I learned about how to interact with it. (Prompts will be indicated by the chat emoji 💬).\nThe main findings are that you need to be precise in your prompting, but there is a limit to what you can prompt for. When it comes to interpreting the result, you need to guide the conversation with ChatGPT. While it can save a lot of time, it cannot do all the work for you."
  },
  {
    "objectID": "posts/2023-03-05-titanic-with-chatgpt/index.html#the-hottest-new-programming-language-is-english",
    "href": "posts/2023-03-05-titanic-with-chatgpt/index.html#the-hottest-new-programming-language-is-english",
    "title": "Titanic with ChatGPT",
    "section": "The hottest new programming language is English",
    "text": "The hottest new programming language is English\nAs Andrej Karpathy tweeted: “The hottest new programming language is English”. Indeed, especially in the EDA-notebook (GitHub/Kaggle), I actually used English as a programming language, and ChatGPT was merely translating to python. Additionally, working with ChatGPT really felt like a conversation: I asked for some visualizations, ChatGPT created the code, I ran the code, and based on the result, I created the next prompt, either asking ChatGPT to fix an error, or to build a new visualization.\nSomehow this felt like “real life” (whatever that is 😉), in which I am a software consultant: A typical workflow in that business is writing a specification document, giving it to a developer who writes the code, and getting the result after some time. With ChatGPT, however, the iteration cycles are 1 to 2 orders of magnitude faster (quasi instant), and iteration speed is essential. Even a super-skilled human developer would be considerably slower, because ChatGPT is just “typing so much faster”. Without being overly enthusiastic, not all tasks are equally well-suited for this workflow with ChatGPT (as of today). The more complex the task are, the more “random” the result will get. With EDA (GitHub/Kaggle), this interaction felt productive. For feature engineering (GitHub/Kaggle), I had to break down the task for ChatGPT into smaller chunks, and for model creation and fine tuning it, I took over in the end.\nThe key to getting good results are well-stated prompts. It is really important to be explicit in your prompts: State what you really want, ChatGPT is not really great at reading between the lines, or extrapolating what might be inferred. As you go through the notebooks, you will see me make that mistake. But than again, by talking to ChatGPT, you can refine the prompts along the way. Or you can simply build upon the results. If ChatGPT produced a function/visualization you do not like, you can simply ask for a new version.\nBut why are some coding tasks more difficult than others? The big limitation of ChatGPT is that it can neither run/debug the code or see the result the code produces, also it does not know the real data. All it can do is read the code and respond (with variations). Therefore, it is understandable that it makes mistakes or produces code which is syntactically incorrect. The best feature from ChatGPT on the other side is that you can talk though the bugs and resolve them quickly (“💬: There is an error: AssertionError: nan values in Fare but not in setup training set - can you please correct the code.”). This works surprisingly well, especially when it is related to the syntax of the code. Tweaking the output of working code, however, is a bigger challenge for ChatGPT because it can neither run/debug the code, read the data, or see the result the code produces. As you can see, you need to remain the driver of the conversion. But done correctly, ChatGPT can be a great “productivity buddy” where ChatGPT can to 80-90% of the work (i.e. write the code), and you take the role of the instructor / interpreter / reviewer."
  },
  {
    "objectID": "posts/2023-03-05-titanic-with-chatgpt/index.html#you-need-to-be-the-eyes-and-ears-of-chatgpt",
    "href": "posts/2023-03-05-titanic-with-chatgpt/index.html#you-need-to-be-the-eyes-and-ears-of-chatgpt",
    "title": "Titanic with ChatGPT",
    "section": "You need to be the Eyes and Ears of ChatGPT",
    "text": "You need to be the Eyes and Ears of ChatGPT\nFor some tasks, however well prompted, ChatGPT struggles to produce great results when it needs the context of the real data, or if it should build on results the code it generated revealed. This is looping back to the same limitations I mentioned before. Let me illustrate what I mean via an example: In feature engineering, ChatGPT had no trouble writing a function to create new feature “Age Group” (“💬: Create a python functions to create the feature”Age Group”“). This prompt would create the following function (in the context of a discussion of the Titanic dataset):\n\ndef get_age_group(age):\n    \"\"\"\n    Bins the age column into different age groups\n    \"\"\"\n    if age &lt; 18:\n        return 'Child'\n    elif age &lt; 65:\n        return 'Adult'\n    else:\n        return 'Senior'\n\nAt first glance, this is reasonable, but it does not take into account, the specific content of our dataset. You could argue that the prompt was not very specific (which was intentional). The point I am trying to make is that ChatGPT creates bins which are logical, but they are not connected to the Titanic dataset. This is the case even though in EDA (GitHub/Kaggle), jointly with ChatGPT, I created the following chart which illustrates the relation between age and chance of survival:\n\nWe can see that small children had a much higher chance of survival (because they probably received the most help and preferential treatment) than any other age group, and people above the age of 60 had a significant lower chance of survival.\nChatGPT cannot see this result even though it generated the code to create the visualization. Again, this is because it can neither run the code nor can it inspect the resulting graph, this remains a human task. But for the refinement, ChatGPT can assist:\n\n💬: Rewrite the get_age_group function to generate the following bins [0, 6, 12, 18, 30, 60, 100], name the bins appropriately\n\nHere is the resulting code:\n\ndef get_age_group(age):\n    \"\"\"\n    Returns the age group based on the given age.\n    \"\"\"\n    if pd.isnull(age):\n        return 'Unknown'\n    elif age &lt;= 6:\n        return 'Infant'\n    elif age &lt;= 12:\n        return 'Child'\n    elif age &lt;= 18:\n        return 'Teenager'\n    elif age &lt;= 30:\n        return 'Young Adult'\n    elif age &lt;= 60:\n        return 'Adult'\n    else:\n        return 'Senior'\n\nMy notebooks contain more examples, but what we can take away is that you need to be the eyes and ears of ChatGPT. ChatGPT is great at generating a lot of code, but you need to interpret the results and guide the way from prompt to prompt. So far, ChatGPT cannot do this. AI cannot take over - yet."
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html",
    "href": "posts/2022-10-28-matrix-multiplication/index.html",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "Since matrix multiplication is a big thing for deep learning and visualizations like http://matrixmultiplication.xyz/ were a bit to fast for me to properly re-understand what I learned in highschool, I decided dive in more systematically without falling into the trap of learning lots of math before continuing with deep learning. This will be short and sweet:\nThis has been done a million times before already, but nonetheless, let me explain what I learned along the way."
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#takeaways-from-khan-academy",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#takeaways-from-khan-academy",
    "title": "Matrix Multiplication",
    "section": "Takeaways from Khan Academy",
    "text": "Takeaways from Khan Academy\nOne thing I was struggling with was to intuit the dimensions of the target matrix.\nLet’s assume two matrixes: \\(A = (m \\times n)\\) and \\(B = (n \\times k)\\)\nThis picture from Khan Academy sums it all up for me:\n Illustration by Khan Academy CC BY-NC-SA 3.0 US  Note: All Khan Academy content is available for free at (www.khanacademy.org)“\nTherefore, a matrix multiplication is defined if the number of columns of matrix \\(A\\) matches the number of rows of matrix \\(B\\).\nThe resulting matrix \\(C = A \\times B\\) has the same number of rows as matrix \\(A\\) and the same number of columns as matrix \\(B\\)."
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#implementing-matrix-multiplication-in-python-from-scratch",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#implementing-matrix-multiplication-in-python-from-scratch",
    "title": "Matrix Multiplication",
    "section": "Implementing Matrix Multiplication in Python from scratch",
    "text": "Implementing Matrix Multiplication in Python from scratch\nOnce that was done, I decided to implement matrix multiplication in python. I found this tutorial which provided me with the task and some guidance along the way, especially on a few things in python.\nAs a starting point, here are 2 matrixes that we want to multiply (example from tutorial sightly adjusted):\n\nimport numpy as np\nnp.random.seed(27)\nA = np.random.randint(1,10,size = (4,3))\nB = np.random.randint(1,10,size = (3,2))\nprint(f\"Matrix A:\\n {A}\\n\")\nprint(f\"Matrix B:\\n {B}\\n\")\n\nMatrix A:\n [[4 9 9]\n [9 1 6]\n [9 2 3]\n [2 2 5]]\n\nMatrix B:\n [[7 4]\n [4 1]\n [6 4]]\n\n\n\nThis is the final result, we want to re-implement from scratch:\n\nA@B\n\narray([[118,  61],\n       [103,  61],\n       [ 89,  50],\n       [ 52,  30]])\n\n\n\nIndexing in Python\nMaybe this is too obvious for many, but I find it worth noting, that the sequence in which python addresses arrays (or tensors) is first by row, than by column. What do I mean by saying that?\nWhen you want to index into an array, you do this by array_name[row:column], for example A[1,2] return 6, it is the second line (which is index 1 when starting to count at 0), and the third column (which is index 1 when starting to count at 0):\n\nA[1,2]\n\n6\n\n\nIs there a way to not only remember this, but to also understand this? Yes, I think so: The most basic array (tensor) is a list (rank 1 tensor), which we can think of as one row of numbers. Therefore, the first index represents the row. You can think of a 2-dimensional array (a rank 2 tensor) as adding the columns to a row of numbers (by adding more rows), therefore the second index represents the columns. Hence to access an element in a 2D-array (rank-2 tensor), this is done by array_name[row:column].\nWhy do we think about indexing? First, to determine if a matrix multiplication is defined, we need to find the dimensions of the matrixes, and later on we need to access the matrix content for the calculation.\nTo access a complete row or column, we use:\n\nFor a row: array_name[row, : ] or the short form array_name[row]\nFor a column: array_name[ : ,column]\n\nThis means: We access a specific row or column by index, and from the other dimension, we access all elements. For example:\n\n# accessing the first row of matrix A\n\nA[0] #same as A[0,:]\n\narray([4, 9, 9])\n\n\n\n# accessing the first column of matrix B\n\nB[:,0]\n\narray([7, 4, 6])"
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#constructing-a-target-matrix-of-zeros",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#constructing-a-target-matrix-of-zeros",
    "title": "Matrix Multiplication",
    "section": "Constructing a target matrix of zeros",
    "text": "Constructing a target matrix of zeros\nThe \\(C\\) target matrix has the same number of rows as A and the same number of columns of B, so in our example that is a matrix with 4 rows and 2 columns:\n\nnp.zeros((4, 2), dtype = int)\n\narray([[0, 0],\n       [0, 0],\n       [0, 0],\n       [0, 0]])\n\n\nThe number of rows is the length of a column, therefore, to get the number of rows of matrix A, we can write:\n\nlen(A[:,0]) #i.e. the length of the first column\n\n4\n\n\nSimilarly, the number of elements in a row if the number of columns, Therefore, the number of columns of B is:\n\nlen(B[0]) #the number of entries in the first row\n\n2\n\n\nWhile to above is correct, there is a more elegant way to write this. Each array (tensor) has an attribute .shape which tells us how many rows and columns an array has (notice the sequence in the tuple: (row,column)):\n\nprint(A.shape)\nprint(B.shape)\n\n(4, 3)\n(3, 2)\n\n\nTherefore, we can re-write:\n\nprint(f'Number of rows in matrix A: {A.shape[0]}') \nprint(f'Number of columns in matrix B: {B.shape[1]}')\n\nNumber of rows in matrix A: 4\nNumber of columns in matrix B: 2\n\n\nNow we can generically construct the target matrix \\(C\\):\n\nC = np.zeros((A.shape[0], B.shape[1]), dtype = int)\nC.shape\n\n(4, 2)"
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#exercise-implement-matrix-multiplication-with-numpy-arrays",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#exercise-implement-matrix-multiplication-with-numpy-arrays",
    "title": "Matrix Multiplication",
    "section": "Exercise: Implement Matrix Multiplication with numpy arrays",
    "text": "Exercise: Implement Matrix Multiplication with numpy arrays\nImplement a function multiply_matrix(A,B) which does the following:\n\nAccept two matrices, A and B, as inputs.\nCheck if matrix multiplication between A and B is valid, if not raise an error.\nIf valid, multiply the two matrices A and B, and return the product matrix C.\n\n\ndef multiply_matrix(A,B):\n    \n    if A.shape[1] != B.shape[0]:\n        raise ValueError('Number of columns of A and number of rows of B do not match')\n    \n    C = np.zeros((A.shape[0], B.shape[1]), dtype=int)\n\n    for row in range(C.shape[0]):\n        for column in range(C.shape[1]):\n            for step in range(A.shape[1]):\n                C[row, column] += A[row, step] * B[step, column]\n    \n    return C\n\nC1 = multiply_matrix(A, B)\nC1\n\narray([[118,  61],\n       [103,  61],\n       [ 89,  50],\n       [ 52,  30]])\n\n\n\nC2 = A@B\nassert np.array_equal(C1, C2)"
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#exercise-implement-matrix-multiplication-with-tensors",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#exercise-implement-matrix-multiplication-with-tensors",
    "title": "Matrix Multiplication",
    "section": "Exercise: Implement Matrix Multiplication with tensors",
    "text": "Exercise: Implement Matrix Multiplication with tensors\nJust for the fun of it, let’s re-implement the same with pytorch tensors. It turns out it same, same, but a little different:\n\nimport torch\n\ntorch.manual_seed(27) #https://pytorch.org/docs/stable/notes/randomness.html\nX = torch.randint(1,10,size = (4,3)) #https://pytorch.org/docs/stable/generated/torch.randint.html\nY = torch.randint(1,10,size = (3,2))\nprint(f\"Matrix X:\\n {X}\\n\")\nprint(f\"Matrix Y:\\n {Y}\\n\")\n\nMatrix X:\n tensor([[1, 1, 5],\n        [8, 8, 6],\n        [1, 7, 1],\n        [4, 4, 1]])\n\nMatrix Y:\n tensor([[7, 6],\n        [3, 7],\n        [9, 5]])\n\n\n\n\nZ = torch.zeros((4, 2), dtype = int)\nZ\n\ntensor([[0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0]])\n\n\n\nZ.shape\n\ntorch.Size([4, 2])\n\n\n\ndef multiply_matrix_torch(A,B):\n    \n    if A.shape[1] != B.shape[0]:\n        raise ValueError('Number of columns of A and number of rows of B do not match')\n    \n    C = torch.zeros((A.shape[0], B.shape[1]), dtype=int)\n\n    for row in range(C.shape[0]):\n        for column in range(C.shape[1]):\n            for step in range(A.shape[1]):\n                C[row, column] += A[row, step] * B[step, column]\n    \n    return C\n\nZ1 = multiply_matrix_torch(X, Y)\nZ1\n\ntensor([[ 55,  38],\n        [134, 134],\n        [ 37,  60],\n        [ 49,  57]])\n\n\n\nZ2 = X@Y\nassert torch.equal(Z1, Z2) == True\n\nThat concludes the “exploration” of matrix multiplication, I learned a lot along the way :)."
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html",
    "href": "posts/2022-11-05-kaggle-titanic/index.html",
    "title": "My First Kaggle Competition: Titanic",
    "section": "",
    "text": "For more practical experience with gradient descent, I decided to participate in the Titanic Competition. Here is how I did it and what I learned.\nI took the following approach:"
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html#installing-kaggle",
    "href": "posts/2022-11-05-kaggle-titanic/index.html#installing-kaggle",
    "title": "My First Kaggle Competition: Titanic",
    "section": "Installing Kaggle",
    "text": "Installing Kaggle\nGetting ready for the Kaggle competition requires registering for the competition (a few clicks on the kaggle website), and installing kaggle on your local machine. The following is based on the Live-Coding Session 7 and the related official topic in the forums.\nThe first step is to install kaggle:\npip install --user kaggle\nAs a result, the following warning is displayed: The script kaggle is installed in '/home/&lt;your user&gt;/.local/bin' which is not on PATH. This means that the you need to add the path to the PATH-variable. This is done by adding the following line to the .bashrc-file and restarting the terminal:\nPATH=~/.local/bin:$PATH\n\nNote: To display the current PATH-variable use: echo $PATH\n\nAs a result, typing the kaggle-command on the command line works, but the next error shows up (as expected): OSError: Could not find kaggle.json. Make sure it's located in /home/chrwittm/.kaggle. Or use the environment method.\nThis means that you cannot authorize against the kaggle platform. To solve this, download your personal kaggle.json On the kaggle website, navigate to: “Account” and click on “Create New API Token”. As a result, the kaggle.json is downloaded.\nCopy the kaggle.json-file into the .kaggle-directory in your home directory.\nTyping the kaggle-command on the command line gives you the final clue as to what is missing: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/chrwittm/.kaggle/kaggle.json'\nTherefore, type:\nchmod 600 /home/&lt;your user&gt;/.kaggle/kaggle.json\nTyping the kaggle-command on the command line again confirms: We are in business :)"
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html#downloading-the-dataset",
    "href": "posts/2022-11-05-kaggle-titanic/index.html#downloading-the-dataset",
    "title": "My First Kaggle Competition: Titanic",
    "section": "Downloading the dataset",
    "text": "Downloading the dataset\nTo download the dataset, run the following command (which you can also find on the kaggle website):\nkaggle competitions download -c titanic\nAs a result, the file titanic.zip is downloaded.\nTo unzip type:\nunzip titanic.zip\nDoing this for the first time, this resulted in an error: /bin/bash: unzip: command not found\nTo install zip and unzip, type:\nsudo apt-get install zip\nsudo apt-get install unzip\nAs a result, unzipping works, and we have a dataset to work with :).\n\nimport pandas as pd\n\ntrain = pd.read_csv(\"train.csv\")\ntrain.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS"
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html#implementing-a-fast.ai-tabular-learner",
    "href": "posts/2022-11-05-kaggle-titanic/index.html#implementing-a-fast.ai-tabular-learner",
    "title": "My First Kaggle Competition: Titanic",
    "section": "Implementing a Fast.ai Tabular Learner",
    "text": "Implementing a Fast.ai Tabular Learner\nThe goal was not to create a perfect submission, but to simply train a model as fast as possible to\n\nget a baseline\nto get to know how a kaggle competition works (remember, this is my first one)\n\nTherefore, I created a dataloaders as shown in lesson 1 or in the docs by sorting the variables into categorical or continuos one, excluding irrelevant ones).\n\nNote 1: In this blog post, I am presenting the steps in a fast-forward way, here is the original notebook.\n\n\nNote 2: When writing this up, I was not able to 100% re-produce the same results, but basically this is how the story went.\n\n\nfrom fastai.tabular.all import *\n\npath = \".\"\n\ndls = TabularDataLoaders.from_csv('train.csv', path=path, y_names=\"Survived\",\n    cat_names = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked'],\n    cont_names = ['Age', 'Fare'],\n    procs = [Categorify, FillMissing, Normalize])\n\nNow we can train a model:\n\nlearn = tabular_learner(dls, metrics=accuracy)\nlearn.fit_one_cycle(10) #change this variable for more/less training\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.548652\n0.315984\n0.640449\n00:00\n\n\n1\n0.454461\n0.325496\n0.640449\n00:00\n\n\n2\n0.373511\n0.289948\n0.640449\n00:00\n\n\n3\n0.319270\n0.251090\n0.640449\n00:00\n\n\n4\n0.280473\n0.196879\n0.640449\n00:00\n\n\n5\n0.249269\n0.173640\n0.640449\n00:00\n\n\n6\n0.225535\n0.152192\n0.640449\n00:00\n\n\n7\n0.207350\n0.141283\n0.640449\n00:00\n\n\n8\n0.192223\n0.137462\n0.640449\n00:00\n\n\n9\n0.180697\n0.137344\n0.640449\n00:00\n\n\n\n\n\nWith this learner, we can make the predictions on the test-dataset.\n\ntest = pd.read_csv(\"test.csv\")\n\n# replacing null values with 0\ntest['Fare'] = test['Fare'].fillna(0)\n\n# create Predictions as suggested here:\n# https://forums.fast.ai/t/tabular-learner-prediction-using-data-frame/90534/2\ntest_dl = learn.dls.test_dl(test)\npreds, _ = learn.get_preds(dl=test_dl)\n\ntest['Survived_pred'] = preds.squeeze()\ntest.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nSurvived_pred\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n0.064765\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n0.454887\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n-0.025921\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n-0.015690\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n0.508172\n\n\n\n\n\n\n\nInterpreting the values in column Survived_pred is important, because we need to turn these values into 0 and 1 for the submission. The submission file should only have the columns PassengerId and Survived. For the first submission, I did not worry about it too much and simply picked a value 0.5. (Let’s come back to that a little later)\n\nthreshold = 0.5 #change this variable for more/less training\ntest['Survived'] = [ 1 if element &gt; threshold else 0 for element in preds.squeeze()]\n\nsubmission1 = test[['PassengerId', 'Survived']]\nsubmission1.to_csv('submission1.csv', index=False)\n\nI uploaded the results, and they were better then random ;) - Score 0.73923\n\nThe score is not great, but the whole point was to get a baseline as quickly as possible, and to “play the whole kaggle game”. Actually, the fact that I produced this result in about 1-2 hours felt pretty good :).\n\nNote: Running this notebook, I got a score of 0.75119, I am not sure, what caused the difference… but better is always good ;)\n\nSo how can we improve the score? More training, interpreting the results differently? As it turns out: Both.\nLet’s look at the distribution of Survived_pred:\n\ntest.Survived_pred.hist();\n\n\n\n\nAs it turned out, setting my threshold to 0.6 created a better result: Score: 0.74162. (this I could not reproduce with this notebook while writing up the blog post)\nAlso more training, produced better results, running for 50 cycles, resulted in a lower loss and a better result. Training with 50 cycles and threshold 0.7, this was the result: Score: 0.76794 (with this notebook 0.77033)\nSo there is some randomness when training, and it is important to properly interpret the results. Getting about 77% right with this simple approach is not to bad."
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html#re-implementing-the-excel-model",
    "href": "posts/2022-11-05-kaggle-titanic/index.html#re-implementing-the-excel-model",
    "title": "My First Kaggle Competition: Titanic",
    "section": "Re-Implementing the Excel Model",
    "text": "Re-Implementing the Excel Model\nAfter the quick win with Fast.AI, I decided to re-implement what Jeremy did in the Excel in video lecture 3 to predict the survivors. Let’s see how it performs against the Fast.AI tabular learner.\nSince that involved quite a bit of code, let me simply link to notebook and discuss the learnings / results.\nAs it turned out:\n\nI had to do a bit of data cleansing.\nThe feature engineering took some time which taught me some general python lessons.\nImplementing the optimizer was a nice exercise, revisiting gradient descent and matrix multiplication, and doing some hands-on work with tensors.\n\nThe first model with just one layer scored 0.75837, even better than the my Fast.AI baseline, but not quite as good as the optimized version.\nThe next iteration with 2 and 3 layers scored better:\n\nScore: 0.77033 (2-layers)\nScore: 0.77272 (3-layers)\n\n\nThis was quite surprising: The self-written algorithm is better than the Fast.AI one, any ideas why that would be?\nNonetheless, it seems to hit a ceiling at 77%, and it would make sense to dive deeper into tabular data, but that is for another time. My goal was not to optimize the competition result, but to participate in my first kaggle competition, and to re-visit the topic of gradient descent and matrix multiplication. I will most likely return to this dataset/challenge in the future."
  },
  {
    "objectID": "posts/2024-02-23-chat-from-scratch/index.html",
    "href": "posts/2024-02-23-chat-from-scratch/index.html",
    "title": "Building Chat for Jupyter Notebooks from Scratch",
    "section": "",
    "text": "Let’s build a light-weight chat for llama2 from scratch which can be reused in your Jupyter notebooks.\nWorking exploratively with large language models (LLMs), I wanted to not only send prompts to LLMs, but to chat with the LLM from within my Jupyter notebook. It turns out, that building a chat from scratch is not complicated. While I use a local llama2 model in this notebook, the concepts I describe are universal and also transfer to other implementations.\nBefore we get started: If you want to interactively run through this blog post, please check out the corresponding Jupyter notebook."
  },
  {
    "objectID": "posts/2024-02-23-chat-from-scratch/index.html#building-chat-messages",
    "href": "posts/2024-02-23-chat-from-scratch/index.html#building-chat-messages",
    "title": "Building Chat for Jupyter Notebooks from Scratch",
    "section": "Building Chat Messages",
    "text": "Building Chat Messages\n“Chat Messages” are the messages you exchange with the LLM. There are 3 roles:\n\nThe system-message gives overall instructions to the LLM which should be used during the whole chat. It only appears once.\nUser-messages are the messages (“prompts”) you send to the LLM.\nThe LLM replies with assistant-messages.\n\nThe chat messages need to be passed to the LLM via the API in a JSON format. Since the chat is stateless, you need to pass the whole previous conversation to be able to ask follow-up questions. The following animated GIF shows the structure in analogy to ChatGPT. Additionally, it animates how the messages build up over time during the chat.\n\n\n\nChat Messages - Please click on the image to start the animation.\n\n\nBased on that, let’s build a class which contains and manages the chat messages:\n\nclass ChatMessages:\n\n    def __init__(self):\n        \"\"\"Initializes the Chat.\"\"\"\n        self._messages = []\n\n    def _append_message(self, role, content):\n        \"\"\"Appends a message with specified role and content to messages list.\"\"\"\n        self._messages.append({\"role\": role, \"content\": content})\n\n    def append_system_message(self, content):\n        \"\"\"Appends a system message with specified content to messages list.\"\"\"\n        self._append_message(\"system\", content)\n\n    def append_user_message(self, content):\n        \"\"\"Appends a user message with specified content to messages list.\"\"\"\n        self._append_message(\"user\", content)\n\n    def append_assistant_message(self, content):\n        \"\"\"Appends an assistant message with specified content to messages list.\"\"\"\n        self._append_message(\"assistant\", content)\n    \n    def get_messages(self):\n        \"\"\"Returns a shallow copy of the messages list.\"\"\"\n        return self._messages[:]\n\nThe 3 methods append_system_message, append_user_message, and append_assistant_message call the private method _append_message so that there is no confusion as to which message types can be added. The get_messages-method returns a copy of the chat messages, making sure that it is not possible to access the private _messages attribute of ChatMessages.\nLet’s quickly re-create the example shown on the image above:\n\nchat_messages = ChatMessages()\nchat_messages.append_system_message(\"For \\\"Question n\\\", please respond with \\\"Response n\\\"\")\nchat_messages.append_user_message(\"Question 1\")\nchat_messages.append_assistant_message(\"Response 1\")\nchat_messages.append_user_message(\"Question 2\")\nchat_messages.append_assistant_message(\"Response 2\")\n\n\nimport json\nfrom pygments import highlight\nfrom pygments.lexers import JsonLexer\nfrom pygments.formatters import TerminalFormatter\n\n# Convert messages to a formatted JSON string\njson_str = json.dumps(chat_messages.get_messages(), indent=4)\n\n# Highlight the JSON string to add colors\nprint(highlight(json_str, JsonLexer(), TerminalFormatter()))\n\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"For \\\"Question n\\\", please respond with \\\"Response n\\\"\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Question 1\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Response 1\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Question 2\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Response 2\"\n    }\n]"
  },
  {
    "objectID": "posts/2024-02-23-chat-from-scratch/index.html#building-chat-version-1",
    "href": "posts/2024-02-23-chat-from-scratch/index.html#building-chat-version-1",
    "title": "Building Chat for Jupyter Notebooks from Scratch",
    "section": "Building Chat Version 1",
    "text": "Building Chat Version 1\nWe have seen in my previous blog post how we can prompt the llama2 model by calling the create_chat_completion-method.\nThe class Llama2ChatVersion2 simplifies prompting the LLM by doing the following:\n\nUpon initialization of the chat object, the chat messages are initialized and the system message is added.\nWhe prompting llama2, both the prompt (the user message) and the response (the assistant message) are added to the chat messages.\nThe plain text returned from llama2 is formatted for better readability.\n\n\nfrom IPython.display import Markdown, clear_output\n\nclass Llama2ChatVersion2:\n\n    def __init__(self, llm, system_message):\n        \"\"\"Initializes the Chat with the system message.\"\"\"\n        self._llm = llm\n        self._chat_messages = ChatMessages()\n        self._chat_messages.append_system_message(system_message)\n\n    def _get_llama2_response(self):\n        \"\"\"Returns Llama2 model response for given messages.\"\"\"\n        self.model_response = self._llm.create_chat_completion(self._chat_messages.get_messages())\n        return self.model_response['choices'][0]['message']['content']\n\n    def _format_markdown_with_style(self, text, font_size=16):\n        \"\"\"Wraps text in a &lt;span&gt; with specified font size, defaults to 16.\"\"\"\n        return f\"&lt;span style='font-size: {font_size}px;'&gt;{text}&lt;/span&gt;\"\n\n    def prompt_llama2(self, user_prompt):\n        \"\"\"Processes user prompt, displays Llama2 response formatted in Markdown.\"\"\"\n        self._chat_messages.append_user_message(user_prompt)\n        llama2_response = self._get_llama2_response()\n        self._chat_messages.append_assistant_message(llama2_response)\n        display(Markdown(self._format_markdown_with_style(llama2_response)))\n\nLet’s create a llama2 instance and prompt it:\n\nfrom llama_cpp import Llama\n#llm = Llama(model_path=\"../models/Llama-2-7b-chat/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=2048, verbose=False)\nllm = Llama(model_path=\"../../../lm-hackers/models/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=2048, verbose=False)\n\n\nchat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\nchat.prompt_llama2(\"Name the planets in the solar system\")\n\n Sure! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\n\nMercury\nVenus\nEarth\nMars\nJupiter\nSaturn\nUranus\nNeptune\n\n\n\nThe result looks good, but from a useability perspective it is not great, because you have to wait for the whole response to be completed before you see an output. On my machine this takes a few seconds for this short reply. Especially for longer answers, however, you might start to wonder if the process is running correctly, or is it just the impatient me?\nIn any case, it would be nice to see the model’s response streamed, i.e. you see the model writing the answer world-by-word / token-by-token, the same way you are used to seeing ChatGPT print out its answers."
  },
  {
    "objectID": "posts/2024-02-23-chat-from-scratch/index.html#how-streaming-works",
    "href": "posts/2024-02-23-chat-from-scratch/index.html#how-streaming-works",
    "title": "Building Chat for Jupyter Notebooks from Scratch",
    "section": "How Streaming Works",
    "text": "How Streaming Works\nThe following cell contains an illustrative example of how streaming works: The function mock_llm_stream() returns a generator object because it does not return a result, but it yields results. This means that the code is not executed when the function is called, but it only returns a generator object which lazily returns values when it is iterated over. The for-loop iterates over the generator object, and each iteration returns a word after some latency, simulating the token generation by the LLM.\n\nimport time\n\ndef fetch_next_word(words, current_index):\n    \"\"\"\n    Mock function to simulate making an API call to fetch the next word from the LLM.\n    \"\"\"\n    # Simulate network delay or processing time\n    time.sleep(0.5)\n    \n    if current_index &lt; len(words):\n        return words[current_index]\n    else:\n        raise StopIteration(\"End of sentence reached.\")\n\ndef mock_llm_stream():\n    sentence = \"This is an example for a text streamed via a generator object.\"\n    words = sentence.split()\n    current_index = 0\n    \n    while True:\n        try:\n            # Simulate fetching the next word from the LLM\n            word = fetch_next_word(words, current_index)\n            yield word\n            current_index += 1\n        except StopIteration:\n            break\n\n# Capture the generator function in a variable\nmock_llm_response = mock_llm_stream()\n\n# Example of how to use this mock_llm_response\nfor word in mock_llm_response:\n    print(word, end=\" \")\n\nThis is an example for a text streamed via a generator object. \n\n\nSo let’s stream a response from llama2:\n\nmessages=[\n    {\"role\": \"system\", \"content\": \"Answer in a very concise and accurate way\"},\n    {\"role\": \"user\", \"content\": \"Name the planets in the solar system\"}]\n\nmodel_response = llm.create_chat_completion(messages = messages, stream=True)\n\ncomplete_response = \"\"\n\nfor part in model_response:\n    # Check if 'content' key exists in the 'delta' dictionary\n    if 'content' in part['choices'][0]['delta']:\n        content = part['choices'][0]['delta']['content']\n        print(content, end='')\n        complete_response += content\n    else:\n        # Handle the case where 'content' key is not present\n        # For example, you can print a placeholder or do nothing\n        # print(\"(no content)\", end='')\n        pass\n\n  Sure! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\n\n1. Mercury\n2. Venus\n3. Earth\n4. Mars\n5. Jupiter\n6. Saturn\n7. Uranus\n8. Neptune"
  },
  {
    "objectID": "posts/2024-02-23-chat-from-scratch/index.html#chat-version-2---streaming-included",
    "href": "posts/2024-02-23-chat-from-scratch/index.html#chat-version-2---streaming-included",
    "title": "Building Chat for Jupyter Notebooks from Scratch",
    "section": "Chat Version 2 - Streaming included",
    "text": "Chat Version 2 - Streaming included\nLet’s include the streaming functionality into our chat messages- and chat-classes. For this we are going to use a nice trick from fastcore to add the 2 new methods: We can @patch the methods into the class:\n\nfrom fastcore.utils import * #for importing patch\n\n\n@patch    \ndef _get_llama2_response_stream(self:Llama2ChatVersion2):\n    \"\"\"Returns generator object for streaming Llama2 model responses for given messages.\"\"\"\n    return self._llm.create_chat_completion(self._chat_messages.get_messages(), stream=True)\n\n@patch\ndef prompt_llama2_stream(self:Llama2ChatVersion2, user_prompt):\n    \"\"\"Processes user prompt in streaming mode, displays updates in Markdown.\"\"\"\n    self._chat_messages.append_user_message(user_prompt)\n    llama2_response_stream = self._get_llama2_response_stream()\n    \n    complete_stream = \"\"\n\n    for part in llama2_response_stream:\n        # Check if 'content' key exists in the 'delta' dictionary\n        if 'content' in part['choices'][0]['delta']:\n            stream_content = part['choices'][0]['delta']['content']\n            complete_stream += stream_content\n\n            # Clear previous output and display new content\n            clear_output(wait=True)\n            display(Markdown(self._format_markdown_with_style(complete_stream)))\n\n        else:\n            # Handle the case where 'content' key is not present\n            pass\n    \n    self._chat_messages.append_assistant_message(complete_stream)\n\nNow we can use the method prompt_llama2_stream to get a more interactive response:\n\nchat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\nchat.prompt_llama2_stream(\"Name the planets in the solar system\")\n\n Sure! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\n\nMercury\nVenus\nEarth\nMars\nJupiter\nSaturn\nUranus\nNeptune\n\n\n\nJust for the fun of it, let’s continue the chat:\n\nchat.prompt_llama2_stream(\"Please reverse the list\")\n\n Of course! Here are the names of the planets in our solar system in reverse order, from farthest to closest to the Sun: 1. Neptune 2. Uranus 3. Saturn 4. Jupiter 5. Mars 6. Earth 7. Venus 8. Mercury\n\n\n\nchat.prompt_llama2_stream(\"Please sort the list by the mass of the planet\")\n\n Sure! Here are the names of the planets in our solar system sorted by their mass, from lowest to highest: 1. Mercury (0.38 Earth masses) 2. Mars (0.11 Earth masses) 3. Venus (0.81 Earth masses) 4. Earth (1.00 Earth masses) 5. Jupiter (29.6 Earth masses) 6. Saturn (95.1 Earth masses) 7. Uranus (14.5 Earth masses) 8. Neptune (17.1 Earth masses)\n\n\nLooping back to the beginning, you can see how the chat is represented in the chat massages.\n\nchat._chat_messages.get_messages()\n\n[{'role': 'system', 'content': 'Answer in a very concise and accurate way'},\n {'role': 'user', 'content': 'Name the planets in the solar system'},\n {'role': 'assistant',\n  'content': '  Sure! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\\n\\n1. Mercury\\n2. Venus\\n3. Earth\\n4. Mars\\n5. Jupiter\\n6. Saturn\\n7. Uranus\\n8. Neptune'},\n {'role': 'user', 'content': 'Please reverse the list'},\n {'role': 'assistant',\n  'content': '  Of course! Here are the names of the planets in our solar system in reverse order, from farthest to closest to the Sun:\\n1. Neptune\\n2. Uranus\\n3. Saturn\\n4. Jupiter\\n5. Mars\\n6. Earth\\n7. Venus\\n8. Mercury'},\n {'role': 'user', 'content': 'Please sort the list by the mass of the planet'},\n {'role': 'assistant',\n  'content': '  Sure! Here are the names of the planets in our solar system sorted by their mass, from lowest to highest:\\n1. Mercury (0.38 Earth masses)\\n2. Mars (0.11 Earth masses)\\n3. Venus (0.81 Earth masses)\\n4. Earth (1.00 Earth masses)\\n5. Jupiter (29.6 Earth masses)\\n6. Saturn (95.1 Earth masses)\\n7. Uranus (14.5 Earth masses)\\n8. Neptune (17.1 Earth masses)'}]"
  },
  {
    "objectID": "posts/2024-02-23-chat-from-scratch/index.html#conclusion",
    "href": "posts/2024-02-23-chat-from-scratch/index.html#conclusion",
    "title": "Building Chat for Jupyter Notebooks from Scratch",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we implemented an LLM chat from scratch in a very light-weight format. We learned how the chat messages need to be handled to create the chat experience and we even added streaming support.\nAnd the best thing, we can re-use this chat functionality in other notebooks without having to re-write it or copy&paste again, keeping our notebooks dry and clean. I have moved the core code of this notebook to a separate .py-file, and this notebook demonstrates how to re-use the notebook chat in another notebook. 😀"
  },
  {
    "objectID": "posts/2024-02-16-c-binding-example/index.html",
    "href": "posts/2024-02-16-c-binding-example/index.html",
    "title": "How to Implement a C Binding in Python with ctypes",
    "section": "",
    "text": "How can compute intensive large language models (LLMs) run on consumer-grade laptops? C bindings are part of this magic, they create wrappers around the C code to make is accessible in higher-level languages like Python. While it might sound complicated, the concept is surprisingly accessible with the right approach. Let’s explore a simple example to how to utilize the ctypes library to implement a C bindings in Python.\nIn my previous blog post, I demonstrated how you can use llama-cpp-python to run a llama2-model using llama.cpp. To understand how the interface between these 2 project works, I created a simple C library to unveil some of the underlying “magic”.\nThe Python ctypes-library is the bridge between the to worlds, and in this notebook, we first create and compile a simple function written in C that accepts an int32_t value and returns its square. Subsequently, we use this function from python to learn how to implement the C binding via the ctypes-library.\nWhile I wrote this notebook on macOS, the principles and techniques are universally applicable, with slight adjustments for Linux or Windows environments.\nA final note before we get started: You can find the notebook version of this blog post on my GitHub."
  },
  {
    "objectID": "posts/2024-02-16-c-binding-example/index.html#step-1-create-the-c-code-library",
    "href": "posts/2024-02-16-c-binding-example/index.html#step-1-create-the-c-code-library",
    "title": "How to Implement a C Binding in Python with ctypes",
    "section": "Step 1: Create the C Code Library",
    "text": "Step 1: Create the C Code Library\nCreate a file named example.c with the following content:\n#include &lt;stdint.h&gt;\n\nint32_t square(int32_t number) {\n    return number * number;\n}\nNext, compile this C code into a shared library via the terminal\n# Linux / so -&gt; shared object\ngcc -shared -fpic -o libexample.so example.c\n# macOS / dylib -&gt; dynamic library\ngcc -shared -fpic -o libexample.dylib example.c\n# Windows / dll -&gt; dynamic-link library\ngcc -shared -o example.dll example.c\nBefore we run the command, let’s break it down:\n\ngcc stands for “GNU Compiler Collection”, and it can compile C by default.\n-shared creates a “shared library”. In Python analogy, this is like a module which can be imported.\n-fpic creates “Position-Independent Code” (PIC), removing any absolute memory references and making them relative.\nFor C developers, it is good practice to prefix the name of shared libraries (dynamic libraries) with “lib”, hence example.c becomes libexample.xxx\n\nSince I am running on a Mac, I use the following command to compile my library:\n\n!gcc -shared -fpic -o libexample.dylib example.c\n\nAs a result, I get a new file called libexample.dylib."
  },
  {
    "objectID": "posts/2024-02-16-c-binding-example/index.html#step-2-python-code",
    "href": "posts/2024-02-16-c-binding-example/index.html#step-2-python-code",
    "title": "How to Implement a C Binding in Python with ctypes",
    "section": "Step 2: Python Code",
    "text": "Step 2: Python Code\nTo call this function from Python, we need to do couple of steps.\nFirst, we need to load the shared libaray via the ctypes.CDLL-method to access our square function:\n\nimport ctypes\n\nlibexample = ctypes.CDLL('./libexample.dylib')  # Use appropriate file name on your system\n\nNext, let’s create an object which represents a 32-bit integer which corresponds to the C type int32_t. This is the type we used in our example C code.\n\nc_int32_type = ctypes.c_int32\n\nPreparing for the call to C, we need to specify the arguments and return type of the C function so that the variables can be converted correctly:\n\nThe arguments argtypes are passed in a list, because there could be more arguments (depending on the function).\nThe result type restype is just a single value, because a function returns exactly one result.\n\n\nlibexample.square.argtypes = [c_int32_type]\nlibexample.square.restype = c_int32_type\n\nWe want to calculate the square of a number_to_be_squared. This Python variable first needs to be converted into a proper int32 representation:\n\nnumber_to_be_squared = 7\ninput_value = c_int32_type(number_to_be_squared)\n\nFinally, we can call the C function:\n\nresult = libexample.square(input_value)\nprint(f\"The square of {input_value.value} is {result}.\")\n\nThe square of 7 is 49.\n\n\nNote that the input type is a C type, therefore we need to use .value to access the Python equivalent, and the result is automatically converted to a Python type:\n\nprint(f\"The input value type is {type(input_value)}\")\nprint(f\"The result value type is {type(result)}\")\n\nThe input value type is &lt;class 'ctypes.c_int'&gt;\nThe result value type is &lt;class 'int'&gt;"
  },
  {
    "objectID": "posts/2024-02-16-c-binding-example/index.html#wrapping-up",
    "href": "posts/2024-02-16-c-binding-example/index.html#wrapping-up",
    "title": "How to Implement a C Binding in Python with ctypes",
    "section": "Wrapping up",
    "text": "Wrapping up\nThis tiny example demonstrated how a C binding works and which steps are needed to call C code from Python. Although aligning the types between Python and C requires some effort, the payoff is significantly enhanced performance for compute-intensive tasks like neural net inference. While introducing additional complexity, the C binding llama-cpp-python makes it possible to run a llama2-model via llama.cpp directly from Python, even on a consumer laptop."
  },
  {
    "objectID": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html",
    "href": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html",
    "title": "Creating a Blog Post using a Jupyter Notebook",
    "section": "",
    "text": "This is somehow a “Hello World”-notebook, since its only purpose is to demonstrate how you can use a jupyter notebook to write a blog post using Quarto.\nSomehow I did not find the key ingredient in the Quarto docs, but in this blog post: To get the necessary header data into the jupyter notebook, you need to add a RAW-cell at the top which contains the metadata. This is how this cell looks like in this notebook (and here is an hello-world example):"
  },
  {
    "objectID": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html#jupyter-notebook-.ipynb-vs.-quarto-.qmd",
    "href": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html#jupyter-notebook-.ipynb-vs.-quarto-.qmd",
    "title": "Creating a Blog Post using a Jupyter Notebook",
    "section": "Jupyter Notebook (.ipynb) vs. Quarto (.qmd)",
    "text": "Jupyter Notebook (.ipynb) vs. Quarto (.qmd)\nFor my current use case of blogging I prefer the jupyter notebooks, and I will most likely write all future blog posts in jupyter notebooks because of the following:\n\nMy spell checking addon for VS Code does not support .qmd files.\nWith jupyter notebook there is no need to render the files, rendering is instant in jupyter notebook when you execute the cell.\nJupyter notebook is the same environment when I code, no need to adjust (even if only slightly)\n\nOf course, all of this is very personal and a current snapshot of preferences (as a beginner) - let’s see if this solidifies or changes."
  },
  {
    "objectID": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html#how-is-code-rendered",
    "href": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html#how-is-code-rendered",
    "title": "Creating a Blog Post using a Jupyter Notebook",
    "section": "How is code rendered?",
    "text": "How is code rendered?\nLet’s try out a little bit of code:\n\nHello World\n\nprint(\"Hello World!\")\n\nHello World!\n\n\n\n\nCalculations\n\na = 1\nb = 2\nc = a+b\nprint(c)\n\n3\n\n\n\n\nPlotting\n\nimport matplotlib.pyplot as plt\n    \nx = [1,2,3]\ny = [2,4,1]\n    \nplt.plot(x, y)\n    \nplt.xlabel('x - axis')\nplt.ylabel('y - axis')\nplt.title('Sample graph')\n    \nplt.show()"
  },
  {
    "objectID": "posts/2022-10-03-bear-detector-2022/index.html",
    "href": "posts/2022-10-03-bear-detector-2022/index.html",
    "title": "Fast.AI with Bears, Cats and Dogs",
    "section": "",
    "text": "The 2022-version of the Fast.AI course was a welcome and well-timed opportunity for me to continue my machine learning journey.\nAfter having reworked lesson 2, here are my first trained models:\n\nBear Detector on HuggingFace\nBear Detector on GitHub Pages\nCat-vs-Dog-Classifier on HuggingFace\nCat-vs-Dog-Classifier on GitHub Pages\n\nFor the complete summary and the source code, check out my GitHub.\n\n\n\nBear Detector 2022"
  },
  {
    "objectID": "posts/2022-10-13-visualizing-gradient-descent-in-3d/index.html",
    "href": "posts/2022-10-13-visualizing-gradient-descent-in-3d/index.html",
    "title": "Visualizing Gradient Descent in 3D",
    "section": "",
    "text": "If you want to understand Machine Learning you have to understand gradient descent, we have all heard that before ;). Since I am a visual person, I tried to not only think through the concept, but also to visualize it.\nBased on Jeremy’s great notebook “How does a neural net really work?”, I created a notebook which visualizes gradient descent in 3D. There are two version:"
  },
  {
    "objectID": "posts/2022-10-13-visualizing-gradient-descent-in-3d/index.html#the-backstory",
    "href": "posts/2022-10-13-visualizing-gradient-descent-in-3d/index.html#the-backstory",
    "title": "Visualizing Gradient Descent in 3D",
    "section": "The backstory",
    "text": "The backstory\nGradient descent is one of the topics of lesson 3 of the 2022-Fast.AI-Course. On a high level, it is pretty straight forward:\n\nCalculate the predictions and the loss (forward-pass)\nInitialize and calculate the gradients (i.e. derivatives of the parameters, i.e. how does changing the parameters change the loss) (backward-pass)\nUpdate the parameters (via the learning rate)\nRestart\n\nLooking at the python code, however, it is very compact, and a lot of magic is going on. Trying to unpack this and to get a solid and intuit understanding of gradient descent, I tried to not only think through the concept, but also to visualize it.\nI started playing with Jeremy’s notebook, and what started out as a rough idea turned into the notebooks on Kaggle and GitHub.\nI learned a lot about gradient descent and python (especially plotting) along the way, and I hope you find the visualizations useful."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html",
    "title": "How I created this Blog",
    "section": "",
    "text": "Blogging is an essential part of the Fast.AI methodology, therefore, I decided to follow the advice and start my blog to document my “Machine Learning Journey”. Here are the steps it took to create this blog.\nThe previous goto solution “Fastpages” has been depreciated in favor of Quarto. There is a good tutorial, but honestly I found it a bit intimidating because it is not a simple step-by-step guide, also the Creating a Blog page did no quite fit this category (for me). Therefore, without being an expert at this, let me share what I did to create this blog.\nSo if your goal is to create a simple blog based on Quarto, just hop on and follow along :).\nA little side-note: This is the fourth blog post I write with Quarto, and by now I feel that some rough edged have been removed, mostly because I realized that really all the blogging can be done in jupyter notebook! Therefore, learning, running my own experiments and blogging happen in the same environment and with a lot of reuse. Knowing that my more things can be done, the only goal of this blog post is to get you up and running with a basic setup and explain the possibility to blog via jupyter notebooks."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-1-create-a-new-github-repo",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-1-create-a-new-github-repo",
    "title": "How I created this Blog",
    "section": "Step 1: Create a new GitHub repo",
    "text": "Step 1: Create a new GitHub repo\nYour blog will reside in a GitHub repo, and it will leverage Github Pages. Follow steps 1 and 2 on the GitHub Pages homepage for the initial setup.\n\nNote: The default recommendation for the repo is &lt;your username&gt;.github.io. I took that recommendation, but anything else should work as well.\n\nAs a result you have an empty repo with just a readme.md file. Here’s how it still looks in my repo.\nFor cloning the repo to my local machine, I did:\ngit clone git@github.com:chrwittm/chrwittm.github.io.git\nTwo final activities are needed to finalize the setup of your repo:\n\nCreate a new branch called gh-pages. To do this, go to your branches (for me that is https://github.com/chrwittm/chrwittm.github.io/branches), and create the new branch by clicking the “New branch”-button in the top right.\nSet the new branch as the branch for GitHub Pages. In your repo, navigate to Settings -&gt; Pages. (In my repo that takes me to https://github.com/chrwittm/chrwittm.github.io/settings/pages.) Change main to gh-pages.\n\n\nFor more info on setting the branch, please refer to the Quarto Docs."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-2-install-quarto",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-2-install-quarto",
    "title": "How I created this Blog",
    "section": "Step 2: Install Quarto",
    "text": "Step 2: Install Quarto\nI am currently working on a Windows 10 machine and I usually work both in WSL (Ubuntu) (e.g. for Jupyter and anything related to Fast.AI development) and in Windows with VS Code (e.g. for writing this blog).\n\nInstalling Quarto in WSL\nFrom previous activities with nbdev, I already had Quarto installed. If I re-traced my steps correctly, here is what I did (as suggested here and here):\nmamba install -c fastchan nbdev\nnbdev_install_quarto\n\n\nInstalling Quarto for Windows (optional)\nOptional: Once I discovered that I can do everything in jupyter, I would label this step as optional, because I only used the Windows installation of Quarto to render previews of .qmd-files - which I do not need anymore when everything is done in jupyter notebooks.\nGo to this page, download and install Quarto.\n\n\nSetup Addons for VS Code (optional)\nOptional: Once I discovered that I can do everything in jupyter, I would label this step as optional, because I only used the Windows installation of Quarto to render previews of .qmd-files - which I do not need anymore when everything is done in jupyter notebooks.\nI also installed the Quarto extension for VS Code."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-3-initial-setup-to-publish-hello-world",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-3-initial-setup-to-publish-hello-world",
    "title": "How I created this Blog",
    "section": "Step 3: Initial setup to publish “Hello World”",
    "text": "Step 3: Initial setup to publish “Hello World”\nBy now we are really close to publishing the “Hello World”-version of our blog: In the command line, go to the directory of your repo, and run the following commands, and the example content for the Quarto blog should be published to your repo.\nquarto create-project --type website:blog\nquarto publish gh-pages\nOnce done, you can open your blog at: &lt;https://\"your username\".github.io/&gt;\nFor some more background on what is happening with these two commands, please refer to this this page (choose “Terminal”) and this page."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-4-create-your-first-blog-post",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-4-create-your-first-blog-post",
    "title": "How I created this Blog",
    "section": "Step 4: Create your first Blog Post",
    "text": "Step 4: Create your first Blog Post\nNow it is time to create your first own blog post.\nIn the posts-directory, create a new folder, for example hello-world. Within this folder, create a notebook called index.ipynb. Add some hello-world content and a RAW-section as the first cell with this content (here is an example):\n---\ntitle: \"Hello World\"\nauthor: \"Your Name\"\ndate: \"2022-01-01\"\n---\nRepublish your blog:\nquarto publish gh-pages\nCongratulations, you just published your first blog post!\nFor a little more detailed version of the hello world blog post, please refer to my other hello world post."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-5-avoiding-disaster",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-5-avoiding-disaster",
    "title": "How I created this Blog",
    "section": "Step 5: Avoiding Disaster",
    "text": "Step 5: Avoiding Disaster\nWhen you run quarto publish gh-pages, your blog posts are rendered, and the rendered versions are pushed to git in branch gh-pages. Your actual notebooks are not uploaded to GitHub. Also any config you make to the blog etc. is uploaded in the rendered versions only. So if something were to happen to your local files, your work would be lost. (Such a disaster almost happened to me but the OneDrive file history saved me.)\nTherefore, I would recommend to also upload the “source”-files to GitHub (in the main branch):\ngit add posts/\ngit add _quarto.yml\ngit add about.qmd\ngit add index.qmd\ngit add profile.png\ngit add styles.css\ngit add .gitignore\ngit commit -m \"uploaded source files\"\ngit push\nAs a result, the source files are also stored on GitHub."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#steps-6-to-n-additional-setup",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#steps-6-to-n-additional-setup",
    "title": "How I created this Blog",
    "section": "Steps 6 to n: Additional setup",
    "text": "Steps 6 to n: Additional setup\nThere are many more things that can be done with the blog, but to keep things down to basics, let me just mention a few topics which will make the blog look like your own blog.\nAdditionally, let me mention one other blog post as a reference which I found only when looking into more detailed setup topics like comments and analytics. Albert Rapp’s blog post The ultimate guide to starting a Quarto blog truly is a great guide for setting up your Quarto blog.\n\nStep 6.1: Remove example content\nNow that the hello world blog post is published, you can remove the default content. I just turned the two example blog posts into drafts by adding the following line in their headers:\ndraft: true\n\n\nStep 6.2: Update remaining example content\nUpdate the following files and add/change the content, so that the blog looks like it is your blog:\n_quarto.yml\nabout.qmd\nindex.qmd"
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#conclusion",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#conclusion",
    "title": "How I created this Blog",
    "section": "Conclusion",
    "text": "Conclusion\nSetting up the blog was not really hard, but it took some time for me. Hopefully, this guide contains some shortcuts for you. Happy blogging!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christian’s Machine Learning Journey",
    "section": "",
    "text": "Visualizing Embeddings in 2D\n\n\n\n\n\n\n\nembeddings\n\n\nllm\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nHow to Convert a Wordpress Blog into Markdown\n\n\n\n\n\n\n\nwordpress\n\n\nmarkdown\n\n\ndataset\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nBuilding Chat for Jupyter Notebooks from Scratch\n\n\n\n\n\n\n\nchat\n\n\njupyter\n\n\nllama2\n\n\nllm\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nHow to Implement a C Binding in Python with ctypes\n\n\n\n\n\n\n\nc\n\n\nbinding\n\n\nllama.cpp\n\n\nllm\n\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nRunning LLama2 locally on a Mac\n\n\n\n\n\n\n\nllama2\n\n\napple silicon\n\n\nllama.cpp\n\n\nhugging face\n\n\nllm\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nHow to call the OpenAI API from a Jupyter Notebook\n\n\n\n\n\n\n\nopenai\n\n\njupyter\n\n\napi\n\n\nllm\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nRunning Fast.AI / Huggingface Transformers on Apple Silicon\n\n\n\n\n\n\n\nml\n\n\nfast.ai\n\n\napple silicon\n\n\nhugging face\n\n\nmnist\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJan 5, 2024\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nInstalling Fast.AI on Apple Silicon\n\n\n\n\n\n\n\nml\n\n\nfast.ai\n\n\napple silicon\n\n\ninstall\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2023\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nTitanic with ChatGPT\n\n\n\n\n\n\n\nml\n\n\nkaggle\n\n\ntitanic\n\n\nchatgpt\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nDiscovering Disaster Tweet Dataset Limitations\n\n\n\n\n\n\n\nml\n\n\ndata\n\n\nhugging face\n\n\nnlp\n\n\nkaggle\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nNatural Language Processing with Disaster Tweets\n\n\n\n\n\n\n\nkaggle\n\n\nnlp\n\n\nhugging face\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nWrapping-up Lesson 3\n\n\n\n\n\n\n\nfast.ai\n\n\nkaggle\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nWhen disaster strikes: Re-building a Quarto Blog\n\n\n\n\n\n\n\nblogging\n\n\nquarto\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nMNIST, the ‘Hello World’ of Computer Vision\n\n\n\n\n\n\n\nkaggle\n\n\nmnist\n\n\nfast.ai\n\n\nvision\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nMy First Kaggle Competition: Titanic\n\n\n\n\n\n\n\nkaggle\n\n\ntitanic\n\n\nfast.ai\n\n\nml\n\n\ntabular\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nMatrix Multiplication\n\n\n\n\n\n\n\nmath\n\n\npython\n\n\nnumpy\n\n\npytorch\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nHow I created this Blog\n\n\n\n\n\n\n\nblogging\n\n\nquarto\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nCreating a Blog Post using a Jupyter Notebook\n\n\n\n\n\n\n\nblogging\n\n\nquarto\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nVisualizing Gradient Descent in 3D\n\n\n\n\n\n\n\nfast.ai\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nFast.AI with Bears, Cats and Dogs\n\n\n\n\n\n\n\nfast.ai\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\nHello World!\n\n\n\n\n\n\n\nblogging\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\nNo matching items"
  }
]