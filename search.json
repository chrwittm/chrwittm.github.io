[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "‚ÄúThe Machine Learning Journey‚Äù ;)\nI am mainly writing for myself, trying to explain/clarify concepts to myself while writing,\ntherefore implementing the concept of ‚ÄúThe Best Way to Learn Is to Teach‚Äù.\nIf you stumbled across this blog and you even find it useful, that would make me happy :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christian‚Äôs Machine Learning Journey",
    "section": "",
    "text": "ml\n\n\ndata\n\n\nhugging face\n\n\nnlp\n\n\nkaggle\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nkaggle\n\n\nnlp\n\n\nhugging face\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfast.ai\n\n\nkaggle\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblogging\n\n\nquarto\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nkaggle\n\n\nmnist\n\n\nfast.ai\n\n\nvision\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nkaggle\n\n\ntitanic\n\n\nfast.ai\n\n\nml\n\n\ntabular\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmath\n\n\npython\n\n\nnumpy\n\n\npytorch\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblogging\n\n\nquarto\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblogging\n\n\nquarto\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfast.ai\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfast.ai\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblogging\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\nChristian Wittmann\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-10-01-hello-world/index.html",
    "href": "posts/2022-10-01-hello-world/index.html",
    "title": "Hello World!",
    "section": "",
    "text": "Hello World!"
  },
  {
    "objectID": "posts/2022-10-03-bear-detector-2022/index.html",
    "href": "posts/2022-10-03-bear-detector-2022/index.html",
    "title": "Fast.AI with Bears, Cats and Dogs",
    "section": "",
    "text": "After having reworked lesson 2, here are my first trained models:\n\nBear Detector on HuggingFace\nBear Detector on GitHub Pages\nCat-vs-Dog-Classifier on HuggingFace\nCat-vs-Dog-Classifier on GitHub Pages\n\nFor the complete summary and the source code, check out my GitHub.\n\n\n\nBear Detector 2022"
  },
  {
    "objectID": "posts/2022-10-13-visualizing-gradient-descent-in-3d/index.html",
    "href": "posts/2022-10-13-visualizing-gradient-descent-in-3d/index.html",
    "title": "Visualizing Gradient Descent in 3D",
    "section": "",
    "text": "If you want to understand Machine Learning you have to understand gradient descent, we have all heard that before ;). Since I am a visual person, I tried to not only think through the concept, but also to visualize it.\nBased on Jeremy‚Äôs great notebook ‚ÄúHow does a neural net really work?‚Äù, I created a notebook which visualizes gradient descent in 3D. There are two version:"
  },
  {
    "objectID": "posts/2022-10-13-visualizing-gradient-descent-in-3d/index.html#the-backstory",
    "href": "posts/2022-10-13-visualizing-gradient-descent-in-3d/index.html#the-backstory",
    "title": "Visualizing Gradient Descent in 3D",
    "section": "The backstory",
    "text": "The backstory\nGradient descent is one of the topics of lesson 3 of the 2022-Fast.AI-Course. On a high level, it is pretty straight forward:\n\nCalculate the predictions and the loss (forward-pass)\nInitialize and calculate the gradients (i.e.¬†derivatives of the parameters, i.e.¬†how does changing the parameters change the loss) (backward-pass)\nUpdate the parameters (via the learning rate)\nRestart\n\nLooking at the python code, however, it is very compact, and a lot of magic is going on. Trying to unpack this and to get a solid and intuit understanding of gradient descent, I tried to not only think through the concept, but also to visualize it.\nI started playing with Jeremy‚Äôs notebook, and what started out as a rough idea turned into the notebooks on Kaggle and GitHub.\nI learned a lot about gradient descent and python (especially plotting) along the way, and I hope you find the visualizations useful."
  },
  {
    "objectID": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html",
    "href": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html",
    "title": "Creating a Blog Post using a Jupyter Notebook",
    "section": "",
    "text": "This is somehow a ‚ÄúHello World‚Äù-notebook, since its only purpose is to demonstrate how you can use a jupyter notebook to write a blog post using Quarto.\nSomehow I did not find the key ingredient in the Quarto docs, but in this blog post: To get the necessary header data into the jupyter notebook, you need to add a RAW-cell at the top which contains the metadata. This is how this cell looks like in this notebook (and here is an hello-world example):"
  },
  {
    "objectID": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html#jupyter-notebook-.ipynb-vs.-quarto-.qmd",
    "href": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html#jupyter-notebook-.ipynb-vs.-quarto-.qmd",
    "title": "Creating a Blog Post using a Jupyter Notebook",
    "section": "Jupyter Notebook (.ipynb) vs.¬†Quarto (.qmd)",
    "text": "Jupyter Notebook (.ipynb) vs.¬†Quarto (.qmd)\nFor my current use case of blogging I prefer the jupyter notebooks, and I will most likely write all future blog posts in jupyter notebooks because of the following:\n\nMy spell checking addon for VS Code does not support .qmd files.\nWith jupyter notebook there is no need to render the files, rendering is instant in jupyter notebook when you execute the cell.\nJupyter notebook is the same environment when I code, no need to adjust (even if only slightly)\n\nOf course, all of this is very personal and a current snapshot of preferences (as a beginner) - let‚Äôs see if this solidifies or changes."
  },
  {
    "objectID": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html#how-is-code-rendered",
    "href": "posts/2022-10-18-blogging-with-jupyter-notebook/index.html#how-is-code-rendered",
    "title": "Creating a Blog Post using a Jupyter Notebook",
    "section": "How is code rendered?",
    "text": "How is code rendered?\nLet‚Äôs try out a little bit of code:\n\nHello World\n\nprint(\"Hello World!\")\n\nHello World!\n\n\n\n\nCalculations\n\na = 1\nb = 2\nc = a+b\nprint(c)\n\n3\n\n\n\n\nPlotting\n\nimport matplotlib.pyplot as plt\n    \nx = [1,2,3]\ny = [2,4,1]\n    \nplt.plot(x, y)\n    \nplt.xlabel('x - axis')\nplt.ylabel('y - axis')\nplt.title('Sample graph')\n    \nplt.show()"
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html",
    "title": "How I created this Blog",
    "section": "",
    "text": "Blogging is an essential part of the Fast.AI methodology, therefore, I decided to follow the advice and start my blog to document my ‚ÄúMachine Learning Journey‚Äù. Here are the steps it took to create this blog.\nThe previous goto solution ‚ÄúFastpages‚Äù has been depreciated in favor of Quarto. There is a good tutorial, but honestly I found it a bit intimidating because it is not a simple step-by-step guide, also the Creating a Blog page did no quite fit this category (for me). Therefore, without being an expert at this, let me share what I did to create this blog.\nSo if your goal is to create a simple blog based on Quarto, just hop on and follow along :).\nA little side-note: This is the fourth blog post I write with Quarto, and by now I feel that some rough edged have been removed, mostly because I realized that really all the blogging can be done in jupyter notebook! Therefore, learning, running my own experiments and blogging happen in the same environment and with a lot of reuse. Knowing that my more things can be done, the only goal of this blog post is to get you up and running with a basic setup and explain the possibility to blog via jupyter notebooks."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-1-create-a-new-github-repo",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-1-create-a-new-github-repo",
    "title": "How I created this Blog",
    "section": "Step 1: Create a new GitHub repo",
    "text": "Step 1: Create a new GitHub repo\nYour blog will reside in a GitHub repo, and it will leverage Github Pages. Follow steps 1 and 2 on the GitHub Pages homepage for the initial setup.\n\nNote: The default recommendation for the repo is <your username>.github.io. I took that recommendation, but anything else should work as well.\n\nAs a result you have an empty repo with just a readme.md file. Here‚Äôs how it still looks in my repo.\nFor cloning the repo to my local machine, I did:\ngit clone git@github.com:chrwittm/chrwittm.github.io.git\nTwo final activities are needed to finalize the setup of your repo:\n\nCreate a new branch called gh-pages. To do this, go to your branches (for me that is https://github.com/chrwittm/chrwittm.github.io/branches), and create the new branch by clicking the ‚ÄúNew branch‚Äù-button in the top right.\nSet the new branch as the branch for GitHub Pages. In your repo, navigate to Settings -> Pages. (In my repo that takes me to https://github.com/chrwittm/chrwittm.github.io/settings/pages.) Change main to gh-pages.\n\n\nFor more info on setting the branch, please refer to the Quarto Docs."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-2-install-quarto",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-2-install-quarto",
    "title": "How I created this Blog",
    "section": "Step 2: Install Quarto",
    "text": "Step 2: Install Quarto\nI am currently working on a Windows 10 machine and I usually work both in WSL (Ubuntu) (e.g.¬†for Jupyter and anything related to Fast.AI development) and in Windows with VS Code (e.g.¬†for writing this blog).\n\nInstalling Quarto in WSL\nFrom previous activities with nbdev, I already had Quarto installed. If I re-traced my steps correctly, here is what I did (as suggested here and here):\nmamba install -c fastchan nbdev\nnbdev_install_quarto\n\n\nInstalling Quarto for Windows (optional)\nOptional: Once I discovered that I can do everything in jupyter, I would label this step as optional, because I only used the Windows installation of Quarto to render previews of .qmd-files - which I do not need anymore when everything is done in jupyter notebooks.\nGo to this page, download and install Quarto.\n\n\nSetup Addons for VS Code (optional)\nOptional: Once I discovered that I can do everything in jupyter, I would label this step as optional, because I only used the Windows installation of Quarto to render previews of .qmd-files - which I do not need anymore when everything is done in jupyter notebooks.\nI also installed the Quarto extension for VS Code."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-3-initial-setup-to-publish-hello-world",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-3-initial-setup-to-publish-hello-world",
    "title": "How I created this Blog",
    "section": "Step 3: Initial setup to publish ‚ÄúHello World‚Äù",
    "text": "Step 3: Initial setup to publish ‚ÄúHello World‚Äù\nBy now we are really close to publishing the ‚ÄúHello World‚Äù-version of our blog: In the command line, go to the directory of your repo, and run the following commands, and the example content for the Quarto blog should be published to your repo.\nquarto create-project --type website:blog\nquarto publish gh-pages\nOnce done, you can open your blog at: <https://\"your username\".github.io/>\nFor some more background on what is happening with these two commands, please refer to this this page (choose ‚ÄúTerminal‚Äù) and this page."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-4-create-your-first-blog-post",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-4-create-your-first-blog-post",
    "title": "How I created this Blog",
    "section": "Step 4: Create your first Blog Post",
    "text": "Step 4: Create your first Blog Post\nNow it is time to create your first own blog post.\nIn the posts-directory, create a new folder, for example hello-world. Within this folder, create a notebook called index.ipynb. Add some hello-world content and a RAW-section as the first cell with this content (here is an example):\n---\ntitle: \"Hello World\"\nauthor: \"Your Name\"\ndate: \"2022-01-01\"\n---\nRepublish your blog:\nquarto publish gh-pages\nCongratulations, you just published your first blog post!\nFor a little more detailed version of the hello world blog post, please refer to my other hello world post."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#step-5-avoiding-disaster",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#step-5-avoiding-disaster",
    "title": "How I created this Blog",
    "section": "Step 5: Avoiding Disaster",
    "text": "Step 5: Avoiding Disaster\nWhen you run quarto publish gh-pages, your blog posts are rendered, and the rendered versions are pushed to git in branch gh-pages. Your actual notebooks are not uploaded to GitHub. Also any config you make to the blog etc. is uploaded in the rendered versions only. So if something were to happen to your local files, your work would be lost. (Such a disaster almost happened to me but the OneDrive file history saved me.)\nTherefore, I would recommend to also upload the ‚Äúsource‚Äù-files to GitHub (in the main branch):\ngit add posts/\ngit add _quarto.yml\ngit add about.qmd\ngit add index.qmd\ngit add profile.png\ngit add styles.css\ngit add .gitignore\ngit commit -m \"uploaded source files\"\ngit push\nAs a result, the source files are also stored on GitHub."
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#steps-6-to-n-additional-setup",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#steps-6-to-n-additional-setup",
    "title": "How I created this Blog",
    "section": "Steps 6 to n: Additional setup",
    "text": "Steps 6 to n: Additional setup\nThere are many more things that can be done with the blog, but to keep things down to basics, let me just mention a few topics which will make the blog look like your own blog.\nAdditionally, let me mention one other blog post as a reference which I found only when looking into more detailed setup topics like comments and analytics. Albert Rapp‚Äôs blog post The ultimate guide to starting a Quarto blog truly is a great guide for setting up your Quarto blog.\n\nStep 6.1: Remove example content\nNow that the hello world blog post is published, you can remove the default content. I just turned the two example blog posts into drafts by adding the following line in their headers:\ndraft: true\n\n\nStep 6.2: Update remaining example content\nUpdate the following files and add/change the content, so that the blog looks like it is your blog:\n_quarto.yml\nabout.qmd\nindex.qmd"
  },
  {
    "objectID": "posts/2022-10-21-how-i-created-this-blog/index.html#conclusion",
    "href": "posts/2022-10-21-how-i-created-this-blog/index.html#conclusion",
    "title": "How I created this Blog",
    "section": "Conclusion",
    "text": "Conclusion\nSetting up the blog was not really hard, but it took some time for me. Hopefully, this guide contains some shortcuts for you. Happy blogging!"
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html",
    "href": "posts/2022-10-28-matrix-multiplication/index.html",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "Since matrix multiplication is a big thing for deep learning and visualizations like http://matrixmultiplication.xyz/ were a bit to fast for me to properly re-understand what I learned in highschool, I decided dive in more systematically without falling into the trap of learning lots of math before continuing with deep learning. This will be short and sweet:\nThis has been done a million times before already, but nonetheless, let me explain what I learned along the way."
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#takeaways-from-khan-academy",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#takeaways-from-khan-academy",
    "title": "Matrix Multiplication",
    "section": "Takeaways from Khan Academy",
    "text": "Takeaways from Khan Academy\nOne thing I was struggling with was to intuit the dimensions of the target matrix.\nLet‚Äôs assume two matrixes: \\(A = (m \\times n)\\) and \\(B = (n \\times k)\\)\nThis picture from Khan Academy sums it all up for me:\n Illustration by Khan Academy CC BY-NC-SA 3.0 US  Note: All Khan Academy content is available for free at (www.khanacademy.org)‚Äú\nTherefore, a matrix multiplication is defined if the number of columns of matrix \\(A\\) matches the number of rows of matrix \\(B\\).\nThe resulting matrix \\(C = A \\times B\\) has the same number of rows as matrix \\(A\\) and the same number of columns as matrix \\(B\\)."
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#implementing-matrix-multiplication-in-python-from-scratch",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#implementing-matrix-multiplication-in-python-from-scratch",
    "title": "Matrix Multiplication",
    "section": "Implementing Matrix Multiplication in Python from scratch",
    "text": "Implementing Matrix Multiplication in Python from scratch\nOnce that was done, I decided to implement matrix multiplication in python. I found this tutorial which provided me with the task and some guidance along the way, especially on a few things in python.\nAs a starting point, here are 2 matrixes that we want to multiply (example from tutorial sightly adjusted):\n\nimport numpy as np\nnp.random.seed(27)\nA = np.random.randint(1,10,size = (4,3))\nB = np.random.randint(1,10,size = (3,2))\nprint(f\"Matrix A:\\n {A}\\n\")\nprint(f\"Matrix B:\\n {B}\\n\")\n\nMatrix A:\n [[4 9 9]\n [9 1 6]\n [9 2 3]\n [2 2 5]]\n\nMatrix B:\n [[7 4]\n [4 1]\n [6 4]]\n\n\n\nThis is the final result, we want to re-implement from scratch:\n\nA@B\n\narray([[118,  61],\n       [103,  61],\n       [ 89,  50],\n       [ 52,  30]])\n\n\n\nIndexing in Python\nMaybe this is too obvious for many, but I find it worth noting, that the sequence in which python addresses arrays (or tensors) is first by row, than by column. What do I mean by saying that?\nWhen you want to index into an array, you do this by array_name[row:column], for example A[1,2] return 6, it is the second line (which is index 1 when starting to count at 0), and the third column (which is index 1 when starting to count at 0):\n\nA[1,2]\n\n6\n\n\nIs there a way to not only remember this, but to also understand this? Yes, I think so: The most basic array (tensor) is a list (rank 1 tensor), which we can think of as one row of numbers. Therefore, the first index represents the row. You can think of a 2-dimensional array (a rank 2 tensor) as adding the columns to a row of numbers (by adding more rows), therefore the second index represents the columns. Hence to access an element in a 2D-array (rank-2 tensor), this is done by array_name[row:column].\nWhy do we think about indexing? First, to determine if a matrix multiplication is defined, we need to find the dimensions of the matrixes, and later on we need to access the matrix content for the calculation.\nTo access a complete row or column, we use:\n\nFor a row: array_name[row, : ] or the short form array_name[row]\nFor a column: array_name[ : ,column]\n\nThis means: We access a specific row or column by index, and from the other dimension, we access all elements. For example:\n\n# accessing the first row of matrix A\n\nA[0] #same as A[0,:]\n\narray([4, 9, 9])\n\n\n\n# accessing the first column of matrix B\n\nB[:,0]\n\narray([7, 4, 6])"
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#constructing-a-target-matrix-of-zeros",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#constructing-a-target-matrix-of-zeros",
    "title": "Matrix Multiplication",
    "section": "Constructing a target matrix of zeros",
    "text": "Constructing a target matrix of zeros\nThe \\(C\\) target matrix has the same number of rows as A and the same number of columns of B, so in our example that is a matrix with 4 rows and 2 columns:\n\nnp.zeros((4, 2), dtype = int)\n\narray([[0, 0],\n       [0, 0],\n       [0, 0],\n       [0, 0]])\n\n\nThe number of rows is the length of a column, therefore, to get the number of rows of matrix A, we can write:\n\nlen(A[:,0]) #i.e. the length of the first column\n\n4\n\n\nSimilarly, the number of elements in a row if the number of columns, Therefore, the number of columns of B is:\n\nlen(B[0]) #the number of entries in the first row\n\n2\n\n\nWhile to above is correct, there is a more elegant way to write this. Each array (tensor) has an attribute .shape which tells us how many rows and columns an array has (notice the sequence in the tuple: (row,column)):\n\nprint(A.shape)\nprint(B.shape)\n\n(4, 3)\n(3, 2)\n\n\nTherefore, we can re-write:\n\nprint(f'Number of rows in matrix A: {A.shape[0]}') \nprint(f'Number of columns in matrix B: {B.shape[1]}')\n\nNumber of rows in matrix A: 4\nNumber of columns in matrix B: 2\n\n\nNow we can generically construct the target matrix \\(C\\):\n\nC = np.zeros((A.shape[0], B.shape[1]), dtype = int)\nC.shape\n\n(4, 2)"
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#exercise-implement-matrix-multiplication-with-numpy-arrays",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#exercise-implement-matrix-multiplication-with-numpy-arrays",
    "title": "Matrix Multiplication",
    "section": "Exercise: Implement Matrix Multiplication with numpy arrays",
    "text": "Exercise: Implement Matrix Multiplication with numpy arrays\nImplement a function multiply_matrix(A,B) which does the following:\n\nAccept two matrices, A and B, as inputs.\nCheck if matrix multiplication between A and B is valid, if not raise an error.\nIf valid, multiply the two matrices A and B, and return the product matrix C.\n\n\ndef multiply_matrix(A,B):\n    \n    if A.shape[1] != B.shape[0]:\n        raise ValueError('Number of columns of A and number of rows of B do not match')\n    \n    C = np.zeros((A.shape[0], B.shape[1]), dtype=int)\n\n    for row in range(C.shape[0]):\n        for column in range(C.shape[1]):\n            for step in range(A.shape[1]):\n                C[row, column] += A[row, step] * B[step, column]\n    \n    return C\n\nC1 = multiply_matrix(A, B)\nC1\n\narray([[118,  61],\n       [103,  61],\n       [ 89,  50],\n       [ 52,  30]])\n\n\n\nC2 = A@B\nassert np.array_equal(C1, C2)"
  },
  {
    "objectID": "posts/2022-10-28-matrix-multiplication/index.html#exercise-implement-matrix-multiplication-with-tensors",
    "href": "posts/2022-10-28-matrix-multiplication/index.html#exercise-implement-matrix-multiplication-with-tensors",
    "title": "Matrix Multiplication",
    "section": "Exercise: Implement Matrix Multiplication with tensors",
    "text": "Exercise: Implement Matrix Multiplication with tensors\nJust for the fun of it, let‚Äôs re-implement the same with pytorch tensors. It turns out it same, same, but a little different:\n\nimport torch\n\ntorch.manual_seed(27) #https://pytorch.org/docs/stable/notes/randomness.html\nX = torch.randint(1,10,size = (4,3)) #https://pytorch.org/docs/stable/generated/torch.randint.html\nY = torch.randint(1,10,size = (3,2))\nprint(f\"Matrix X:\\n {X}\\n\")\nprint(f\"Matrix Y:\\n {Y}\\n\")\n\nMatrix X:\n tensor([[1, 1, 5],\n        [8, 8, 6],\n        [1, 7, 1],\n        [4, 4, 1]])\n\nMatrix Y:\n tensor([[7, 6],\n        [3, 7],\n        [9, 5]])\n\n\n\n\nZ = torch.zeros((4, 2), dtype = int)\nZ\n\ntensor([[0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0]])\n\n\n\nZ.shape\n\ntorch.Size([4, 2])\n\n\n\ndef multiply_matrix_torch(A,B):\n    \n    if A.shape[1] != B.shape[0]:\n        raise ValueError('Number of columns of A and number of rows of B do not match')\n    \n    C = torch.zeros((A.shape[0], B.shape[1]), dtype=int)\n\n    for row in range(C.shape[0]):\n        for column in range(C.shape[1]):\n            for step in range(A.shape[1]):\n                C[row, column] += A[row, step] * B[step, column]\n    \n    return C\n\nZ1 = multiply_matrix_torch(X, Y)\nZ1\n\ntensor([[ 55,  38],\n        [134, 134],\n        [ 37,  60],\n        [ 49,  57]])\n\n\n\nZ2 = X@Y\nassert torch.equal(Z1, Z2) == True\n\nThat concludes the ‚Äúexploration‚Äù of matrix multiplication, I learned a lot along the way :)."
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html",
    "href": "posts/2022-11-05-kaggle-titanic/index.html",
    "title": "My First Kaggle Competition: Titanic",
    "section": "",
    "text": "For more practical experience with gradient descent, I decided to participate in the Titanic Competition. Here is how I did it and what I learned.\nI took the following approach:"
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html#installing-kaggle",
    "href": "posts/2022-11-05-kaggle-titanic/index.html#installing-kaggle",
    "title": "My First Kaggle Competition: Titanic",
    "section": "Installing Kaggle",
    "text": "Installing Kaggle\nGetting ready for the Kaggle competition requires registering for the competition (a few clicks on the kaggle website), and installing kaggle on your local machine. The following is based on the Live-Coding Session 7 and the related official topic in the forums.\nThe first step is to install kaggle:\npip install --user kaggle\nAs a result, the following warning is displayed: The script kaggle is installed in '/home/<your user>/.local/bin' which is not on PATH. This means that the you need to add the path to the PATH-variable. This is done by adding the following line to the .bashrc-file and restarting the terminal:\nPATH=~/.local/bin:$PATH\n\nNote: To display the current PATH-variable use: echo $PATH\n\nAs a result, typing the kaggle-command on the command line works, but the next error shows up (as expected): OSError: Could not find kaggle.json. Make sure it's located in /home/chrwittm/.kaggle. Or use the environment method.\nThis means that you cannot authorize against the kaggle platform. To solve this, download your personal kaggle.json On the kaggle website, navigate to: ‚ÄúAccount‚Äù and click on ‚ÄúCreate New API Token‚Äù. As a result, the kaggle.json is downloaded.\nCopy the kaggle.json-file into the .kaggle-directory in your home directory.\nTyping the kaggle-command on the command line gives you the final clue as to what is missing: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/chrwittm/.kaggle/kaggle.json'\nTherefore, type:\nchmod 600 /home/<your user>/.kaggle/kaggle.json\nTyping the kaggle-command on the command line again confirms: We are in business :)"
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html#downloading-the-dataset",
    "href": "posts/2022-11-05-kaggle-titanic/index.html#downloading-the-dataset",
    "title": "My First Kaggle Competition: Titanic",
    "section": "Downloading the dataset",
    "text": "Downloading the dataset\nTo download the dataset, run the following command (which you can also find on the kaggle website):\nkaggle competitions download -c titanic\nAs a result, the file titanic.zip is downloaded.\nTo unzip type:\nunzip titanic.zip\nDoing this for the first time, this resulted in an error: /bin/bash: unzip: command not found\nTo install zip and unzip, type:\nsudo apt-get install zip\nsudo apt-get install unzip\nAs a result, unzipping works, and we have a dataset to work with :).\n\nimport pandas as pd\n\ntrain = pd.read_csv(\"train.csv\")\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S"
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html#implementing-a-fast.ai-tabular-learner",
    "href": "posts/2022-11-05-kaggle-titanic/index.html#implementing-a-fast.ai-tabular-learner",
    "title": "My First Kaggle Competition: Titanic",
    "section": "Implementing a Fast.ai Tabular Learner",
    "text": "Implementing a Fast.ai Tabular Learner\nThe goal was not to create a perfect submission, but to simply train a model as fast as possible to\n\nget a baseline\nto get to know how a kaggle competition works (remember, this is my first one)\n\nTherefore, I created a dataloaders as shown in lesson 1 or in the docs by sorting the variables into categorical or continuos one, excluding irrelevant ones).\n\nNote 1: In this blog post, I am presenting the steps in a fast-forward way, here is the original notebook.\n\n\nNote 2: When writing this up, I was not able to 100% re-produce the same results, but basically this is how the story went.\n\n\nfrom fastai.tabular.all import *\n\npath = \".\"\n\ndls = TabularDataLoaders.from_csv('train.csv', path=path, y_names=\"Survived\",\n    cat_names = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked'],\n    cont_names = ['Age', 'Fare'],\n    procs = [Categorify, FillMissing, Normalize])\n\nNow we can train a model:\n\nlearn = tabular_learner(dls, metrics=accuracy)\nlearn.fit_one_cycle(10) #change this variable for more/less training\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.548652\n      0.315984\n      0.640449\n      00:00\n    \n    \n      1\n      0.454461\n      0.325496\n      0.640449\n      00:00\n    \n    \n      2\n      0.373511\n      0.289948\n      0.640449\n      00:00\n    \n    \n      3\n      0.319270\n      0.251090\n      0.640449\n      00:00\n    \n    \n      4\n      0.280473\n      0.196879\n      0.640449\n      00:00\n    \n    \n      5\n      0.249269\n      0.173640\n      0.640449\n      00:00\n    \n    \n      6\n      0.225535\n      0.152192\n      0.640449\n      00:00\n    \n    \n      7\n      0.207350\n      0.141283\n      0.640449\n      00:00\n    \n    \n      8\n      0.192223\n      0.137462\n      0.640449\n      00:00\n    \n    \n      9\n      0.180697\n      0.137344\n      0.640449\n      00:00\n    \n  \n\n\n\nWith this learner, we can make the predictions on the test-dataset.\n\ntest = pd.read_csv(\"test.csv\")\n\n# replacing null values with 0\ntest['Fare'] = test['Fare'].fillna(0)\n\n# create Predictions as suggested here:\n# https://forums.fast.ai/t/tabular-learner-prediction-using-data-frame/90534/2\ntest_dl = learn.dls.test_dl(test)\npreds, _ = learn.get_preds(dl=test_dl)\n\ntest['Survived_pred'] = preds.squeeze()\ntest.head()\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n      Survived_pred\n    \n  \n  \n    \n      0\n      892\n      3\n      Kelly, Mr. James\n      male\n      34.5\n      0\n      0\n      330911\n      7.8292\n      NaN\n      Q\n      0.064765\n    \n    \n      1\n      893\n      3\n      Wilkes, Mrs. James (Ellen Needs)\n      female\n      47.0\n      1\n      0\n      363272\n      7.0000\n      NaN\n      S\n      0.454887\n    \n    \n      2\n      894\n      2\n      Myles, Mr. Thomas Francis\n      male\n      62.0\n      0\n      0\n      240276\n      9.6875\n      NaN\n      Q\n      -0.025921\n    \n    \n      3\n      895\n      3\n      Wirz, Mr. Albert\n      male\n      27.0\n      0\n      0\n      315154\n      8.6625\n      NaN\n      S\n      -0.015690\n    \n    \n      4\n      896\n      3\n      Hirvonen, Mrs. Alexander (Helga E Lindqvist)\n      female\n      22.0\n      1\n      1\n      3101298\n      12.2875\n      NaN\n      S\n      0.508172\n    \n  \n\n\n\n\nInterpreting the values in column Survived_pred is important, because we need to turn these values into 0 and 1 for the submission. The submission file should only have the columns PassengerId and Survived. For the first submission, I did not worry about it too much and simply picked a value 0.5. (Let‚Äôs come back to that a little later)\n\nthreshold = 0.5 #change this variable for more/less training\ntest['Survived'] = [ 1 if element > threshold else 0 for element in preds.squeeze()]\n\nsubmission1 = test[['PassengerId', 'Survived']]\nsubmission1.to_csv('submission1.csv', index=False)\n\nI uploaded the results, and they were better then random ;) - Score 0.73923\n\nThe score is not great, but the whole point was to get a baseline as quickly as possible, and to ‚Äúplay the whole kaggle game‚Äù. Actually, the fact that I produced this result in about 1-2 hours felt pretty good :).\n\nNote: Running this notebook, I got a score of 0.75119, I am not sure, what caused the difference‚Ä¶ but better is always good ;)\n\nSo how can we improve the score? More training, interpreting the results differently? As it turns out: Both.\nLet‚Äôs look at the distribution of Survived_pred:\n\ntest.Survived_pred.hist();\n\n\n\n\nAs it turned out, setting my threshold to 0.6 created a better result: Score: 0.74162. (this I could not reproduce with this notebook while writing up the blog post)\nAlso more training, produced better results, running for 50 cycles, resulted in a lower loss and a better result. Training with 50 cycles and threshold 0.7, this was the result: Score: 0.76794 (with this notebook 0.77033)\nSo there is some randomness when training, and it is important to properly interpret the results. Getting about 77% right with this simple approach is not to bad."
  },
  {
    "objectID": "posts/2022-11-05-kaggle-titanic/index.html#re-implementing-the-excel-model",
    "href": "posts/2022-11-05-kaggle-titanic/index.html#re-implementing-the-excel-model",
    "title": "My First Kaggle Competition: Titanic",
    "section": "Re-Implementing the Excel Model",
    "text": "Re-Implementing the Excel Model\nAfter the quick win with Fast.AI, I decided to re-implement what Jeremy did in the Excel in video lecture 3 to predict the survivors. Let‚Äôs see how it performs against the Fast.AI tabular learner.\nSince that involved quite a bit of code, let me simply link to notebook and discuss the learnings / results.\nAs it turned out:\n\nI had to do a bit of data cleansing.\nThe feature engineering took some time which taught me some general python lessons.\nImplementing the optimizer was a nice exercise, revisiting gradient descent and matrix multiplication, and doing some hands-on work with tensors.\n\nThe first model with just one layer scored 0.75837, even better than the my Fast.AI baseline, but not quite as good as the optimized version.\nThe next iteration with 2 and 3 layers scored better:\n\nScore: 0.77033 (2-layers)\nScore: 0.77272 (3-layers)\n\n\nThis was quite surprising: The self-written algorithm is better than the Fast.AI one, any ideas why that would be?\nNonetheless, it seems to hit a ceiling at 77%, and it would make sense to dive deeper into tabular data, but that is for another time. My goal was not to optimize the competition result, but to participate in my first kaggle competition, and to re-visit the topic of gradient descent and matrix multiplication. I will most likely return to this dataset/challenge in the future."
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html",
    "href": "posts/2022-11-26-mnist/index.html",
    "title": "MNIST, the ‚ÄòHello World‚Äô of Computer Vision",
    "section": "",
    "text": "After Cat vs.¬†Dog, this is the next challenge for me in computer vision: Building on chapter 4 of the book, I challenged myself to implement a model based on the MNIST dataset, as recommended as further research.\nThis challenge is also available as a Kaggle competition, and I found this bit of competitive spirit to add some spice to the project. Additionally, it broadened the spectrum of implementation topics, because training a model is one thing, but using it for meaningful predictions in equally important and also required some effort. Last, but not least, submitting the results was a nice way to check if the results are actually correct. As predicted: ‚ÄúThis was a significant project and took you quite a bit of time to complete! I needed to do some of my own research to figure out how to overcome some obstacles on the way‚Äù.\nI took an iterative approach, following a similar path as for working on the Titanic-Challenge:\nIt was a challenging project, and I learned a lot on the way. Below are some key points and learnings."
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html#the-fast.ai-version-without-a-submission",
    "href": "posts/2022-11-26-mnist/index.html#the-fast.ai-version-without-a-submission",
    "title": "MNIST, the ‚ÄòHello World‚Äô of Computer Vision",
    "section": "The Fast.AI version without a submission",
    "text": "The Fast.AI version without a submission\nBy now this is pretty straight-forward for me. I just copy&pasted a few lines of code to do the training, and I was able to create a decent model very quickly in this notebook.\nThe catch with this version is, however, that it is not ready for the mass data load: 28.000 predictions need to be done in the competition - something which I addressed in my second iteration.\nAdditionally, I found it interesting that the MNIST dataset was already pushing the limits of my laptop: The training time of about 40 minutes was ok, but it is already quite a burden if it needs to be done multiple times. Moving the learning to Paperspace, training on a free GPU, was 10x faster (no surprise). Since I like to still have everything locally, it is quite convenient moving files back and forth via git, also for the .pkl-files. This way the training can be done with GPU, and the inference can be done locally. Interestingly, in all my other notebooks, local performance was not an issue. (But I expect that to change in future other projects)"
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html#resubmitting-working-with-the-csv-files",
    "href": "posts/2022-11-26-mnist/index.html#resubmitting-working-with-the-csv-files",
    "title": "MNIST, the ‚ÄòHello World‚Äô of Computer Vision",
    "section": "Resubmitting: Working with the csv-files",
    "text": "Resubmitting: Working with the csv-files\nI found not very elegant to just convert the csv-files to png-images. That seems convenient, but a bit wasteful. Therefore, I re-implemented the process this notebook.\nIt was surprisingly difficult to convert the data into the right formal in memory so that the learner would accept the image. But finally I was able to convert a PIL.Image.Image to fastai.vision.core.PILImage. As usual with these things, once it was done, it looks easy.\nNot surprisingly, but a nice way to verify the result, the submission score was the same:"
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html#my-first-submission",
    "href": "posts/2022-11-26-mnist/index.html#my-first-submission",
    "title": "MNIST, the ‚ÄòHello World‚Äô of Computer Vision",
    "section": "My first submission",
    "text": "My first submission\nWhen I first downloaded the Kaggle data, I was quite surprised to see that the download did not contain any image files, but just 2 large csv-files. Since I only knew how to handle images, I simply converted the data to png-images in this notebook.\nOnce that was done, I could take the model trained before in my first notebook to make my first submission. In this notebook, I took the converted images and collected the predictions. I found the result of 99.4% quite impressive."
  },
  {
    "objectID": "posts/2022-11-26-mnist/index.html#the-from-scratch-version",
    "href": "posts/2022-11-26-mnist/index.html#the-from-scratch-version",
    "title": "MNIST, the ‚ÄòHello World‚Äô of Computer Vision",
    "section": "The from-scratch version",
    "text": "The from-scratch version\nDoing it all from scratch was an interesting learning exercise because I think that I already had a good understanding of what needed to be done even before implementing it. But, as it turns out, there is this tremendous difference between thinking that you understood it, and actually implementing it. There is a lot of fine print, and you have to pay attention to the details: Formatting the data, getting it into the correctly shaped tensors, and implementing the gradient descent. Irrespective of what I had learned/understood before, this has greatly deepened and solidified by implementing the MNIST challenge.\nSome minor mysteries remain, which I also documented in the notebook, if you can guide me how to fix them, please let me know.\nThe finale result of my from scratch-version is not up to the first implementation with resnet18, but I am proud of it for other reasons ;)."
  },
  {
    "objectID": "posts/2022-11-27-rebuilding-quarto-blog/index.html",
    "href": "posts/2022-11-27-rebuilding-quarto-blog/index.html",
    "title": "When disaster strikes: Re-building a Quarto Blog",
    "section": "",
    "text": "For the last 2 months I have been a proud writer of this blog, until yesterday disaster struck: Upon publishing of my MNIST-blog post via the usual quarto publish gh-pages, I received the following error message üò®:\nThe publishing was not completed, and my blog only showed a naked header, but no posts any more, essentially everything was gone üò± (at least online)\nTrying to google a quick fix did not reveal any real result. Due to lack of time, I had to (officially) stop for the day, but back in my mind, this was really nagging me‚Ä¶"
  },
  {
    "objectID": "posts/2022-11-27-rebuilding-quarto-blog/index.html#what-is-needed-to-re-build-a-quarto-blog",
    "href": "posts/2022-11-27-rebuilding-quarto-blog/index.html#what-is-needed-to-re-build-a-quarto-blog",
    "title": "When disaster strikes: Re-building a Quarto Blog",
    "section": "What is needed to Re-build a Quarto Blog?",
    "text": "What is needed to Re-build a Quarto Blog?\nAs I kept thinking about this, the error message clearly pointed to something on my local machine. Additionally, I previously posted on how to avoid disaster, so what would be the best way to re-build everything?\nBut let‚Äôs think through the matter: What do you actually need to re-build a Quarto blog? You only need your posts-directory and a few other files (the ones mentioned here).\nThis is important, so let me re-phrase this: On your GitHub repo, there are (should be) 2 versions of your blog:\n\nIn the main-branch, you store your source-files:\n\nThe Jupyter notebooks, markdown files, some pictures used within your posts\nThe configuration files: Some .yml-, .qmd- and .css-files\n\nThe gh-pages branch gets generated to contain the rendered versions of your posts: When you check out your _site directory, it contains a file structure similar to the posts-directory in your main-branch, but the _site directory deals in html-, xml-, and json-files.\n\nTherefore, to re-build your site, as far as I understand it, you only need the content of your main branch, and the content of the gh-pages gets generated once you run quarto publish gh-pages."
  },
  {
    "objectID": "posts/2022-11-27-rebuilding-quarto-blog/index.html#re-building-my-quarto-blog",
    "href": "posts/2022-11-27-rebuilding-quarto-blog/index.html#re-building-my-quarto-blog",
    "title": "When disaster strikes: Re-building a Quarto Blog",
    "section": "Re-building my Quarto Blog",
    "text": "Re-building my Quarto Blog\nWith the above in mind, here is what I did:\n\nI moved my local copy of the blog‚Äôs repo‚Äôs main branch to my temp folder (the milder version of deleting it)\nI re-cloned the repo (the main branch): git clone git@github.com:chrwittm/chrwittm.github.io.git\nI re-published the blog: quarto publish gh-pages\n\nAnd viol√†: My blog was back online. üòÉ"
  },
  {
    "objectID": "posts/2022-11-27-rebuilding-quarto-blog/index.html#conclusion",
    "href": "posts/2022-11-27-rebuilding-quarto-blog/index.html#conclusion",
    "title": "When disaster strikes: Re-building a Quarto Blog",
    "section": "Conclusion",
    "text": "Conclusion\nFirst of all: With these kind of seeming disasters: Sit back and relax: Is this a big deal? Well my blog was down, who cared? Probably I cared the most about it - my ego üòâ.\nDon‚Äôt panic: Think through a problem: What could be the root cause, from what angle can you approach the problem?\nI think, the solution is actually really nice. The setup with the 2 branches has some nice redundancy built-in, and without having tried it before, the disaster recovery performed very well.\nThe solution also reminded me of the fast-setup approach, which Jeremy discussed in the Live-Coding session 1: You should spend time using your tools, not configuring them. Quarto, at least to me, nicely proved that point that even if something goes wrong, you can quickly recover.\nHappy blogging!"
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html",
    "title": "Wrapping-up Lesson 3",
    "section": "",
    "text": "Lesson 3 took me a while to rework, because it churned out quite a few interesting projects:\nAlongside these 3 main activities, I also started a blog on my machine learning journey, which is based on Quarto.\nLet me summarize what I have done and learned."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html#gradient-descent",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html#gradient-descent",
    "title": "Wrapping-up Lesson 3",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nThe main focus of lesson 3 for me was/is gradient descent, which I found pretty easy to understand on a high level:\n\nCalculate the predictions and the loss (forward-pass)\nCalculate the derivatives of the parameters (i.e.¬†how does changing the parameters change the loss) (backward-pass)\nUpdate the parameters (via the learning rate)\nDon‚Äôt forget to initialize the gradients\nRestart\n\nHowever, in the actual implementation and its simplicity, there is a lot of magic, which I tried to unpack for myself. Working through Jeremy‚Äôs notebook ‚ÄúHow does a neural net really work?‚Äù, I tried to not only think through the concept, but also to visualize it. The result is available as\n\na blog post\na forum post\na Kaggle notebook in which you can easily also play with the visualizations interactively (by copying and running the notebook)\na GitHub notebook\n\nI was excited and honored to read that Ben, a fellow Fast.AI student, created even better visualizations building on my work. I highly recommend playing with it and also checking out his other projects.\nWhile I truly love the Fast.AI content, I also need to mention the great video ‚ÄúThe spelled-out intro to neural networks and backpropagation: building micrograd‚Äù from Andrej Karpathy, which dives at least one level deeper. If there is one key takeaway from this video, it is this one: ‚ÄúA single gradient tells us the effect changing a parameter has on the loss‚Äù. This insight is powerful, and somehow it tends to get lost from my point of view, because you either have a very complex model with many, many parameters so that this is difficult to grasp, or you have a seemingly simple model in which you mix up the slope of the quadratic with a gradient. Implementing the visualization of gradient descent was also about building the intuition of what is actually going on under to hood."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html#the-titanic-competition",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html#the-titanic-competition",
    "title": "Wrapping-up Lesson 3",
    "section": "The Titanic Competition",
    "text": "The Titanic Competition\nInspired by the Excel-based version of the Titanic Competition, I decided to enter the kaggle competition. As with many good project, this resulted in a few other mini-projects:\n\nThe logistics on how a kaggle competition actually works, which included installing kaggle on my local machine. The Live-Coding Sessions of the 2022 Fast.AI course are probably quite underrated (at least looking at the number of views they get on Youtube). I find them a great addition to the official lessons because the tackle side problems like installing kaggle (in Live-Coding Session 7 and the related official topic in the forums) which otherwise would set you back some hours (or more). A big shout-out for these sessions!\nRevisiting matrix multiplication. Apart from the math, this also was about some python basics for me. While the result of implementing matrix multiplication from scratch has probably been done a million times, it still taught me some valuable lessons.\n\nIn the actual Titanic Competition, I did not focus too much on submitting a perfect result, but I rather aimed at re-visiting/solidifying the topic of gradient descent by replicating the actual lesson content. I built for following 2 notebooks\n\nThe first one uses a Fast.AI tabular learner to create a baseline while getting the know the data.\nNext, I re-implemented the Excel-based version from the video in python in this notebook.\n\nWhile my final high score of 77.2% is far away from perfect, I decided to come back to this competition another time, focusing more on the content, not just on gradient descent (like this time)."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html#mnist-the-hello-world-of-computer-vision",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html#mnist-the-hello-world-of-computer-vision",
    "title": "Wrapping-up Lesson 3",
    "section": "MNIST, the ‚ÄòHello World‚Äô of Computer Vision",
    "text": "MNIST, the ‚ÄòHello World‚Äô of Computer Vision\nAs Jeremy points out at the end of lesson 3, this lesson corresponds to chapter 4 of the book. Indeed, it covers very similar topics, but the example used is the light version of the MNIST dataset (which only contains 3s and 7s). Following the recommendation for further research, I implemented a model for the complete MNIST dataset. As predicted: ‚ÄúThis was a significant project and took you quite a bit of time to complete! I needed to do some of my own research to figure out how to overcome some obstacles on the way‚Äù.\nAfter all the previous activities around gradient descent, the actual mechanics of what needed to be done were not too difficult. Nonetheless, I found the competition to be hard, because of the actual technicalities of the python implementation. Put differently, I think I could have easily written a good specification on how to solve the MNIST competition, but actually doing it yourself is a different thing.\nSeemingly simple tasks like converting the csv-files to images, converting a PIL image to a Fast.AI PIL image, or getting the tensors in the right shape took me some time to implement in python. I am still struggling with python as a language but I am seeing good progress, and the only way to improve is to keep coding."
  },
  {
    "objectID": "posts/2022-11-30-wrapping-up-lesson3/index.html#wrapping-up-lesson-3",
    "href": "posts/2022-11-30-wrapping-up-lesson3/index.html#wrapping-up-lesson-3",
    "title": "Wrapping-up Lesson 3",
    "section": "Wrapping-up Lesson 3",
    "text": "Wrapping-up Lesson 3\nWhile I could improve the results of my projects, both for Titanic and MNIST, it feels like it would be some way over-optimizing. I did not enter the competitions to win, but to learn about gradient descent. Having spent the last 8 weeks with my lesson 3-projects (and allowing myself to get somewhat side-tracked), I feel it is time to move on to the next lesson. I am looking forward to the next challenging projects!"
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "",
    "text": "Based on the Fast.AI lesson 4, I transferred the approach from Jeremy‚Äôs notebook ‚ÄúGetting started with NLP for absolute beginners‚Äù to the Kaggle competition ‚ÄúNatural Language Processing with Disaster Tweets‚Äù. This post describes my approach and the key learnings.\nMy approach was the following: Classifying tweets or patent phrases is essentially ‚Äúthe same‚Äù task:"
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#key-learnings",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#key-learnings",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Key Learnings",
    "text": "Key Learnings\nIn the last about 4 week I probably trained 250+ model and made 70+ submissions, trying to build up intuition on what works and what not. Therefore, please treat this as an empirical report, not as a scientific paper. I try to back my claims with actual data of my model training, but sometimes I can only report on observations and I try to reason why these observations makes sense. Here are my key learnings:\n\nCleaning the data helps, both syntactically and semantically: Not only did I clean up special (rubbish) character, but also re-classified some tweets, mainly automatically.\nUpon cleaning the data, keep a close eye on what is noise and what is signal, for example, converting everything to lower case did not help, because I believe that it removed signal.\nHelping the model understand the data helps by using special tokens, for example by replacing URLs with a special token.\nUsing bigger models helps, on average moving from the small to the base to the large versions of the deberta models increased the score by a few tenth. However, for training large models on Kaggle, you need to apply some tricks not to run out of memory. Additionally, training bigger models is comparable time-consuming.\nSmall batch sizes help to train models more quickly.\nShowing the model more data then just the initial training set helps.\nOverall, the pre-trained models are already very good. My first version submission of the baseline notebook scored 0.81949, in the current final iteration my best submission scored 0.84676. That is an increase of only 2.7 percentage points. The bigger difference is the rank on the leaderboard. At the time of writing 0.81949 would have put me on 218/938 while 0.84278 put me on rank 34/938. If you deduct the submission which scored 100% (29 submission), I am pretty happy with rank 5. üòÄ\n\n\nOn a side note: I wonder how I would score by hand-labeling all tweets in the test set. Would I beat AI or would AI beat me? Find out at the end."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#syntactical-data-cleansing",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#syntactical-data-cleansing",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Syntactical Data Cleansing",
    "text": "Syntactical Data Cleansing\nThe dataset for the disaster tweets is far from perfect in terms of data quality, I guess it is just real world data üòâ. Even though the model was doing a very good job at classifying tweets even without any additional work, some data cleaning was called for.\nThere are quite a few non-sense special characters in the dataset, for example there are HTML representations spaces as %20, or leftovers of an unsuccessful unicode conversion, for example √õ¬™ which should be '. I did replace the rubbish characters where possible, and I simply deleted any other non-ascii characters. I suspect that more could be done here to, for example trying to reconstruct emojis, but I could not find a way to do that. (suggestions welcome)\nI did not take the route of converting everything to lowercase or removing stop-word, because it did not have any positive training effect (just the opposite!). My theory on this would be that I would have removed signal by doing these optimizations (more on that in a bit). On another train of thought, a modern language model should be able to deal with stop word, abbreviation etc. anyway, so why bother to put in time and effort to clean data where it is not required. What is your experience or opinion? Are these kinds of optimizations becoming obsolete as we move from classical machine learning to large language models?"
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#semantic-data-cleansing-correcting-incorrectly-labeled-tweets",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#semantic-data-cleansing-correcting-incorrectly-labeled-tweets",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Semantic Data Cleansing: Correcting incorrectly labeled Tweets",
    "text": "Semantic Data Cleansing: Correcting incorrectly labeled Tweets\nBrowsing through discussions on the competition, there was the claim that some tweets in the training set were not labeled correctly. As it turned out: That is true. But how to correct this without reading every tweet and potentially re-labeling it manually? (Something I definitely would not do!)\nAs it turned out, there are also tweet duplicates which are sometimes not labeled identically. These were pretty easy to catch:\n\nIntroducing a field which counts the occurrences of the tweets. (Since the duplicates were not 100% identical, for example, they contained different URLs, the counting was only possible after converting the URLs to special tokens (see below)).\nCreating a new field which contains the average tweet label\nRounding the label to get a majority vote\n\nThe only catch with this procedure is that tweets which have exactly one duplicate cannot be corrected this way. Well, what can you do about it, I had to re-label these ones manually."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#noise-vs.-signal",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#noise-vs.-signal",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Noise vs.¬†Signal",
    "text": "Noise vs.¬†Signal\nWhen introducing new data or removing data, you need to think about what is noise (i.e.¬†unwanted interference) and what is signal (i.e.¬†data which is helpful). Or putting it into other words: Which elements of the data help the model to learn and which elements create confusion or distraction? Let‚Äôs consider the following examples.\nInspired by the notebook ‚ÄúGetting started with NLP for absolute beginners‚Äù my first ‚Äúimprovement‚Äù was to concatenate the additional columns (keyword, location) of the dataset into the tweets. I was pretty disappointed that the result was worse then the baseline, and even with additional optimization (e.g.¬†special tokens) it was impossible to beat the baseline. Finally, I removed the additional fields, and voil√†, the result was immediately better! Why? The keywords added noise, not signal. The keyword ‚Äúcollision‚Äù, for example, is also assigned to many non-disaster tweets.\nAnother source of noise can be the URLs which are part of the tweets: The content of the URLs themselves is pretty random, especially for shortened ones. To make the URLs more meaningful for the language model, I tried the following:\n\nEnclosing the URLs with special tokens for URL beginning and URL end -> did not help\nRemoving the URLs completely -> better but not the best solution\nReplacing the URLs with a special token -> empirically the best solution\n\nWhen thinking about it, it makes sense: The fact that there is a URL in a tweet seems to contain signal, but the content of the URL (i.e.¬†random characters) does not.\nAs a final topic for noise vs.¬†signal, let‚Äôs consider capitalization. Even though suggested in many sources, converting the text to lower case did not yield a better result (quite the opposite!). Again, I could imagine that capitalization carries signal. Think of capitalization as shouting. Therefore, it makes sense not to convert everything to lower-case because you would remove signal from the tweets. Consider these 2 examples where there is a difference between ‚Äúburning‚Äù and ‚ÄúBURNING‚Äù:\n\nimport warnings, logging\n\nwarnings.simplefilter('ignore')\nlogging.disable(logging.WARNING)\n\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\ntokz = AutoTokenizer.from_pretrained('microsoft/deberta-v3-small')\n\ntokz.tokenize(\"The house is BURNING.\")\n\n['‚ñÅThe', '‚ñÅhouse', '‚ñÅis', '‚ñÅBURN', 'ING', '.']\n\n\n\ntokz.tokenize(\"The house is burning.\")\n\n['‚ñÅThe', '‚ñÅhouse', '‚ñÅis', '‚ñÅburning', '.']"
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#working-with-special-tokens",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#working-with-special-tokens",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Working with Special Tokens",
    "text": "Working with Special Tokens\nOne of the steps of syntactically cleaning the dataset was the removal of special characters, many of them being misrepresentations of unicode characters. There are, however, also meaningful special characters. In the tweets, not prominent the hashtag # and the mention @. Since these special characters carry signal, I decided to turn them into special tokens. This time, however, I did not replace the whole mention or keyword with a special token, but I wrapped the mentions in special tokens:\n\n[MB]: Mention Beginning\n[ME]: Mention End\n[HB]: Hashtag Beginning\n[HE]: Hashtag End\n\nAfter stating what I have been doing in full confidence, I have to admit that, at the time of coding, I do not really know what I was doing, I was rather implementing it in analogy to the two notebooks ‚ÄúGetting started with NLP for absolute beginners‚Äù and ‚ÄúIterate like a grandmaster‚Äù.\nWhile writing up this blog post I was doing a little bit of research on special tokens, which was not as straight-forward as I imagined it to be. Chatting with ChatGTP quite was insightful, and I learned about the usage of the predefined special tokens, which can be retrieved by inspecting the attribute tokz.all_special_tokens: (I hope the following is accurate)\n\nThe ‚Äú[CLS]‚Äù-special token (‚Äúclassification token‚Äù) is used to represent the entire input sequence in tasks such as text classification, sentiment analysis and named entity recognition etc. The token is added to the beginning of the input sequence and is then passed through the model along with the rest of the input. In the context of disaster tweets, if a tweet was ‚ÄúThere is a fire‚Äù, after adding the token, the tweet looked like this: ‚Äú[CLS] There is a fire‚Äù. In the final version of the notebook I did add the [CLS]-token to all the tweets, but I did not notice any improvement in model performance.\nThe ‚Äú[SEP]‚Äù-special token (‚Äúseparation token‚Äù) is used to separate multiple sentences or segments within a single input sequence. The token is added at the end of each sentence or segment, and it is used to indicate the end of one segment and the beginning of another. I did not add any [SEP]-tokens to the tweets because a tweet is a unit of its own. Interestingly, the tokens for . and [SEP] are not the same, which makes sense, because not any .-character is automatically a separator.\n\nThe ‚Äú[UNK]‚Äù-special token is used to represent unknown or out-of-vocabulary words that the model has not seen during training. During the preprocessing step, any word that is not in the vocabulary is replaced with the ‚Äú[UNK]‚Äù token. Somehow I would have expected the tokenizer to replace unknown words with [UNK], but that did not happen whatever I tried.\nThe ‚Äú[PAD]‚Äù-special token is used to pad the input sequences to a fixed length. A BERT (and deberta) model is a transformer-based model which requires that all input sequences have the same length before they can be passed through the model. I did not use this token, but I trust the inner mechanics of the model to take care of this.\nThe ‚Äú[MASK]‚Äù-special token is used in the pre-training process called Masked Language Model (MLM). In this task, a proportion of tokens in the input sequence is replaced with the ‚Äú[MASK]‚Äù token, and the model is trained to predict the original token that was replaced by the ‚Äú[MASK]‚Äù token. This pre-training process allows the model to learn the context of the words in the sentence and understand the relationship between words in the sentence. Since this is a token used in training, I did not use it in my notebooks."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#training-bigger-models",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#training-bigger-models",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Training Bigger Models",
    "text": "Training Bigger Models\nAfter I was done cleaning the data, I tried to work with bigger models, and the result is: Yes, size does matter. The improvements were not dramatically better, nonetheless significant. I worked with microsoft/deberta-v3-small, microsoft/deberta-v3-base and microsoft/deberta-v3-large. Here are the best scores by model (even though the training approaches are not 100% comparable):\n\nmicrosoft/deberta-v3-small: 0.83757\nmicrosoft/deberta-v3-base : 0.84002\nmicrosoft/deberta-v3-large: 0.84676\n\nUpgrading from the small model to the base model was a smooth process. The expected trade off between better results and longer training time materialized as expected. When moving to the large model, that process was not so smooth, because the kaggle runtime was running out of memory both in the GPU and on disk. Here is how I fixed it:\n\nFixing running out of GPU memory: Larger models require more memory in the GPU. This can easily be fixed by reducing the batch size. A small batch size may be a desired training parameter (as I will discuss later), however, if you do not want to increase the number of gradient descent steps, you can use gradient accumulation (also more on that in a later section).\nFixing running out of disk memory: The Hugging Face Trainer saves huge checkpoint files after every 500 training steps by default. When working with small batch sizes or larger numbers of epochs, this can exceed the allowed disk space by Kaggle. The easy fix is to disable the checkpoint file creation. This can be done with the parameter save_steps=-1 which you need to pass to the TrainingArguments class."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#gradient-accumulation",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#gradient-accumulation",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Gradient Accumulation",
    "text": "Gradient Accumulation\nAs presented in Live Coding Session 10, Gradient Accumulation is a technique to train large models on regular hardware.\nIf you run out of memory on a GPU, you need to decrease the batch size. With the same number of epochs this increases the number or optimization cycles. To train a larger model with the same number of iteration cycles, gradient accumulation can be used, because it results in updating the model only after x number of batches.\nAs it turns out, gradient accumulation can also easily be used with a Hugging Face Trainer, you just need to pass the parameter gradient_accumulation_steps to the TrainingArguments-class:\n\n    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n        num_train_epochs=epochs, weight_decay=0.01, report_to='none', gradient_accumulation_steps=gradient_accumulation_steps)\n\nWhen I first tried gradient accumulation, I was puzzled by the result. Smaller batch sizes yielded better results, and in my initial attempts, the bigger models even performed worse than small ones. This was, I assume by now, because I was accumulating the gradients too much.\nWhile gradient accumulation is for sure a good tool for some problems, it was not part of my best submissions because I also came to learn that the small batch sizes were more beneficial in training. What started as a perceived problem (being forced to lower the batch size), turned out to create better results and quicker training."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#implementing-metrics",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#implementing-metrics",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Implementing Metrics",
    "text": "Implementing Metrics\nBefore turning to the discussion of smaller batch sizes, let me first address the computation of metrics, because it will be useful when evaluating the batch size.\nI did not pay too much attention to this at first because implementing metrics (in addition to the loss) was not part of my baseline, and I was focussing on other topics first (as outlines above), but upon writing this post, I noticed the white spot.\nThe evaluation of the kaggle competition is based on the F1 score. Based on this tutorial leveraging the evaluate library, the implementation was surprisingly easy.\n\nImplement a function based on the pattern described in the tutorial or in the evaluate docs\nAs metrics, there is a lot to choose from: This is the full list.\n\nMy final metric implementation looks like this:\nimport numpy as np\nimport evaluate\n\ndef compute_metrics(eval_preds):\n    metric = evaluate.load(\"f1\")\n    preds, labels = eval_preds\n    #predictions = np.argmax(logits, axis=-1) #not needed\n    return metric.compute(predictions=preds, references=labels)\n\nPersonal note: With the disaster tweet dataset, the line predictions = np.argmax(logits, axis=-1) caused a type error. For solving it, this tutorial had exactly the right input I needed to debug the problem."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#using-small-batch-sizes",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#using-small-batch-sizes",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Using Small Batch Sizes",
    "text": "Using Small Batch Sizes\nWhat started as a curiosity got solidified when I found this tweet by Yan LeCun:\n\n\n\n\n\nTherefore, small batch sizes are a recognized way to train faster in a way which can generalize better. There seems to be a tradeoff between smaller batch size and training time per epoch because the GPU is not used as efficiently with smaller batch sizes. In my disaster tweet project, small batch sizes proved to be helpful.\nIs there a way to intuit why a small batch size is a good thing? Yes, I think so: When you think about the training set as book with 1000 pictures that we want to learn the labels of, the batch size symbolizes how many pictures you look at before you think about what you have seen. With a batch size of 100, you would look at 100 pictures, reflect (i.e.¬†doing the learning, or updating the model based on the gradients), look at another 100 pictures, reflect etc. Within an epoch, i.e.¬†looking though the whole book, you would reflect on what you have seen 10 times. At a batch size of 10, you would look at 10 picture, reflect, look at 10 more pictures, reflect‚Ä¶ Within an epoch, you would reflect 100 times (not 10 times), i.e.¬†you would actually do more learning instead of just looking at the pictures. Therefore, it makes sense that a smaller batch size can result in better and quicker training results.\nAnother way to thin about this: By lowering the batch size, the iteration speed is increased. Quoting Andrej Karpathy with some additions [in bracket]:\n\n‚Äú[When] dealing with mini batches the quality of our gradient is lower, so the direction is not as reliable, it‚Äôs not the actual gradient direction [compared to taking a larger batch], but the gradient direction is good enough even when it‚Äôs estimating on only 32 examples that it is useful and so it‚Äôs much better to have an approximate gradient and just make more steps than it is to evaluate the exact gradient and take fewer steps. So that‚Äôs why in practice this works quite well.‚Äù\n\nTo support the claim that smaller batch sized help/work, here is a comparison of training 4 epochs with different batch sizes. As a result, you can see that training time increases, but the training quality increases as well, so much that the model already shows signs of overfitting when training for more than 2 epochs at a batch size of 8 and below:\n\nYou can also see how the model is getting more confident about the predictions in the histograms: The smaller the batch size, the more the predictions peak at 0 and 1, and the number of uncertain cases decreases:\n\nI wonder how general this finding about smaller learning rates is‚Ä¶ At least I am now sensitive to the topic and I will continue to watch it."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#showing-the-model-more-data",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#showing-the-model-more-data",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Showing the model more data",
    "text": "Showing the model more data\nIn the notebook ‚ÄúIterate like a grandmaster‚Äù, Jeremy suggested for further improvement: ‚ÄúBefore submitting a model, retrain it on the full dataset, rather than just the 75% training subset we‚Äôve used here.‚Äù I was wondering how I could follow the advice, and I came up with 2 ideas. The first one did not work, the second did:\n\nTraining 3 models with differently shuffled training and validation sets. In the end the models would be averaged. I did not notice a significant effect, therefore I did not pursue this approach any further. When you think about it, it also makes sense, because in the end you are training identical model, because, if done right, even given different data, the model capabilities should converge to the same capabilities. Only if different training approaches had been used, the ensemble would help.\nTrain a model, then re-shuffle the dataset again to show the model a different training set and a different validation set. Therefore a generically pre-trained model becomes a pre-pre-trained model. In this particular case the deberta model was trained on tweet classification now. It actually worked, shuffling had a good positive effect.\n\nShuffling once: Notebook V5 scored 0.83481\nShuffling twice: Notebook V6 scored 0.83879\nShuffling a third time: Notebook V7 scored 0.84002\nShuffling more often had a negative effect, my notebook V8 scored only 0.83542, most likely due to over-fitting.\n\n\nTo cross check that the improved training results were not just caused by more training, I did the following experiment, training the small model at batch size 32:\n\nTraining 4 epochs: Score 0.82715\nTraining 6 epochs: Score 0.8345\nTraining 8 epochs: Score 0.83297\nTraining 10 epochs: Score 0.8345\nTraining 4 epochs, but re-shuffling 3 times and training 2 more epochs: Score 0.83634\n\nWhile the effect of ‚Äúmore training help‚Äù is definitively visible, the attempt including the reshuffling was the best result. This is not surprising, because the model can learn more by looking at more diverse data.\nDiscussing the question of how to use 100% of the training data on the forums lead to a third approach: First, I trained the model as ‚Äúas usual‚Äù, specifically with training for 2 epochs with batch size 4, afterwards I showed the model the full dataset for another epoch. The results were better than doing the re-shuffling approach. Again, as a disclaimer, the training approaches were not identical, so it is difficult to directly compare the results. Nonetheless, my best result of 0.84676 used the following training approach: Training the large model for 2 epochs at bs=4. Afterwards training another epoch with the full training set and at bs=4 but half the learning rate.\nIt would be interesting to systematically explore the matter. Maybe I will return to this another time."
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#conclusion",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#conclusion",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Conclusion",
    "text": "Conclusion\nWhen starting out with lessen 4, I did not expect it to become such an extended endeavor. Porting the notebook notebook ‚ÄúGetting started with NLP for absolute beginners‚Äù to the Kaggle competition ‚ÄúNatural Language Processing with Disaster Tweets‚Äù introduced me to many different aspects of natural language processing in particular and machine learning in general.\nMaybe the most profound learning is that natural language processing with pre-trained models is already very robust. Consider all the effort I put in to improve from a baseline of 0.81949 to 0.84676, where the majority of the improvement was due to better training parameters or bigger models. The dataset is challenging and an honest 100% score is impossible, because there is also quite some subjectivity in there, and quite a few tweets are ambiguous. Putting myself to the test: I tried to compete against my models, just relying on my pre-trained brain: Would I beat 84.6%?\nI must admit, I did not hand-label the whole dataset, but only the first 100 tweets of the test set. Then I compared my results to the leaked 100%-solution. My score was 81% (even worse than my baseline!) - so I lost the competition: AI beat me in this competition - who would have thought?"
  },
  {
    "objectID": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#credits-working-with-ai-as-a-team",
    "href": "posts/2023-01-17-nlp-with-disaster-tweets/index.html#credits-working-with-ai-as-a-team",
    "title": "Natural Language Processing with Disaster Tweets",
    "section": "Credits: Working with AI as a Team",
    "text": "Credits: Working with AI as a Team\nWhile I wrote this blog post myself (no longer a given these days‚Ä¶), I need to give credit to ChatGPT for helping me in the research and also for the implementation of some functions, especially the regular expressions. While not all the replies from ChatGPT were correct, they were mostly helpful and frequently pointed me in the right directions. I also enjoy just ChatGTP is a non-annoying website in the sense that is does not ask for cookies after each question, and it does not want me to sign up for newsletters etc. More time is spent on thinking and problem solving than on these distractions.\nThe title picture of this blog post was generated by DALL-E 2 upon the prompt: ‚Äútwitter logo on fire digital art‚Äù."
  },
  {
    "objectID": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html",
    "href": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html",
    "title": "Discovering Disaster Tweet Dataset Limitations",
    "section": "",
    "text": "What started out as a simple exercise to visualize model performance using a confusion matrix revealed that the training set contains lots of incorrectly labeled tweets, and that my trained model actually performed better than the score suggests.\n\n\nAfter I had published my first blog post on ‚ÄúNatural Language Processing with Disaster Tweets‚Äù), I realized that I had forgotten one element I planned to incorporate into the notebook: I wanted to visualize the model performance using a confusion matrix. Since the implementation was pretty straightforward, the subsequent data analysis will be the real content of this blog post."
  },
  {
    "objectID": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#implementing-the-confusion-matrix",
    "href": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#implementing-the-confusion-matrix",
    "title": "Discovering Disaster Tweet Dataset Limitations",
    "section": "Implementing the Confusion Matrix",
    "text": "Implementing the Confusion Matrix\nTo create the confusion matrix, we need the ground truth from the training data and the predictions from the model on the training data:\n\n# ground truth from the training data\ntrain_true = [int(x) for x in tok_ds['labels']]\n\n# model predictions on the training data\ntrain_preds = trainer.predict(tok_ds).predictions.astype(float)\ntrain_preds = [ 1 if element > 0.6 else 0 for element in train_preds.squeeze()]\n\nLeveraging scikit-learn, here is how I created the confusion matrix\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ncm = confusion_matrix(train_true, train_preds, labels=[0, 1])\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot()\n\nplt.show()\n\n\nThe confusion matrix above displays the best training result I could get using all the methods in my previous blog post/notebook: Training on the model microsoft/deberta-v3-large with 2 epochs at batch size 4 and afterwards another epoch on the full training set. The model nicely classifies many tweets correctly, but there are still 196 false-positives and 553 false negatives.\nThis results in a surprisingly high F1-score of 0.87691 - a lot higher than any of the F1 scores in my submissions.\nSimilar to the approach taken in lesson 2 (where we used the first trained model to find the picture which had the top losses), I reviewed at the incorrectly labeled tweets and found surprising results."
  },
  {
    "objectID": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#data-analysis-discovering-mislabeled-tweets",
    "href": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#data-analysis-discovering-mislabeled-tweets",
    "title": "Discovering Disaster Tweet Dataset Limitations",
    "section": "Data Analysis / Discovering Mislabeled Tweets",
    "text": "Data Analysis / Discovering Mislabeled Tweets\nInspecting the false positives and the true positives revealed the following:\n\nThe false positives (the model classified a tweet as disaster, but there was none), were indeed mostly incorrect predictions made by the model - even though there is quite some room for interpretation.\nThe false negatives (the model classified a tweet as non-disaster, but there was one), however, revealed a significant number of mislabeled tweets.\n\nThe following tweets for sure are no disaster tweets, nonetheless they are labeled as disasters in the training data:\n\n\n\n\n\n\n\n\n\nid\nTweet\nlabel\npred\n\n\n\n\n443\nShort Reading Apocalypse 21:1023 In the spirit the angel took me to the top of an enormous high mountain and‚Ä¶\n1\n0\n\n\n794\nChevrolet : Avalanche LT 2011 lt used 5.3 l v 8 16 v automatic 4 wd pickup truck premium b_\n1\n0\n\n\n1051\nI waited 2.5 hours to get a cab my feet are bleeding\n1\n0\n\n\n1239\npeople with a #tattoo out there.. Are u allowed to donate blood and receive blood as well or not?\n1\n0\n\n\n\nWhile all these tweets contain words which could potentially indicate a disaster (apocalypse, avalanche, bleeding, blood)\n\nthe first tweet is a quote from the bible,\nthe second one is related to a car,\nthe third one undoubtedly feels unpleasant,\nand fourth one is simply a question on donating blood.\n\nAs it turned out, these 4 examples are not the only ones. After having read through the IDs up to 1000 in the false negatives, I had found about 80 mis-labeled tweets."
  },
  {
    "objectID": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#re-training-a-model-using-corrected-labels",
    "href": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#re-training-a-model-using-corrected-labels",
    "title": "Discovering Disaster Tweet Dataset Limitations",
    "section": "Re-Training a model using corrected labels",
    "text": "Re-Training a model using corrected labels\nEven though re-labeling tweets is a tedious work, in this notebook, which is a copy of my previous one, I have added a new section which re-labels tweets based on the results in the first notebook.\nMy clear expectation was that model performance would go up because of the increased quality of the updated training set. The opposite, however, was the case: Model performance dropped from 84.6% to 84% üòÆ - what was going on?\nThe only explanation I have for this result is that both the training and the test data systematically contain mislabeled tweets. Reasons could be\n\nmisinterpretation of the tweets, some tweets are obviously ambiguous\nsimple error in the monotonous task of labeling data\nusing an outdated algorithm for labeling tweets - just speculating üòâ\n\nThe current latest version of the notebook (V8) further supports this claim. It has scored a little bit better (84.3%). This increase was caused by mislabeling a bunch of tweets: There are quite a few tweets in the dataset which follow this pattern: ‚Äú#FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps‚Äù. This tweet simply informs about a new FedEx policy, thus it is no disaster tweet from my point of view. But in the dataset 66% of tweets with this content are labeled as disasters. By accepting the majority rule, the score increased, even though the label is incorrect in my opinion.\nAll of this tells us that there is a problem in the dataset of the Kaggle competition, some of it may be subjective, other tweets are objectively mislabeled. Therefore, if you are working on the Kaggle competition for disaster tweets and the improvements you make to your model do not materialize in a better result, it might also be the dataset‚Äôs fault, not yours. Consider this: If your model would classify every tweet correctly in the test set, you would not score 100% because of the mislabeled tweets in the test set.\nBut I think we can gain some deeper insights than the simple observation that the dataset contains some issues."
  },
  {
    "objectID": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#conclusion",
    "href": "posts/2023-01-27-disaster-tweet-dataset-limitations/index.html#conclusion",
    "title": "Discovering Disaster Tweet Dataset Limitations",
    "section": "Conclusion",
    "text": "Conclusion\nWhen working in machine learning, we sometimes tend to focus a lot on the technology aspects, trying to tune hyperparameters, using bigger or more advanced models etc. In this case, the ground truth is the limiting factor to what the model can achieve. The pre-trained model has actually outperformed the data set by clearly pointing out mislabeled data. This shows that the pre-trained model understands quite a bit about language, not only the grammar, but also about the content. This is not super-surprising when you think about the task for the model. A language model is basically trained to predict the next word for a prompt, and we have re-trained it (the last layer or a bit more) to classify tweets. So it is not just trained on the classification task, but the model still understands quite a lot about the content about the tweets.\nThe (training) data is maybe the most important asset in machine learning, and it can come with quite a few problems: Not only can it be biased, difficult to obtain, hard to parse, it can also be simply wrong! So the true lesson is not to blindly trust the data, but thorough inspection of the data is always called for. By using faulty data, our model has become biased: It has learned from the mislabeled tweets, and I suspect it has become too sensitive when detecting disasters. When non-disaster tweets are classified as disasters in the training data, the model calls out a disaster when there is none - similar to if you called an ambulance for a slight cough. The pre-training, however, still seems to contain enough ‚Äúcommon sense‚Äù that the model can still call out mislabeled tweets.\nTherefore, the F1-score of the submission does not represent the true model performance: The score should be higher. My subjective estimate is that at least 30% of the false negatives should be true positives. This would lift the F1-score by about 2.5 percentage points.\nWhat do you think? Did I assign too much credit to the pre-trained model, for example, by crediting it with the capability to do a bit of common-sense reasoning? Does the data quality of the competition define the ceiling of the score that you can achieve, or do you see ways to navigate around this obstacle?"
  }
]